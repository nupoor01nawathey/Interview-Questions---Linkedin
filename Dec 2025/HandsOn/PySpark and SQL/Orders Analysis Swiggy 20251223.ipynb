{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb506d34-10b4-47f0-ac7e-9b5fb7bb5922",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------+-----------+-------------------+-------------------+--------------+\n|orderid|custid|     city|del_partner|         order_time|       deliver_time|predicted_time|\n+-------+------+---------+-----------+-------------------+-------------------+--------------+\n|      1|   101|   Mumbai|  Partner A|2024-12-18 10:00:00|2024-12-18 11:30:00|            60|\n|      2|   102|    Delhi|  Partner A|2024-12-18 09:00:00|2024-12-18 10:00:00|            45|\n|      3|   103|     Pune|  Partner A|2024-12-18 15:00:00|2024-12-18 15:30:00|            30|\n|      4|   104|   Mumbai|  Partner A|2024-12-18 14:00:00|2024-12-18 14:50:00|            45|\n|      5|   105|Bangalore|  Partner B|2024-12-18 08:00:00|2024-12-18 08:29:00|            30|\n|      6|   106|Hyderabad|  Partner B|2024-12-18 13:00:00|2024-12-18 14:00:00|            70|\n|      7|   107|  Kolkata|  Partner B|2024-12-18 10:00:00|2024-12-18 10:40:00|            45|\n|      8|   108|    Delhi|  Partner B|2024-12-18 18:00:00|2024-12-18 18:30:00|            40|\n|      9|   109|  Chennai|  Partner C|2024-12-18 07:00:00|2024-12-18 07:40:00|            30|\n|     10|   110|   Mumbai|  Partner C|2024-12-18 12:00:00|2024-12-18 13:00:00|            50|\n|     11|   111|    Delhi|  Partner C|2024-12-18 09:00:00|2024-12-18 09:35:00|            30|\n|     12|   112|Hyderabad|  Partner C|2024-12-18 16:00:00|2024-12-18 16:45:00|            30|\n+-------+------+---------+-----------+-------------------+-------------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Swiggy\n",
    "\n",
    "Write a sql query to calculate the number of delayed orders for each delivery partner. An order is considered \n",
    "delayed if the actual delivery time exceeds predicted delivery time.\n",
    "NOTE : If a partner has no delayed orders then display 0\n",
    "\n",
    "Input\n",
    "+---------+--------+-----------+-------------+---------------------+---------------------+----------------+\n",
    "| orderid | custid | city      | del_partner | order_time          | deliver_time        | predicted_time |\n",
    "+---------+--------+-----------+-------------+---------------------+---------------------+----------------+\n",
    "|       1 |    101 | Mumbai    | Partner A   | 2024-12-18 10:00:00 | 2024-12-18 11:30:00 |             60 |\n",
    "|       2 |    102 | Delhi     | Partner A   | 2024-12-18 09:00:00 | 2024-12-18 10:00:00 |             45 |\n",
    "|       3 |    103 | Pune      | Partner A   | 2024-12-18 15:00:00 | 2024-12-18 15:30:00 |             30 |\n",
    "|       4 |    104 | Mumbai    | Partner A   | 2024-12-18 14:00:00 | 2024-12-18 14:50:00 |             45 |\n",
    "|       5 |    105 | Bangalore | Partner B   | 2024-12-18 08:00:00 | 2024-12-18 08:29:00 |             30 |\n",
    "|       6 |    106 | Hyderabad | Partner B   | 2024-12-18 13:00:00 | 2024-12-18 14:00:00 |             70 |\n",
    "|       7 |    107 | Kolkata   | Partner B   | 2024-12-18 10:00:00 | 2024-12-18 10:40:00 |             45 |\n",
    "|       8 |    108 | Delhi     | Partner B   | 2024-12-18 18:00:00 | 2024-12-18 18:30:00 |             40 |\n",
    "|       9 |    109 | Chennai   | Partner C   | 2024-12-18 07:00:00 | 2024-12-18 07:40:00 |             30 |\n",
    "|      10 |    110 | Mumbai    | Partner C   | 2024-12-18 12:00:00 | 2024-12-18 13:00:00 |             50 |\n",
    "|      11 |    111 | Delhi     | Partner C   | 2024-12-18 09:00:00 | 2024-12-18 09:35:00 |             30 |\n",
    "|      12 |    112 | Hyderabad | Partner C   | 2024-12-18 16:00:00 | 2024-12-18 16:45:00 |             30 |\n",
    "+---------+--------+-----------+-------------+---------------------+---------------------+----------------+\n",
    "\n",
    "Output\n",
    "+-------------+----------------+\n",
    "| del_partner | delayed_orders |\n",
    "+-------------+----------------+\n",
    "| Partner A   |              3 |\n",
    "| Partner B   |              0 |\n",
    "| Partner C   |              4 |\n",
    "+-------------+----------------+\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"orderid\", IntegerType(), True),\n",
    "    StructField(\"custid\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"del_partner\", StringType(), True),\n",
    "    StructField(\"order_time\", StringType(), True), # Load as string first for easy data entry\n",
    "    StructField(\"deliver_time\", StringType(), True),\n",
    "    StructField(\"predicted_time\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (1, 101, \"Mumbai\", \"Partner A\", \"2024-12-18 10:00:00\", \"2024-12-18 11:30:00\", 60),\n",
    "    (2, 102, \"Delhi\", \"Partner A\", \"2024-12-18 09:00:00\", \"2024-12-18 10:00:00\", 45),\n",
    "    (3, 103, \"Pune\", \"Partner A\", \"2024-12-18 15:00:00\", \"2024-12-18 15:30:00\", 30),\n",
    "    (4, 104, \"Mumbai\", \"Partner A\", \"2024-12-18 14:00:00\", \"2024-12-18 14:50:00\", 45),\n",
    "    (5, 105, \"Bangalore\", \"Partner B\", \"2024-12-18 08:00:00\", \"2024-12-18 08:29:00\", 30),\n",
    "    (6, 106, \"Hyderabad\", \"Partner B\", \"2024-12-18 13:00:00\", \"2024-12-18 14:00:00\", 70),\n",
    "    (7, 107, \"Kolkata\", \"Partner B\", \"2024-12-18 10:00:00\", \"2024-12-18 10:40:00\", 45),\n",
    "    (8, 108, \"Delhi\", \"Partner B\", \"2024-12-18 18:00:00\", \"2024-12-18 18:30:00\", 40),\n",
    "    (9, 109, \"Chennai\", \"Partner C\", \"2024-12-18 07:00:00\", \"2024-12-18 07:40:00\", 30),\n",
    "    (10, 110, \"Mumbai\", \"Partner C\", \"2024-12-18 12:00:00\", \"2024-12-18 13:00:00\", 50),\n",
    "    (11, 111, \"Delhi\", \"Partner C\", \"2024-12-18 09:00:00\", \"2024-12-18 09:35:00\", 30),\n",
    "    (12, 112, \"Hyderabad\", \"Partner C\", \"2024-12-18 16:00:00\", \"2024-12-18 16:45:00\", 30)\n",
    "]\n",
    "\n",
    "swiggy_orders_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Convert string columns to actual Timestamp types\n",
    "from pyspark.sql.functions import col\n",
    "swiggy_orders_df = swiggy_orders_df.withColumn(\"order_time\", col(\"order_time\").cast(TimestampType())) \\\n",
    "       .withColumn(\"deliver_time\", col(\"deliver_time\").cast(TimestampType()))\n",
    "\n",
    "swiggy_orders_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca1db0d8-319d-4636-9e98-64f037e84281",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SPARK SQL       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e75ed7e-b73b-45a7-9514-8f233e0e3981",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+\n|del_partner|delayed_partners|\n+-----------+----------------+\n|  Partner A|               3|\n|  Partner B|               0|\n|  Partner C|               4|\n+-----------+----------------+\n\n+-----------+----------------+\n|del_partner|delayed_partners|\n+-----------+----------------+\n|  Partner A|               3|\n|  Partner B|               0|\n|  Partner C|               4|\n+-----------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "swiggy_orders_df.createOrReplaceTempView(\"swiggy_orders\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "          select \n",
    "            so1.del_partner, coalesce(delayed_partners, 0) as delayed_partners \n",
    "          from \n",
    "          (\n",
    "              select distinct del_partner from swiggy_orders \n",
    "          ) so1 left join\n",
    "          (\n",
    "            select \n",
    "                del_partner, count(*) as delayed_partners\n",
    "            from swiggy_orders\n",
    "            where timestampdiff(minute, order_time, deliver_time) > predicted_time\n",
    "            group by del_partner\n",
    "          ) so2\n",
    "          on so1.del_partner = so2.del_partner\n",
    "        \"\"\").show()\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select del_partner, sum(case when timestampdiff(minute, order_time, deliver_time) > predicted_time then 1 else 0 end) as delayed_partners\n",
    "    from swiggy_orders\n",
    "    group by del_partner\n",
    "    \"\"\"\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b08e7823-d724-4ce0-a23d-7631d180c12a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DF API        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08a5d94c-eceb-4042-a7d3-8142cd2e0182",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+\n|del_partner|delayed_partners|\n+-----------+----------------+\n|  Partner A|               3|\n|  Partner B|               0|\n|  Partner C|               4|\n+-----------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "delayed_partners_df = swiggy_orders_df.withColumn(\"actual_time\", timestamp_diff('MINUTE', col(\"order_time\"), col(\"deliver_time\"))) \\\n",
    "    .filter(col(\"actual_time\") > col(\"predicted_time\")) \\\n",
    "    .groupBy(col(\"del_partner\")).agg(count(\"*\").alias(\"delayed_partners\")) \\\n",
    "\n",
    "non_delayed_partners_df = swiggy_orders_df.select(\"del_partner\").distinct()\n",
    "\n",
    "non_delayed_partners_df.join(delayed_partners_df, non_delayed_partners_df[\"del_partner\"] == delayed_partners_df[\"del_partner\"], \"left\") \\\n",
    "    .withColumn(\"delayed_partners\", coalesce(col(\"delayed_partners\"), lit(0))) \\\n",
    "    .select(non_delayed_partners_df[\"del_partner\"], \"delayed_partners\") \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6bfdf19-c04b-42d3-85b0-090f09fa1000",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+----------+----+-----------+\n|order_date|customer_id|store_id|product_id|sale|order_value|\n+----------+-----------+--------+----------+----+-----------+\n|2024-12-01|        109|       1|         3|   2|        700|\n|2024-12-02|        110|       2|         2|   1|        300|\n|2024-12-03|        111|       1|         5|   3|        900|\n|2024-12-04|        112|       3|         1|   2|        500|\n|2024-12-05|        113|       3|         4|   4|       1200|\n|2024-12-05|        114|       3|         4|   2|        400|\n|2024-12-05|        115|       3|         4|   1|        300|\n|2024-12-01|        101|       1|         4|   2|        500|\n|2024-12-01|        102|       1|         4|   1|        300|\n|2024-12-02|        103|       2|         4|   3|        900|\n|2024-12-02|        104|       2|         4|   1|        400|\n|2024-12-03|        105|       1|         4|   2|        600|\n|2024-12-03|        106|       1|         4|   3|        800|\n|2024-12-04|        107|       3|         4|   1|        200|\n|2024-12-04|        108|       3|         4|   2|        500|\n+----------+-----------+--------+----------+----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "On which date did the 3rd highest sale of product 4 take place in terms of value(sale: qty_sold; \n",
    "value: qty_sold * price_of_product)\n",
    "+------------+-------------+----------+------------+------+-------------+\n",
    "| order_date | customer_id | store_id | product_id | sale | order_value |\n",
    "+------------+-------------+----------+------------+------+-------------+\n",
    "| 2024-12-01 |         109 |        1 |          3 |    2 |         700 |\n",
    "| 2024-12-02 |         110 |        2 |          2 |    1 |         300 |\n",
    "| 2024-12-03 |         111 |        1 |          5 |    3 |         900 |\n",
    "| 2024-12-04 |         112 |        3 |          1 |    2 |         500 |\n",
    "| 2024-12-05 |         113 |        3 |          4 |    4 |        1200 |\n",
    "| 2024-12-05 |         114 |        3 |          4 |    2 |         400 |\n",
    "| 2024-12-05 |         115 |        3 |          4 |    1 |         300 |\n",
    "| 2024-12-01 |         101 |        1 |          4 |    2 |         500 |\n",
    "| 2024-12-01 |         102 |        1 |          4 |    1 |         300 |\n",
    "| 2024-12-02 |         103 |        2 |          4 |    3 |         900 |\n",
    "| 2024-12-02 |         104 |        2 |          4 |    1 |         400 |\n",
    "| 2024-12-03 |         105 |        1 |          4 |    2 |         600 |\n",
    "| 2024-12-03 |         106 |        1 |          4 |    3 |         800 |\n",
    "| 2024-12-04 |         107 |        3 |          4 |    1 |         200 |\n",
    "| 2024-12-04 |         108 |        3 |          4 |    2 |         500 |\n",
    "+------------+-------------+----------+------------+------+-------------+\n",
    "\n",
    "+------------+-------------+\n",
    "| order_date | daily_sales |\n",
    "+------------+-------------+\n",
    "| 2024-12-02 |        1300 |\n",
    "+------------+-------------+\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"order_date\", StringType(), True), # Load as string first\n",
    "  StructField(\"customer_id\", IntegerType(), True),\n",
    "  StructField(\"store_id\", IntegerType(), True),\n",
    "  StructField(\"product_id\", IntegerType(), True),\n",
    "  StructField(\"sale\", IntegerType(), True),\n",
    "  StructField(\"order_value\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "  (\"2024-12-01\", 109, 1, 3, 2, 700),\n",
    "  (\"2024-12-02\", 110, 2, 2, 1, 300),\n",
    "  (\"2024-12-03\", 111, 1, 5, 3, 900),\n",
    "  (\"2024-12-04\", 112, 3, 1, 2, 500),\n",
    "  (\"2024-12-05\", 113, 3, 4, 4, 1200),\n",
    "  (\"2024-12-05\", 114, 3, 4, 2, 400),\n",
    "  (\"2024-12-05\", 115, 3, 4, 1, 300),\n",
    "  (\"2024-12-01\", 101, 1, 4, 2, 500),\n",
    "  (\"2024-12-01\", 102, 1, 4, 1, 300),\n",
    "  (\"2024-12-02\", 103, 2, 4, 3, 900),\n",
    "  (\"2024-12-02\", 104, 2, 4, 1, 400),\n",
    "  (\"2024-12-03\", 105, 1, 4, 2, 600),\n",
    "  (\"2024-12-03\", 106, 1, 4, 3, 800),\n",
    "  (\"2024-12-04\", 107, 3, 4, 1, 200),\n",
    "  (\"2024-12-04\", 108, 3, 4, 2, 500)\n",
    "]\n",
    "\n",
    "sales_data_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Convert the order_date string column to an actual DateType\n",
    "from pyspark.sql.functions import col\n",
    "sales_data_df = sales_data_df.withColumn(\"order_date\", col(\"order_date\").cast(DateType()))\n",
    "\n",
    "sales_data_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25d84211-0a8e-4d78-8746-bc6bcfb58948",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SPARK SQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfe36b2a-0610-46e1-bc2c-8eca69cb0299",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n|order_date|daily_sales|\n+----------+-----------+\n|2024-12-02|       1300|\n+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "sales_data_df.createOrReplaceTempView(\"sales_data\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "          with cte as (\n",
    "            select\n",
    "              order_date, sum(order_value) as daily_sales\n",
    "            from sales_data\n",
    "            where product_id=4\n",
    "            group by order_date\n",
    "          ), cte2 as (\n",
    "            select \n",
    "              *, \n",
    "              row_number() over(order by daily_sales desc) as rn\n",
    "            from cte\n",
    "          )\n",
    "          select \n",
    "            order_date, daily_sales\n",
    "          from cte2 where rn=3\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8551bce4-8db9-48d9-b18d-ede7f82bc0d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DF API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "339d247c-b449-48d7-b433-be5999654734",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n|order_date|daily_sales|\n+----------+-----------+\n|2024-12-02|       1300|\n+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "\n",
    "sales_data_df.filter(col(\"product_id\") == 4).groupBy(col(\"order_date\")).agg(sum(col(\"order_value\")).alias(\"daily_sales\")) \\\n",
    "    .withColumn(\"rn\", row_number().over(Window.orderBy(desc(col(\"daily_sales\"))))) \\\n",
    "    .filter(col(\"rn\") == 3) \\\n",
    "    .select(\"order_date\", \"daily_sales\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3642aa6e-5669-409e-851f-275c82cfae50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Orders Analysis Swiggy 20251223",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}