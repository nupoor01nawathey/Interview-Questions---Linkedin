{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4020cc69-c8fc-46bb-8971-5dcf87da2d93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----+----+----------+\n| id|experience|sql_|algo|bug_fixing|\n+---+----------+----+----+----------+\n|  1|         3| 100|NULL|        50|\n|  2|         5|NULL| 100|       100|\n|  3|         1| 100| 100|       100|\n|  4|         5| 100|  50|      NULL|\n|  5|         5| 100| 100|       100|\n+---+----------+----+----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "/*\n",
    "EPAM\n",
    "\n",
    "Write a SQL query to count the number of candidates got a perfect score in each experience category, \n",
    "in which they were requested to solve the task(NULL means the candidate was not requested to solve the \n",
    "tasks in that category)\n",
    "\n",
    "100 -> Perfect score\n",
    "NULL -> as good as perfect score since candidate was not requested to solve the tasks\n",
    "\n",
    "Input for dataset 1:\n",
    "+------+------------+------+------+------------+\n",
    "| id   | experience | sql_ | algo | bug_fixing |\n",
    "+------+------------+------+------+------------+\n",
    "|    1 |          3 |  100 | NULL |         50 |\n",
    "|    2 |          5 | NULL |  100 |        100 |\n",
    "|    3 |          1 |  100 |  100 |        100 |\n",
    "|    4 |          5 |  100 |   50 |       NULL |\n",
    "|    5 |          5 |  100 |  100 |        100 |\n",
    "+------+------------+------+------+------------+\n",
    "\n",
    "Output for datase 1:\n",
    "+------------+------------------+-----------+\n",
    "| experience | total_candidates | max_score |\n",
    "+------------+------------------+-----------+\n",
    "|          3 |                1 |         0 |\n",
    "|          5 |                3 |         2 |\n",
    "|          1 |                1 |         1 |\n",
    "+------------+------------------+-----------+\n",
    "\n",
    "\n",
    "Input for dataset 2:\n",
    "+------+------------+------+------+------------+\n",
    "| id   | experience | sql_ | algo | bug_fixing |\n",
    "+------+------------+------+------+------------+\n",
    "|    1 |          2 | NULL | NULL |       NULL |\n",
    "|    2 |         20 | NULL | NULL |         20 |\n",
    "|    3 |          7 |  100 | NULL |        100 |\n",
    "|    4 |          3 |  100 |   50 |       NULL |\n",
    "|    5 |          2 |   40 |  100 |        100 |\n",
    "+------+------------+------+------+------------+\n",
    "Output for datase 2:\n",
    "+------------+------------------+-----------+\n",
    "| experience | total_candidates | max_score |\n",
    "+------------+------------------+-----------+\n",
    "|          2 |                2 |         1 |\n",
    "|         20 |                1 |         0 |\n",
    "|          7 |                1 |         1 |\n",
    "|          3 |                1 |         0 |\n",
    "+------------+------------------+-----------+\n",
    "*/\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "assessments_df = spark.createDataFrame(\n",
    "    [\n",
    "        (1,3,100,None,50),\n",
    "        (2,5,None,100,100),\n",
    "        (3,1,100,100,100),\n",
    "        (4,5,100,50,None),\n",
    "        (5,5,100,100,100) \n",
    "    ],\n",
    "    [\"id\", \"experience\",\"sql_\", \"algo\", \"bug_fixing\"]\n",
    ")\n",
    "\n",
    "assessments_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "707379fd-0ffb-49d2-bc41-a37de416eac6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+------------------------+\n|experience|total_candidates|max_score_per_experience|\n+----------+----------------+------------------------+\n|         3|               1|                       0|\n|         5|               3|                       2|\n|         1|               1|                       1|\n+----------+----------------+------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "assessments_df.withColumn(\"sql_score\", when(col(\"sql_\").isNull() | (col(\"sql_\") == lit(100)), lit(1)).otherwise(lit(0))) \\\n",
    "    .withColumn(\"algo_score\", when(col(\"algo\").isNull() | (col(\"algo\") == lit(100)), lit(1)).otherwise(lit(0))) \\\n",
    "    .withColumn(\"bug_fixing\", when(col(\"bug_fixing\").isNull() | (col(\"bug_fixing\") == lit(100)), lit(1)).otherwise(lit(0))) \\\n",
    "    .withColumn(\"total_score\", when(col(\"sql_score\") + col(\"algo_score\") + col(\"bug_fixing\") == lit(3), lit(1)).otherwise(lit(0))) \\\n",
    "    .groupBy(\"experience\") \\\n",
    "    .agg(count(\"*\").alias(\"total_candidates\"), sum(col(\"total_score\")).alias(\"max_score_per_experience\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e98f2cc-a987-4114-b87a-38b1ff2d6848",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----+----+----------+\n| id|experience|sql_|algo|bug_fixing|\n+---+----------+----+----+----------+\n|  1|         2|NULL|NULL|      NULL|\n|  2|        20|NULL|NULL|        20|\n|  3|         7| 100|NULL|       100|\n|  4|         3| 100|  50|      NULL|\n|  5|         2|  40| 100|       100|\n+---+----------+----+----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# For input 2\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "assessments_df = spark.createDataFrame(\n",
    "    [\n",
    "        (1,2,None,None,None),\n",
    "        (2,20,None,None,20),\n",
    "        (3,7,100,None,100),\n",
    "        (4,3,100,50,None),\n",
    "        (5,2,40,100,100)\n",
    "    ],\n",
    "    [\"id\", \"experience\",\"sql_\", \"algo\", \"bug_fixing\"]\n",
    ")\n",
    "\n",
    "assessments_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58b45416-9e38-40d8-9e11-373ab7f0936c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+------------------------+\n|experience|total_candidates|max_score_per_experience|\n+----------+----------------+------------------------+\n|         2|               2|                       1|\n|        20|               1|                       0|\n|         7|               1|                       1|\n|         3|               1|                       0|\n+----------+----------------+------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "assessments_df.withColumn(\"sql_score\", when(col(\"sql_\").isNull() | (col(\"sql_\") == lit(100)), lit(1)).otherwise(lit(0))) \\\n",
    "    .withColumn(\"algo_score\", when(col(\"algo\").isNull() | (col(\"algo\") == lit(100)), lit(1)).otherwise(lit(0))) \\\n",
    "    .withColumn(\"bug_fixing\", when(col(\"bug_fixing\").isNull() | (col(\"bug_fixing\") == lit(100)), lit(1)).otherwise(lit(0))) \\\n",
    "    .withColumn(\"total_score\", when(col(\"sql_score\") + col(\"algo_score\") + col(\"bug_fixing\") == lit(3), lit(1)).otherwise(lit(0))) \\\n",
    "    .groupBy(\"experience\") \\\n",
    "    .agg(count(\"*\").alias(\"total_candidates\"), sum(col(\"total_score\")).alias(\"max_score_per_experience\")) \\\n",
    "    .show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Candidates with perfect score EPAM 20251225",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}