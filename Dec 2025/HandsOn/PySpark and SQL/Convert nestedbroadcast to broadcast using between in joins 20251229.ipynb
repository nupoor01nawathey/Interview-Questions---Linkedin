{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10b58713-e554-4306-a3ce-b06eab527655",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------------+--------------------+-------------------+--------+\n|SalaryDataID|CalendarYear|      EmployeeName|          Department|           JobTitle|  salary|\n+------------+------------+------------------+--------------------+-------------------+--------+\n|           1|        2008|   Glover, Eugenia|Louisville Metro ...|     Police Officer|45952.98|\n|           2|        2008|      Embry, James|Louisville Metro ...|     Police Officer|37221.94|\n|           3|        2008|  Detalente, Frank|Louisville Metro ...|     Police Officer|70366.42|\n|           4|        2008|Passafiume, Donald|Louisville Metro ...|     Police Officer|55602.99|\n|           5|        2008|Hendricks, Maurice|Louisville Metro ...|     Police Officer|56916.34|\n|           6|        2008|     Maye, Barbara|       Neighborhoods|Business Specialist|34904.11|\n|           7|        2008|    Compton, Terry|Louisville Metro ...|     Police Officer|62047.26|\n|           8|        2008|   Hawkins, Benton|Louisville Metro ...|     Police Officer|57590.21|\n|           9|        2008|  Sanders, Rebecca|Louisville Metro ...|     Police Officer|24886.39|\n|          10|        2008|     Utsey, Darren|Louisville Metro ...|     Police Officer| 59775.4|\n|          11|        2008|       Barry, Elva|    Parks&Recreation|Business Specialist|35937.08|\n|          12|        2008|  Haworth, Jessica|Louisville Metro ...|     Police Officer|56886.21|\n|          13|        2008| Hanifen, Patricia|Louisville Metro ...|     Police Officer|23061.67|\n+------------+------------+------------------+--------------------+-------------------+--------+\n\n+----------+----------+-------------+\n|min_salary|max_salary|bonus_percent|\n+----------+----------+-------------+\n|         1|     20000|          2.0|\n|     20001|     30000|          3.0|\n|     30001|     50000|          3.5|\n|     50001|     65000|          3.7|\n|     65001|    100000|          3.9|\n+----------+----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "# Initialize Spark Session (if not already running)\n",
    "# spark = SparkSession.builder.appName(\"SalaryDataframes\").getOrCreate()\n",
    "\n",
    "# 1. Define the Schema\n",
    "employee_schema = StructType([\n",
    "    StructField(\"SalaryDataID\", IntegerType(), True),\n",
    "    StructField(\"CalendarYear\", IntegerType(), True),\n",
    "    StructField(\"EmployeeName\", StringType(), True),\n",
    "    StructField(\"Department\", StringType(), True),\n",
    "    StructField(\"JobTitle\", StringType(), True),\n",
    "    StructField(\"salary\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# 2. Define the Data\n",
    "employee_data = [\n",
    "    (1, 2008, 'Glover, Eugenia', 'Louisville Metro Police', 'Police Officer', 45952.98),\n",
    "    (2, 2008, 'Embry, James', 'Louisville Metro Police', 'Police Officer', 37221.94),\n",
    "    (3, 2008, 'Detalente, Frank', 'Louisville Metro Police', 'Police Officer', 70366.42),\n",
    "    (4, 2008, 'Passafiume, Donald', 'Louisville Metro Police', 'Police Officer', 55602.99),\n",
    "    (5, 2008, 'Hendricks, Maurice', 'Louisville Metro Police', 'Police Officer', 56916.34),\n",
    "    (6, 2008, 'Maye, Barbara', 'Neighborhoods', 'Business Specialist', 34904.11),\n",
    "    (7, 2008, 'Compton, Terry', 'Louisville Metro Police', 'Police Officer', 62047.26),\n",
    "    (8, 2008, 'Hawkins, Benton', 'Louisville Metro Police', 'Police Officer', 57590.21),\n",
    "    (9, 2008, 'Sanders, Rebecca', 'Louisville Metro Police', 'Police Officer', 24886.39),\n",
    "    (10, 2008, 'Utsey, Darren', 'Louisville Metro Police', 'Police Officer', 59775.4),\n",
    "    (11, 2008, 'Barry, Elva', 'Parks&Recreation', 'Business Specialist', 35937.08),\n",
    "    (12, 2008, 'Haworth, Jessica', 'Louisville Metro Police', 'Police Officer', 56886.21),\n",
    "    (13, 2008, 'Hanifen, Patricia', 'Louisville Metro Police', 'Police Officer', 23061.67)\n",
    "]\n",
    "\n",
    "# 3. Create the DataFrame\n",
    "df_employees = spark.createDataFrame(employee_data, schema=employee_schema)\n",
    "df_employees.show()\n",
    "\n",
    "\n",
    "\n",
    "# 1. Define the Schema\n",
    "bonus_schema = StructType([\n",
    "    StructField(\"min_salary\", IntegerType(), True),\n",
    "    StructField(\"max_salary\", IntegerType(), True),\n",
    "    StructField(\"bonus_percent\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# 2. Define the Data\n",
    "bonus_data = [\n",
    "    (1, 20000, 2.0),\n",
    "    (20001, 30000, 3.0),\n",
    "    (30001, 50000, 3.5),\n",
    "    (50001, 65000, 3.7),\n",
    "    (65001, 100000, 3.9)\n",
    "]\n",
    "\n",
    "# 3. Create the DataFrame\n",
    "df_bonus = spark.createDataFrame(bonus_data, schema=bonus_schema)\n",
    "df_bonus.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2250857-811d-4cfe-9d04-75ac3132a9f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   ColumnarToRow\n   +- PhotonResultStage\n      +- PhotonBroadcastNestedLoopJoin LeftOuter, BuildRight, ((salary#13328 >= cast(min_salary#13339 as double)) AND (salary#13328 <= cast(max_salary#13340 as double)))\n         :- PhotonRowToColumnar\n         :  +- LocalTableScan [SalaryDataID#13323, CalendarYear#13324, EmployeeName#13325, Department#13326, JobTitle#13327, salary#13328, temp_col#13356]\n         +- PhotonShuffleExchangeSource\n            +- PhotonShuffleMapStage EXECUTOR_BROADCAST, [id=#8280]\n               +- PhotonShuffleExchangeSink SinglePartition\n                  +- PhotonRowToColumnar\n                     +- LocalTableScan [min_salary#13339, max_salary#13340, bonus_percent#13341, temp_col#13353]\n\n\n== Photon Explanation ==\nThe query is fully supported by Photon.\n"
     ]
    }
   ],
   "source": [
    "employees_df.alias(\"e\").join(bonus_df.alias(\"b\"), \n",
    "                            sf.col(\"e.salary\").between(sf.col(\"b.min_salary\"), sf.col(\"b.max_salary\")), \"left\") \\\n",
    "            .explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4049922c-43bb-450c-9fdb-5c757218ba4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n'Join LeftOuter, 'and('and('`>=`('e.salary, 'b.min_salary), '`<=`('e.salary, 'b.max_salary)), '`==`('e.temp_col, 'b.temp_col))\n:- SubqueryAlias e\n:  +- Project [SalaryDataID#13323, CalendarYear#13324, EmployeeName#13325, Department#13326, JobTitle#13327, salary#13328, 1 AS temp_col#13414]\n:     +- LocalRelation [SalaryDataID#13323, CalendarYear#13324, EmployeeName#13325, Department#13326, JobTitle#13327, salary#13328]\n+- SubqueryAlias b\n   +- Project [min_salary#13339, max_salary#13340, bonus_percent#13341, 1 AS temp_col#13416]\n      +- LocalRelation [min_salary#13339, max_salary#13340, bonus_percent#13341]\n\n== Analyzed Logical Plan ==\nSalaryDataID: int, CalendarYear: int, EmployeeName: string, Department: string, JobTitle: string, salary: double, temp_col: int, min_salary: int, max_salary: int, bonus_percent: double, temp_col: int\nJoin LeftOuter, (((salary#13328 >= cast(min_salary#13339 as double)) AND (salary#13328 <= cast(max_salary#13340 as double))) AND (temp_col#13414 = temp_col#13416))\n:- SubqueryAlias e\n:  +- Project [SalaryDataID#13323, CalendarYear#13324, EmployeeName#13325, Department#13326, JobTitle#13327, salary#13328, 1 AS temp_col#13414]\n:     +- LocalRelation [SalaryDataID#13323, CalendarYear#13324, EmployeeName#13325, Department#13326, JobTitle#13327, salary#13328]\n+- SubqueryAlias b\n   +- Project [min_salary#13339, max_salary#13340, bonus_percent#13341, 1 AS temp_col#13416]\n      +- LocalRelation [min_salary#13339, max_salary#13340, bonus_percent#13341]\n\n== Optimized Logical Plan ==\nJoin LeftOuter, (((salary#13328 >= cast(min_salary#13339 as double)) AND (salary#13328 <= cast(max_salary#13340 as double))) AND (temp_col#13414 = temp_col#13416))\n:- LocalRelation [SalaryDataID#13323, CalendarYear#13324, EmployeeName#13325, Department#13326, JobTitle#13327, salary#13328, temp_col#13414]\n+- LocalRelation [min_salary#13339, max_salary#13340, bonus_percent#13341, temp_col#13416]\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   ColumnarToRow\n   +- PhotonResultStage\n      +- PhotonBroadcastHashJoin [temp_col#13414], [temp_col#13416], LeftOuter, BuildRight, ((salary#13328 >= cast(min_salary#13339 as double)) AND (salary#13328 <= cast(max_salary#13340 as double))), false, true\n         :- PhotonRowToColumnar\n         :  +- LocalTableScan [SalaryDataID#13323, CalendarYear#13324, EmployeeName#13325, Department#13326, JobTitle#13327, salary#13328, temp_col#13414]\n         +- PhotonShuffleExchangeSource\n            +- PhotonShuffleMapStage EXECUTOR_BROADCAST, [id=#8435]\n               +- PhotonShuffleExchangeSink SinglePartition\n                  +- PhotonRowToColumnar\n                     +- LocalTableScan [min_salary#13339, max_salary#13340, bonus_percent#13341, temp_col#13416]\n\n== Photon Explanation ==\nThe query is fully supported by Photon.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as sf\n",
    "\n",
    "\n",
    "bonus_df = df_bonus.withColumn(\"temp_col\", sf.lit(1))\n",
    "employees_df = df_employees.withColumn(\"temp_col\", sf.lit(1))\n",
    "\n",
    "employees_df.alias(\"e\").join(bonus_df.alias(\"b\"), \n",
    "                            (sf.col(\"e.salary\").between(sf.col(\"b.min_salary\"), sf.col(\"b.max_salary\"))) & \n",
    "                            (sf.col(\"e.temp_col\")==sf.col(\"b.temp_col\")), \"left\")\\\n",
    "            .explain(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f890f2c8-dc43-46d9-9027-e04212de0d56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------------+--------------------+-------------------+--------+--------+----------+----------+-------------+--------+\n|SalaryDataID|CalendarYear|      EmployeeName|          Department|           JobTitle|  salary|temp_col|min_salary|max_salary|bonus_percent|temp_col|\n+------------+------------+------------------+--------------------+-------------------+--------+--------+----------+----------+-------------+--------+\n|           1|        2008|   Glover, Eugenia|Louisville Metro ...|     Police Officer|45952.98|       1|     30001|     50000|          3.5|       1|\n|           3|        2008|  Detalente, Frank|Louisville Metro ...|     Police Officer|70366.42|       1|     65001|    100000|          3.9|       1|\n|           2|        2008|      Embry, James|Louisville Metro ...|     Police Officer|37221.94|       1|     30001|     50000|          3.5|       1|\n|           4|        2008|Passafiume, Donald|Louisville Metro ...|     Police Officer|55602.99|       1|     50001|     65000|          3.7|       1|\n|           5|        2008|Hendricks, Maurice|Louisville Metro ...|     Police Officer|56916.34|       1|     50001|     65000|          3.7|       1|\n|           6|        2008|     Maye, Barbara|       Neighborhoods|Business Specialist|34904.11|       1|     30001|     50000|          3.5|       1|\n|           7|        2008|    Compton, Terry|Louisville Metro ...|     Police Officer|62047.26|       1|     50001|     65000|          3.7|       1|\n|           8|        2008|   Hawkins, Benton|Louisville Metro ...|     Police Officer|57590.21|       1|     50001|     65000|          3.7|       1|\n|           9|        2008|  Sanders, Rebecca|Louisville Metro ...|     Police Officer|24886.39|       1|     20001|     30000|          3.0|       1|\n|          10|        2008|     Utsey, Darren|Louisville Metro ...|     Police Officer| 59775.4|       1|     50001|     65000|          3.7|       1|\n|          11|        2008|       Barry, Elva|    Parks&Recreation|Business Specialist|35937.08|       1|     30001|     50000|          3.5|       1|\n|          12|        2008|  Haworth, Jessica|Louisville Metro ...|     Police Officer|56886.21|       1|     50001|     65000|          3.7|       1|\n|          13|        2008| Hanifen, Patricia|Louisville Metro ...|     Police Officer|23061.67|       1|     20001|     30000|          3.0|       1|\n+------------+------------+------------------+--------------------+-------------------+--------+--------+----------+----------+-------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as sf\n",
    "\n",
    "\n",
    "bonus_df = df_bonus.withColumn(\"temp_col\", sf.lit(1))\n",
    "employees_df = df_employees.withColumn(\"temp_col\", sf.lit(1))\n",
    "\n",
    "employees_df.alias(\"e\").join(bonus_df.alias(\"b\"), \n",
    "                            (sf.col(\"e.salary\").between(sf.col(\"b.min_salary\"), sf.col(\"b.max_salary\"))) & \n",
    "                            (sf.col(\"e.temp_col\")==sf.col(\"b.temp_col\")), \"left\")\\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5271d9d-759f-4da0-b597-ab5a0627cdc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ex: In a real-time scenario, Letâ€™s consider one partition has 100K lines, and broadcasted table has 20K records.\n",
    "So with the above calculation, for each partition, BroadcastNextedLoopJoin will iterate over 2,000,000,000 times. Which could lead to performance issues. Here we have considered that all partitions are evenly distributed(Maybe before this step, we have to perform other join or group by or window operations). Which is not the case all the time.\n",
    "\n",
    "Spark will use Broadcast Hash Join if we add one equal condition in join like below.\n",
    "\n",
    "(sf.col(\"tbl1_col1\").between(sf.col(\"tbl2_col1\"), sf.col(\"tbl2_col2\"))) &((sf.col(\"tbl1_temp_col2\")==sf.col(\"tab2_temp_col2\"))\n",
    "\n",
    "But how we will get this new column, which we can use in the joining condition and it does not impact the joining outcome.\n",
    "Let us try to add default columns in both Dataframe and use it in the joining condition.\n",
    "\n",
    "bonus_df = bonus_df.withColumn(\"temp_clm\", sf.lit(1))\n",
    "emp_df = emp_df.withColumn(\"temp_clm\", sf.lit(1))\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Convert nestedbroadcast to broadcast using between in joins 20251229",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}