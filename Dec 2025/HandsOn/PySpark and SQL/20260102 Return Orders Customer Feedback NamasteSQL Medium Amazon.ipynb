{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3ffb7b1-7971-45b9-aef6-f4175f1bc6a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------------+-----+\n|order_id|order_date|customer_name|sales|\n+--------+----------+-------------+-----+\n|       1|2023-01-01|        Alexa|  100|\n|       2|2023-01-02|        Alexa|  200|\n|       3|2023-01-03|        Alexa|  300|\n|       4|2023-01-03|        Alexa|  400|\n|       5|2023-01-01|       Ramesh|  500|\n|       6|2023-01-02|       Ramesh|  600|\n|       7|2023-01-03|       Ramesh|  700|\n|       8|2023-01-03|         Neha|  800|\n|       9|2023-01-03|        Ankit|  800|\n|      10|2023-01-04|        Ankit|  900|\n|      11|2023-01-05|         Amit|  250|\n|      12|2023-01-06|         Amit|  300|\n|      13|2023-01-07|         Amit|  350|\n|      14|2023-01-05|         John|  150|\n|      15|2023-01-06|         John|  200|\n|      16|2023-01-07|         John|  250|\n|      17|2023-01-08|        Sneha|  400|\n|      18|2023-01-09|        Sneha|  450|\n|      19|2023-01-10|        Sneha|  500|\n|      20|2023-01-11|         Ravi|  600|\n+--------+----------+-------------+-----+\nonly showing top 20 rows\n+--------+-----------+\n|order_id|return_date|\n+--------+-----------+\n|       1| 2023-01-02|\n|       2| 2023-01-04|\n|       3| 2023-01-05|\n|       7| 2023-01-06|\n|       9| 2023-01-06|\n|      10| 2023-01-07|\n|      23| 2023-01-16|\n|      24| 2023-01-17|\n+--------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Namastekart, an e-commerce company, has observed a notable surge in return orders recently. \n",
    "They suspect that a specific group of customers may be responsible for a significant portion of these returns. \n",
    "To address this issue, their initial goal is to identify customers who have returned more than 50% of their orders. \n",
    "This way, they can proactively reach out to these customers to gather feedback.\n",
    "\n",
    "Write an SQL to find list of customers along with their return percent (Round to 2 decimal places), \n",
    "display the output in ascending order of customer name.\n",
    "\n",
    "Table: orders (primary key : order_id)\n",
    "+---------------+-------------+\n",
    "| COLUMN_NAME   | DATA_TYPE   |\n",
    "+---------------+-------------+\n",
    "| customer_name | varchar(10) |\n",
    "| order_date    | date        |\n",
    "| order_id      | int         |\n",
    "| sales         | int         |\n",
    "+---------------+-------------+\n",
    "\n",
    "Table: returns (primary key : order_id)\n",
    "+-------------+-----------+\n",
    "| COLUMN_NAME | DATA_TYPE |\n",
    "+-------------+-----------+\n",
    "| order_id    | int       |\n",
    "| return_date | date      |\n",
    "+-------------+-----------+\n",
    "\n",
    "+----------+------------+---------------+-------+\n",
    "| order_id | order_date | customer_name | sales |\n",
    "+----------+------------+---------------+-------+\n",
    "|        1 | 2023-01-01 | Alexa         |   100 |\n",
    "|        2 | 2023-01-02 | Alexa         |   200 |\n",
    "|        3 | 2023-01-03 | Alexa         |   300 |\n",
    "|        4 | 2023-01-03 | Alexa         |   400 |\n",
    "|        5 | 2023-01-01 | Ramesh        |   500 |\n",
    "|        6 | 2023-01-02 | Ramesh        |   600 |\n",
    "|        7 | 2023-01-03 | Ramesh        |   700 |\n",
    "|        8 | 2023-01-03 | Neha          |   800 |\n",
    "|        9 | 2023-01-03 | Ankit         |   800 |\n",
    "|       10 | 2023-01-04 | Ankit         |   900 |\n",
    "|       11 | 2023-01-05 | Amit          |   250 |\n",
    "|       12 | 2023-01-06 | Amit          |   300 |\n",
    "|       13 | 2023-01-07 | Amit          |   350 |\n",
    "|       14 | 2023-01-05 | John          |   150 |\n",
    "|       15 | 2023-01-06 | John          |   200 |\n",
    "|       16 | 2023-01-07 | John          |   250 |\n",
    "|       17 | 2023-01-08 | Sneha         |   400 |\n",
    "|       18 | 2023-01-09 | Sneha         |   450 |\n",
    "|       19 | 2023-01-10 | Sneha         |   500 |\n",
    "|       20 | 2023-01-11 | Ravi          |   600 |\n",
    "|       21 | 2023-01-12 | Ravi          |   650 |\n",
    "|       22 | 2023-01-13 | Ravi          |   700 |\n",
    "|       23 | 2023-01-14 | Priya         |   300 |\n",
    "|       24 | 2023-01-15 | Priya         |   350 |\n",
    "|       25 | 2023-01-16 | Priya         |   400 |\n",
    "+----------+------------+---------------+-------+\n",
    "+----------+-------------+\n",
    "| order_id | return_date |\n",
    "+----------+-------------+\n",
    "|        1 | 2023-01-02  |\n",
    "|        2 | 2023-01-04  |\n",
    "|        3 | 2023-01-05  |\n",
    "|        7 | 2023-01-06  |\n",
    "|        9 | 2023-01-06  |\n",
    "|       10 | 2023-01-07  |\n",
    "|       23 | 2023-01-16  |\n",
    "|       24 | 2023-01-17  |\n",
    "+----------+-------------+\n",
    "\n",
    "\n",
    "Output\n",
    "+---------------+----------------+\n",
    "| customer_name | return_percent |\n",
    "+---------------+----------------+\n",
    "| Alexa         |          75.00 |\n",
    "| Ankit         |         100.00 |\n",
    "| Priya         |          66.67 |\n",
    "+---------------+----------------+\n",
    "\"\"\"\n",
    "\n",
    "orders_df = spark.createDataFrame(\n",
    "    [\n",
    "        (1,'2023-01-01','Alexa',100),\n",
    "        (2,'2023-01-02','Alexa',200),\n",
    "        (3,'2023-01-03','Alexa',300),\n",
    "        (4,'2023-01-03','Alexa',400),\n",
    "        (5,'2023-01-01','Ramesh',500),\n",
    "        (6,'2023-01-02','Ramesh',600),\n",
    "        (7,'2023-01-03','Ramesh',700),\n",
    "        (8,'2023-01-03','Neha',800),\n",
    "        (9,'2023-01-03','Ankit',800),\n",
    "        (10,'2023-01-04','Ankit',900),\n",
    "        (11,'2023-01-05','Amit',250),\n",
    "        (12,'2023-01-06','Amit',300),\n",
    "        (13,'2023-01-07','Amit',350),\n",
    "        (14,'2023-01-05','John',150),\n",
    "        (15,'2023-01-06','John',200),\n",
    "        (16,'2023-01-07','John',250),\n",
    "        (17,'2023-01-08','Sneha',400),\n",
    "        (18,'2023-01-09','Sneha',450),\n",
    "        (19,'2023-01-10','Sneha',500),\n",
    "        (20,'2023-01-11','Ravi',600),\n",
    "        (21,'2023-01-12','Ravi',650),\n",
    "        (22,'2023-01-13','Ravi',700),\n",
    "        (23,'2023-01-14','Priya',300),\n",
    "        (24,'2023-01-15','Priya',350),\n",
    "        (25,'2023-01-16','Priya',400)\n",
    "    ], [\"order_id\", \"order_date\", \"customer_name\", \"sales\"]\n",
    ")\n",
    "\n",
    "returns_df = spark.createDataFrame(\n",
    "    [\n",
    "        (1,'2023-01-02'),\n",
    "        (2,'2023-01-04'),\n",
    "        (3,'2023-01-05'),\n",
    "        (7,'2023-01-06'),\n",
    "        (9,'2023-01-06'),\n",
    "        (10,'2023-01-07'),\n",
    "        (23,'2023-01-16'),\n",
    "        (24,'2023-01-17')\n",
    "    ], [\"order_id\", \"return_date\"]\n",
    ")\n",
    "\n",
    "orders_df.show()\n",
    "returns_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ca64f63-161b-4087-bfac-847be06df6db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n|customer_name|return_percentage|\n+-------------+-----------------+\n|        Alexa|             75.0|\n|        Ankit|            100.0|\n|        Priya|            66.67|\n+-------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "orders_df.join(returns_df, orders_df.order_id==returns_df.order_id, \"left\") \\\n",
    "    .groupBy(\"customer_name\") \\\n",
    "    .agg(\n",
    "        round( 100.0 * count(returns_df.order_id) / count(orders_df.order_id) ,2).alias(\"return_percentage\")\n",
    "    ) \\\n",
    "    .filter(col(\"return_percentage\") > lit(50.0)) \\\n",
    "    .show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "20260102 Return Orders Customer Feedback NamasteSQL Medium Amazon",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}