{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4d23c37-4294-406e-a2a2-7751c94c0ad7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from delta.tables import *\n",
    "\n",
    "# create DeltaTable instances using the path\n",
    "delta_table = DeltaTable.forPath(spark, \"path/to/table\")\n",
    "\n",
    "# convert an existing Parquet table in place into a Delta table\n",
    "delta_table = DeltaTable.spark(spark, \"parquet.`path/to/table`\")\n",
    "\n",
    "# Delete data from the table using predicate\n",
    "delta_table.delete(\"date < '2017-01-01'\")   # predicate using SQL formatted string\n",
    "deltaTable.delete(col(\"date\") < \"2017-01-01\")   # predicate using Spark SQL functions\n",
    "\n",
    "# Update data from the table on the rows that match the given condition, which performs the rules defined by set\n",
    "# 1. condition using SQL formatted string\n",
    "delta_table.update(\n",
    "    condition = \"dt = existing_date\",\n",
    "    set = {\"dt\": \"'your_date'\"}\n",
    ")\n",
    "\n",
    "# 2. condition using Spark SQL functions\n",
    "delta_table.update(\n",
    "    condition = col(\"dt\") == \"existing_date\",\n",
    "    set = {col(\"dt\"): lit(\"your_date\")}\n",
    ")\n",
    "\n",
    "# merge command\n",
    "target_df.alias(\"trgt\").merge(source_df.alias(\"src\"), \"trgt.id = src.id\")\\\n",
    "    .whenMatchedUpdateAll()\\\n",
    "    .whenNotMatchedInsertAll()\\\n",
    "    .execute()\n",
    "\n",
    "target_df.alias(\"trgt\").merge(source_df.alias(\"src\"), \"trgt.id = src.id\")\\\n",
    "    .whenMatchedUpdate(set = {\n",
    "        \"key1\": \"value1\",\n",
    "        \"key2\": \"value2\"\n",
    "    })\\\n",
    "    .whenNotMatchedInsert(values = {\n",
    "        \"key3\": \"value3\",\n",
    "        \"key4\": \"value4\"\n",
    "    }).execute()\n",
    "\n",
    "# vacuum, physically delete files\n",
    "delta_table = vacuum() # vacuum files not required by versions more than 7 days old\n",
    "deltaTable.vacuum(100)  # vacuum files not required by versions more than 100 hours old\n",
    "\n",
    "# history command\n",
    "full_table_history = delta_table.history()      # get the full history of the table\n",
    "specific_table_history = delta_table.history(5) # get the last operation\n",
    "\n",
    "# Get the details of a Delta table such as the format, name, and size.\n",
    "delta_table.detail()\n",
    "\n",
    "# restore / rollback\n",
    "delta_table.restoreToVersion(2)\n",
    "restoreToTimestamp('2021-01-01')\n",
    "restoreToTimestamp('2021-01-01 01:01:01')\n",
    "\n",
    "# optimize, small file compaction\n",
    "delta_table.optimize().executeCompaction()\n",
    "delta_table.optimize().where(\"date='2021-11-18'\").executeCompaction()\n",
    "\n",
    "# SCD\n",
    "Type 1 https://github.com/delta-io/delta-examples/blob/master/notebooks/pyspark/delta-merge.ipynb\n",
    "Type 2 https://iterationinsights.com/article/how-to-implement-slowly-changing-dimensions-scd-type-2-using-delta-table/"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Cheetsheet Delta Lake PySpark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}