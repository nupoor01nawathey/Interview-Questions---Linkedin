{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f25f324-c0f1-47bb-be7e-548870310ddc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+----------+\n",
      "| id|name|    src_dt|  src_hash|\n",
      "+---+----+----------+----------+\n",
      "|  1|Josh|2025-01-02|-850647380|\n",
      "+---+----+----------+----------+\n",
      "\n",
      "+----+----+---------+------------+----------+---------+\n",
      "|  id|name|is_active|eff_start_dt|eff_end_dt|trgt_hash|\n",
      "+----+----+---------+------------+----------+---------+\n",
      "|NULL|    |         |            |          |142593372|\n",
      "+----+----+---------+------------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "    SCD 2\n",
    "    Assuming there are no logical or physical deletes, only new and updates are considered with usual versioning logic\n",
    "\n",
    "    1 - Mark Y to N if there comes any update for existing rec\n",
    "    2 - Insert new rec for existing rec if match is found based on hash\n",
    "    3 - Insert only new rec using left_anti join\n",
    "    4 - Select rows which are only in target (we have to keep untouched records as is in the target table)\n",
    "    5 - Union 1, 2, 3 and 4\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "src_df = spark.createDataFrame([\n",
    "    (\"1\", \"Josh\",\"2025-01-02\")\n",
    "], [\"id\", \"name\", \"src_dt\"])\n",
    "\n",
    "trgt_df = spark.createDataFrame([\n",
    "    (\"\", \"\", \"\", \"\", \"\")\n",
    "], [\"id\", \"name\", \"is_active\", \"eff_start_dt\", \"eff_end_dt\"])\n",
    "\n",
    "src_df = src_df.withColumn(\"id\", col(\"id\").cast(\"int\")).withColumn(\"src_hash\", hash(\"id\", \"name\"))\n",
    "trgt_df = trgt_df.withColumn(\"id\", col(\"id\").cast(\"int\")).withColumn(\"trgt_hash\", hash(\"id\", \"name\"))\n",
    "\n",
    "src_df.show()\n",
    "trgt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f9fd36e-a5a9-47cc-ba3b-1c7a43c53b58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+------------+----------+----------+\n",
      "| id|name|is_active|eff_start_dt|eff_end_dt| trgt_hash|\n",
      "+---+----+---------+------------+----------+----------+\n",
      "|  1|Josh|        N|  2025-01-02|2025-01-03|-850647380|\n",
      "+---+----+---------+------------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+------------+----------+-----------+\n",
      "| id|  name|is_active|eff_start_dt|eff_end_dt|   src_hash|\n",
      "+---+------+---------+------------+----------+-----------+\n",
      "|  1|Joshua|        Y|  2025-01-03|9999-12-31|-1131722944|\n",
      "+---+------+---------+------------+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+------------+----------+----------+\n",
      "| id|name|is_active|eff_start_dt|eff_end_dt|  src_hash|\n",
      "+---+----+---------+------------+----------+----------+\n",
      "|  2|John|        Y|  2025-01-03|9999-12-31|-433068882|\n",
      "+---+----+---------+------------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+------------+----------+-----------+\n",
      "| id|  name|is_active|eff_start_dt|eff_end_dt|  trgt_hash|\n",
      "+---+------+---------+------------+----------+-----------+\n",
      "|  1|  Josh|        N|  2025-01-02|2025-01-03| -850647380|\n",
      "|  1|Joshua|        Y|  2025-01-03|9999-12-31|-1131722944|\n",
      "|  2|  John|        Y|  2025-01-03|9999-12-31| -433068882|\n",
      "+---+------+---------+------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "existing_only_in_trgt = trgt_df.join(src_df, src_df[\"id\"]==trgt_df[\"id\"], \"left_anti\")\n",
    "\n",
    "\n",
    "### mark only active records to inactive first\n",
    "active_to_inactive = trgt_df.join(src_df, trgt_df[\"id\"] == src_df[\"id\"], \"left\")\\\n",
    "    .filter(col(\"is_active\") == \"Y\")\\\n",
    "    .withColumn(\"is_active\", lit(\"N\")).withColumn(\"eff_end_dt\", col(\"src_dt\"))\\\n",
    "    .select(trgt_df[\"id\"], trgt_df[\"name\"], \"is_active\", \"eff_start_dt\", \"eff_end_dt\", \"trgt_hash\")\n",
    "active_to_inactive.show()\n",
    "\n",
    "active_for_existing = trgt_df.join(src_df, trgt_df[\"id\"] == src_df[\"id\"], \"left\")\\\n",
    "    .filter(col(\"src_hash\") != col(\"trgt_hash\"))\\\n",
    "    .select(src_df[\"id\"], src_df[\"name\"], lit(\"Y\").alias(\"is_active\"), col(\"src_dt\").alias(\"eff_start_dt\"), lit(\"9999-12-31\").alias(\"eff_end_dt\"), \"src_hash\")\n",
    "active_for_existing.show()\n",
    "\n",
    "new_rec = src_df.join(trgt_df, src_df[\"id\"]==trgt_df[\"id\"], \"left_anti\")\\\n",
    "    .select(src_df[\"id\"], src_df[\"name\"], lit(\"Y\").alias(\"is_active\"), col(\"src_dt\").alias(\"eff_start_dt\"), lit(\"9999-12-31\").alias(\"eff_end_dt\"), \"src_hash\")\n",
    "new_rec.show()\n",
    "\n",
    "final_data = active_to_inactive.union(existing_only_in_trgt).union(active_for_existing).union(new_rec).dropDuplicates()\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbe39abb-3131-414c-b2c6-7ede7b443f57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+-----------+\n",
      "| id|  name|    src_dt|   src_hash|\n",
      "+---+------+----------+-----------+\n",
      "|  1|Joshua|2025-01-03|-1131722944|\n",
      "|  2|  John|2025-01-03| -433068882|\n",
      "+---+------+----------+-----------+\n",
      "\n",
      "+---+----+---------+------------+----------+----------+\n",
      "| id|name|is_active|eff_start_dt|eff_end_dt| trgt_hash|\n",
      "+---+----+---------+------------+----------+----------+\n",
      "|  1|Josh|        Y|  2025-01-02|9999-12-31|-850647380|\n",
      "+---+----+---------+------------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+------------+----------+----------+\n",
      "| id|name|is_active|eff_start_dt|eff_end_dt| trgt_hash|\n",
      "+---+----+---------+------------+----------+----------+\n",
      "|  1|Josh|        N|  2025-01-02|2025-01-03|-850647380|\n",
      "+---+----+---------+------------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+------------+----------+-----------+\n",
      "| id|  name|is_active|eff_start_dt|eff_end_dt|   src_hash|\n",
      "+---+------+---------+------------+----------+-----------+\n",
      "|  1|Joshua|        Y|  2025-01-03|9999-12-31|-1131722944|\n",
      "+---+------+---------+------------+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+------------+----------+----------+\n",
      "| id|name|is_active|eff_start_dt|eff_end_dt|  src_hash|\n",
      "+---+----+---------+------------+----------+----------+\n",
      "|  2|John|        Y|  2025-01-03|9999-12-31|-433068882|\n",
      "+---+----+---------+------------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+------------+----------+-----------+\n",
      "| id|  name|is_active|eff_start_dt|eff_end_dt|  trgt_hash|\n",
      "+---+------+---------+------------+----------+-----------+\n",
      "|  1|  Josh|        N|  2025-01-02|2025-01-03| -850647380|\n",
      "|  1|Joshua|        Y|  2025-01-03|9999-12-31|-1131722944|\n",
      "|  2|  John|        Y|  2025-01-03|9999-12-31| -433068882|\n",
      "+---+------+---------+------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "src_df = spark.createDataFrame([\n",
    "    (\"1\", \"Joshua\",\"2025-01-03\"),\n",
    "    (\"2\", \"John\",\"2025-01-03\")\n",
    "], [\"id\", \"name\", \"src_dt\"])\n",
    "src_df = src_df.withColumn(\"id\", col(\"id\").cast(\"int\")).withColumn(\"src_hash\", hash(\"id\", \"name\"))\n",
    "trgt_df = spark.createDataFrame([\n",
    "    (1, \"Josh\", \"Y\", \"2025-01-02\", \"9999-12-31\", -850647380)\n",
    "], [\"id\", \"name\", \"is_active\", \"eff_start_dt\", \"eff_end_dt\", \"trgt_hash\"])\n",
    "\n",
    "src_df.show()\n",
    "trgt_df.show()\n",
    "\n",
    "existing_only_in_trgt = trgt_df.join(src_df, src_df[\"id\"]==trgt_df[\"id\"], \"left_anti\")\n",
    "\n",
    "### mark only active records to inactive first\n",
    "active_to_inactive = trgt_df.join(src_df, trgt_df[\"id\"] == src_df[\"id\"], \"left\")\\\n",
    "    .filter((col(\"src_hash\") != col(\"trgt_hash\")) & (col(\"is_active\") == \"Y\"))\\\n",
    "    .withColumn(\"is_active\", lit(\"N\")).withColumn(\"eff_end_dt\", col(\"src_dt\"))\\\n",
    "    .select(trgt_df[\"id\"], trgt_df[\"name\"], \"is_active\", \"eff_start_dt\", \"eff_end_dt\", \"trgt_hash\")\n",
    "active_to_inactive.show()\n",
    "\n",
    "active_for_existing = trgt_df.join(src_df, trgt_df[\"id\"] == src_df[\"id\"], \"left\")\\\n",
    "    .filter(col(\"src_hash\") != col(\"trgt_hash\"))\\\n",
    "    .select(src_df[\"id\"], src_df[\"name\"], lit(\"Y\").alias(\"is_active\"), col(\"src_dt\").alias(\"eff_start_dt\"), lit(\"9999-12-31\").alias(\"eff_end_dt\"), \"src_hash\")\\\n",
    "    .dropDuplicates()\n",
    "active_for_existing.show()\n",
    "\n",
    "new_rec = src_df.join(trgt_df, src_df[\"id\"]==trgt_df[\"id\"], \"left_anti\")\\\n",
    "    .select(src_df[\"id\"], src_df[\"name\"], lit(\"Y\").alias(\"is_active\"), col(\"src_dt\").alias(\"eff_start_dt\"), lit(\"9999-12-31\").alias(\"eff_end_dt\"), \"src_hash\")\n",
    "new_rec.show()\n",
    "\n",
    "final_data = active_to_inactive.union(existing_only_in_trgt).union(active_for_existing).union(new_rec).union(trgt_df.join(src_df, trgt_df[\"id\"] == src_df[\"id\"], \"left\").filter(col(\"is_active\") == \"N\").select(trgt_df[\"id\"], trgt_df[\"name\"], \"is_active\", \"eff_start_dt\", \"eff_end_dt\", \"trgt_hash\") \\\n",
    "                                                                                                            ).dropDuplicates()\n",
    "final_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a7c929e-794f-4894-9ef6-887f978d2d92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+-----------+\n",
      "| id| name|    src_dt|   src_hash|\n",
      "+---+-----+----------+-----------+\n",
      "|  2|Scott|2025-01-04|-2119877053|\n",
      "|  3| Jake|2025-01-04| 1749065146|\n",
      "+---+-----+----------+-----------+\n",
      "\n",
      "+---+------+---------+------------+----------+-----------+\n",
      "| id|  name|is_active|eff_start_dt|eff_end_dt|  trgt_hash|\n",
      "+---+------+---------+------------+----------+-----------+\n",
      "|  1|  Josh|        N|  2025-01-02|2025-01-03| -850647380|\n",
      "|  1|Joshua|        Y|  2025-01-03|9999-12-31|-1131722944|\n",
      "|  2|  John|        Y|  2025-01-03|9999-12-31| -433068882|\n",
      "+---+------+---------+------------+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+------------+----------+----------+\n",
      "| id|name|is_active|eff_start_dt|eff_end_dt| trgt_hash|\n",
      "+---+----+---------+------------+----------+----------+\n",
      "|  2|John|        N|  2025-01-03|2025-01-04|-433068882|\n",
      "+---+----+---------+------------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+------------+----------+-----------+\n",
      "| id| name|is_active|eff_start_dt|eff_end_dt|   src_hash|\n",
      "+---+-----+---------+------------+----------+-----------+\n",
      "|  2|Scott|        Y|  2025-01-04|9999-12-31|-2119877053|\n",
      "+---+-----+---------+------------+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+------------+----------+----------+\n",
      "| id|name|is_active|eff_start_dt|eff_end_dt|  src_hash|\n",
      "+---+----+---------+------------+----------+----------+\n",
      "|  3|Jake|        Y|  2025-01-04|9999-12-31|1749065146|\n",
      "+---+----+---------+------------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+------------+----------+-----------+\n",
      "| id|  name|is_active|eff_start_dt|eff_end_dt|  trgt_hash|\n",
      "+---+------+---------+------------+----------+-----------+\n",
      "|  2|  John|        N|  2025-01-03|2025-01-04| -433068882|\n",
      "|  1|  Josh|        N|  2025-01-02|2025-01-03| -850647380|\n",
      "|  1|Joshua|        Y|  2025-01-03|9999-12-31|-1131722944|\n",
      "|  2| Scott|        Y|  2025-01-04|9999-12-31|-2119877053|\n",
      "|  3|  Jake|        Y|  2025-01-04|9999-12-31| 1749065146|\n",
      "+---+------+---------+------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "src_df = spark.createDataFrame([\n",
    "    (2, \"Scott\",\"2025-01-04\"),\n",
    "    (3, \"Jake\",\"2025-01-04\")\n",
    "], [\"id\", \"name\", \"src_dt\"])\n",
    "src_df = src_df.withColumn(\"id\", col(\"id\").cast(\"int\")).withColumn(\"src_hash\", hash(\"id\", \"name\"))\n",
    "trgt_df = spark.createDataFrame([\n",
    "    (1,\"Josh\",\"N\",\"2025-01-02\",\"2025-01-03\",-850647380),\n",
    "    (1,\"Joshua\",\"Y\",\"2025-01-03\",\"9999-12-31\",-1131722944),\n",
    "    (2,\"John\",\"Y\",\"2025-01-03\",\"9999-12-31\",-433068882),\n",
    "], [\"id\", \"name\", \"is_active\", \"eff_start_dt\", \"eff_end_dt\", \"trgt_hash\"])\n",
    "\n",
    "\n",
    "src_df.show()\n",
    "trgt_df.show()\n",
    "\n",
    "existing_only_in_trgt = trgt_df.join(src_df, src_df[\"id\"]==trgt_df[\"id\"], \"left_anti\")\n",
    "\n",
    "### mark only active records to inactive first\n",
    "active_to_inactive = trgt_df.join(src_df, trgt_df[\"id\"] == src_df[\"id\"], \"left\")\\\n",
    "    .filter((col(\"src_hash\") != col(\"trgt_hash\")) & (col(\"is_active\") == \"Y\"))\\\n",
    "    .withColumn(\"is_active\", lit(\"N\")).withColumn(\"eff_end_dt\", col(\"src_dt\"))\\\n",
    "    .select(trgt_df[\"id\"], trgt_df[\"name\"], \"is_active\", \"eff_start_dt\", \"eff_end_dt\", \"trgt_hash\")\n",
    "active_to_inactive.show()\n",
    "\n",
    "active_for_existing = trgt_df.join(src_df, trgt_df[\"id\"] == src_df[\"id\"], \"left\")\\\n",
    "    .filter(col(\"src_hash\") != col(\"trgt_hash\"))\\\n",
    "    .select(src_df[\"id\"], src_df[\"name\"], lit(\"Y\").alias(\"is_active\"), col(\"src_dt\").alias(\"eff_start_dt\"), lit(\"9999-12-31\").alias(\"eff_end_dt\"), \"src_hash\")\\\n",
    "    .dropDuplicates()\n",
    "active_for_existing.show()\n",
    "\n",
    "new_rec = src_df.join(trgt_df, src_df[\"id\"]==trgt_df[\"id\"], \"left_anti\")\\\n",
    "    .select(src_df[\"id\"], src_df[\"name\"], lit(\"Y\").alias(\"is_active\"), col(\"src_dt\").alias(\"eff_start_dt\"), lit(\"9999-12-31\").alias(\"eff_end_dt\"), \"src_hash\")\n",
    "new_rec.show()\n",
    "\n",
    "final_data = active_to_inactive.union(existing_only_in_trgt).union(active_for_existing).union(new_rec). \\\n",
    "select(trgt_df[\"id\"], trgt_df[\"name\"], \"is_active\", \"eff_start_dt\", \"eff_end_dt\", \"trgt_hash\")\n",
    "final_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82e16560-1419-49eb-9708-a377f9d923b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+----------+\n",
      "| id|name|    src_dt|  src_hash|\n",
      "+---+----+----------+----------+\n",
      "|  1|Josh|2025-01-02|-850647380|\n",
      "+---+----+----------+----------+\n",
      "\n",
      "+----+----+-------+---------+\n",
      "|  id|name|trgt_dt|trgt_hash|\n",
      "+----+----+-------+---------+\n",
      "|NULL|    |       |142593372|\n",
      "+----+----+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Consider same data as above to implement SCD Type 1\n",
    "    1 - Find matching rec from src and trg, and fully overwrite existing rec with incoming src rec\n",
    "    2 - Use left_anti join to determine only new rec\n",
    "    3 - Union 1, 2 \n",
    "\"\"\" \n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "src_df = spark.createDataFrame([\n",
    "    (\"1\", \"Josh\",\"2025-01-02\")\n",
    "], [\"id\", \"name\", \"src_dt\"])\n",
    "\n",
    "trgt_df = spark.createDataFrame([\n",
    "    (\"\", \"\", \"\")\n",
    "], [\"id\", \"name\", \"trgt_dt\"])\n",
    "\n",
    "src_df = src_df.withColumn(\"id\", col(\"id\").cast(\"int\")).withColumn(\"src_hash\", hash(\"id\", \"name\"))\n",
    "trgt_df = trgt_df.withColumn(\"id\", col(\"id\").cast(\"int\")).withColumn(\"trgt_hash\", hash(\"id\", \"name\"))\n",
    "\n",
    "src_df.show()\n",
    "trgt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4de1c6d-46de-4849-ab01-ef4c8d7aca65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 771:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+--------+\n",
      "| id|name|src_dt|src_hash|\n",
      "+---+----+------+--------+\n",
      "+---+----+------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+----------+\n",
      "| id|name|   trgt_dt|  src_hash|\n",
      "+---+----+----------+----------+\n",
      "|  1|Josh|2025-01-02|-850647380|\n",
      "+---+----+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 780:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+----------+\n",
      "| id|name|    src_dt|  src_hash|\n",
      "+---+----+----------+----------+\n",
      "|  1|Josh|2025-01-02|-850647380|\n",
      "+---+----+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "overwrite_for_existing = trgt_df.join(src_df, trgt_df[\"id\"] == src_df[\"id\"], \"left\")\\\n",
    "    .filter(col(\"src_hash\") != col(\"trgt_hash\"))\\\n",
    "    .select(src_df[\"id\"], src_df[\"name\"], \"src_dt\" , \"src_hash\")\\\n",
    "    .dropDuplicates()\n",
    "overwrite_for_existing.show()\n",
    "\n",
    "new_rec = src_df.join(trgt_df, src_df[\"id\"]==trgt_df[\"id\"], \"left_anti\")\\\n",
    "    .select(src_df[\"id\"], src_df[\"name\"], col(\"src_dt\").alias(\"trgt_dt\"), \"src_hash\")\n",
    "new_rec.show()\n",
    "\n",
    "final_data = overwrite_for_existing.union(new_rec)\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9656138c-45aa-40eb-8c84-b7fe7b41e9c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+-----------+\n",
      "| id|  name|    src_dt|   src_hash|\n",
      "+---+------+----------+-----------+\n",
      "|  1|Joshua|2025-01-03|-1131722944|\n",
      "+---+------+----------+-----------+\n",
      "\n",
      "+---+----+----------+----------+\n",
      "| id|name|   trgt_dt| trgt_hash|\n",
      "+---+----+----------+----------+\n",
      "|  1|Josh|2025-01-02|-850647380|\n",
      "+---+----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "src_df = spark.createDataFrame([\n",
    "    (\"1\", \"Joshua\",\"2025-01-03\")\n",
    "], [\"id\", \"name\", \"src_dt\"])\n",
    "src_df = src_df.withColumn(\"id\", col(\"id\").cast(\"int\")).withColumn(\"src_hash\", hash(\"id\", \"name\"))\n",
    "trgt_df = spark.createDataFrame([\n",
    "    (1, \"Josh\",  \"2025-01-02\", -850647380)\n",
    "], [\"id\", \"name\", \"trgt_dt\", \"trgt_hash\"])\n",
    "\n",
    "src_df.show()\n",
    "trgt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a5f4f37-e40a-4831-82a4-6791eef42da7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+-----------+\n",
      "| id|  name|    src_dt|   src_hash|\n",
      "+---+------+----------+-----------+\n",
      "|  1|Joshua|2025-01-03|-1131722944|\n",
      "+---+------+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+--------+\n",
      "| id|name|trgt_dt|src_hash|\n",
      "+---+----+-------+--------+\n",
      "+---+----+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 805:>                (0 + 4) / 4][Stage 808:>                (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+-----------+\n",
      "| id|  name|    src_dt|   src_hash|\n",
      "+---+------+----------+-----------+\n",
      "|  1|Joshua|2025-01-03|-1131722944|\n",
      "+---+------+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "overwrite_for_existing = trgt_df.join(src_df, trgt_df[\"id\"] == src_df[\"id\"], \"left\")\\\n",
    "    .filter(col(\"src_hash\") != col(\"trgt_hash\"))\\\n",
    "    .select(src_df[\"id\"], src_df[\"name\"], \"src_dt\" , \"src_hash\")\\\n",
    "    .dropDuplicates()\n",
    "overwrite_for_existing.show()\n",
    "\n",
    "new_rec = src_df.join(trgt_df, src_df[\"id\"]==trgt_df[\"id\"], \"left_anti\")\\\n",
    "    .select(src_df[\"id\"], src_df[\"name\"], col(\"src_dt\").alias(\"trgt_dt\"), \"src_hash\")\n",
    "new_rec.show()\n",
    "\n",
    "final_data = overwrite_for_existing.union(new_rec)\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c4278bc-0486-4afb-a2aa-bae8f8acf228",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+-----------+\n",
      "| id| name|    src_dt|   src_hash|\n",
      "+---+-----+----------+-----------+\n",
      "|  2|Scott|2025-01-04|-2119877053|\n",
      "|  1| Jake|2025-01-04| -811822073|\n",
      "+---+-----+----------+-----------+\n",
      "\n",
      "+---+------+----------+-----------+\n",
      "| id|  name|   trgt_dt|  trgt_hash|\n",
      "+---+------+----------+-----------+\n",
      "|  1|Joshua|2025-01-03|-1131722944|\n",
      "+---+------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "src_df = spark.createDataFrame([\n",
    "    (2, \"Scott\",\"2025-01-04\"),\n",
    "    (1, \"Jake\",\"2025-01-04\")\n",
    "], [\"id\", \"name\", \"src_dt\"])\n",
    "src_df = src_df.withColumn(\"id\", col(\"id\").cast(\"int\")).withColumn(\"src_hash\", hash(\"id\", \"name\"))\n",
    "trgt_df = spark.createDataFrame([\n",
    "    (1, \"Joshua\",  \"2025-01-03\", -1131722944)\n",
    "], [\"id\", \"name\", \"trgt_dt\", \"trgt_hash\"])\n",
    "\n",
    "src_df.show()\n",
    "trgt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c259b401-79c3-4181-a82c-457fa36a2cad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+----------+\n",
      "| id|name|    src_dt|  src_hash|\n",
      "+---+----+----------+----------+\n",
      "|  1|Jake|2025-01-04|-811822073|\n",
      "+---+----+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+-----------+\n",
      "| id| name|   trgt_dt|   src_hash|\n",
      "+---+-----+----------+-----------+\n",
      "|  2|Scott|2025-01-04|-2119877053|\n",
      "+---+-----+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+-----------+\n",
      "| id| name|    src_dt|   src_hash|\n",
      "+---+-----+----------+-----------+\n",
      "|  1| Jake|2025-01-04| -811822073|\n",
      "|  2|Scott|2025-01-04|-2119877053|\n",
      "+---+-----+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "overwrite_for_existing = trgt_df.join(src_df, trgt_df[\"id\"] == src_df[\"id\"], \"left\")\\\n",
    "    .filter(col(\"src_hash\") != col(\"trgt_hash\"))\\\n",
    "    .select(src_df[\"id\"], src_df[\"name\"], \"src_dt\" , \"src_hash\")\\\n",
    "    .dropDuplicates()\n",
    "overwrite_for_existing.show()\n",
    "\n",
    "new_rec = src_df.join(trgt_df, src_df[\"id\"]==trgt_df[\"id\"], \"left_anti\")\\\n",
    "    .select(src_df[\"id\"], src_df[\"name\"], col(\"src_dt\").alias(\"trgt_dt\"), \"src_hash\")\n",
    "new_rec.show()\n",
    "\n",
    "final_data = overwrite_for_existing.union(new_rec)\n",
    "final_data.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "25 SCD 1 and 2 Fully Working Example PySpark",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
