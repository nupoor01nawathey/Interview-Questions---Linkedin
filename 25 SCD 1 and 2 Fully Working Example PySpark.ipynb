{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f25f324-c0f1-47bb-be7e-548870310ddc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+----------+\n| id|name|    src_dt|  src_hash|\n+---+----+----------+----------+\n|  1|Josh|2025-01-02|-850647380|\n+---+----+----------+----------+\n\n+---+----+---------+------------+----------+---------+\n| id|name|is_active|eff_start_dt|eff_end_dt|trgt_hash|\n+---+----+---------+------------+----------+---------+\n|  1|John|        Y|  2025-01-01|9999-12-31|199793157|\n+---+----+---------+------------+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "    SCD 2\n",
    "    Assuming 1st load completed and trgt has at least 1 row data \n",
    "    There are no logical or physical deletes, only new and updates are considered with usual versioning logic\n",
    "\n",
    "    1 - Mark Y to N if there comes any update for existing rec\n",
    "    2 - Insert new rec for existing rec if match is found based on hash\n",
    "    3 - Insert only new rec using left_anti join\n",
    "    4 - Again pick up only N rec which were filtered in step 1, 2, 3\n",
    "    5 - Union 1, 2, 3 and 4\n",
    "    6 - We have to either create hash column(considering all cols in a row as any column can be a SCD for our data OR create sk. This is because we need unique identified for each row in the df)\n",

    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",

    "spark = SparkSession.builder.getOrCreate()\n",

    "src_df = spark.createDataFrame([\n",
    "    (\"1\", \"Josh\",\"2025-01-02\")\n",
    "], [\"id\", \"name\", \"src_dt\"])\n",
    "\n",
    "trgt_df = spark.createDataFrame([\n",
    "    (\"1\", \"John\", \"Y\", \"2025-01-01\", \"9999-12-31\")\n",
    "], [\"id\", \"name\", \"is_active\", \"eff_start_dt\", \"eff_end_dt\"])\n",
    "\n",
    "src_df = src_df.withColumn(\"id\", col(\"id\").cast(\"int\")).withColumn(\"src_hash\", hash(\"id\", \"name\"))\n",
    "trgt_df = trgt_df.withColumn(\"id\", col(\"id\").cast(\"int\")).withColumn(\"trgt_hash\", hash(\"id\", \"name\"))\n",
    "\n",
    "src_df.show()\n",
    "trgt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f9fd36e-a5a9-47cc-ba3b-1c7a43c53b58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+------------+----------+---------+\n| id|name|is_active|eff_start_dt|eff_end_dt|trgt_hash|\n+---+----+---------+------------+----------+---------+\n|  1|John|        N|  2025-01-01|2025-01-02|199793157|\n+---+----+---------+------------+----------+---------+\n\n+---+----+---------+------------+----------+----------+\n| id|name|is_active|eff_start_dt|eff_end_dt|  src_hash|\n+---+----+---------+------------+----------+----------+\n|  1|Josh|        Y|  2025-01-02|9999-12-31|-850647380|\n+---+----+---------+------------+----------+----------+\n\n+---+----+---------+------------+----------+--------+\n| id|name|is_active|eff_start_dt|eff_end_dt|src_hash|\n+---+----+---------+------------+----------+--------+\n+---+----+---------+------------+----------+--------+\n\n+---+----+---------+------------+----------+----------+\n| id|name|is_active|eff_start_dt|eff_end_dt| trgt_hash|\n+---+----+---------+------------+----------+----------+\n|  1|John|        N|  2025-01-01|2025-01-02| 199793157|\n|  1|Josh|        Y|  2025-01-02|9999-12-31|-850647380|\n+---+----+---------+------------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "### mark only active records to inactive first\n",
    "active_to_inactive = trgt_df.join(src_df, trgt_df[\"id\"] == src_df[\"id\"], \"left\")\\\n",
    "    .filter(col(\"is_active\") == \"Y\")\\\n",
    "    .withColumn(\"is_active\", lit(\"N\")).withColumn(\"eff_end_dt\", col(\"src_dt\"))\\\n",
    "    .select(trgt_df[\"id\"], trgt_df[\"name\"], \"is_active\", \"eff_start_dt\", \"eff_end_dt\", \"trgt_hash\")\n",
    "active_to_inactive.show()\n",
    "\n",
    "active_for_existing = trgt_df.join(src_df, trgt_df[\"id\"] == src_df[\"id\"], \"left\")\\\n",
    "    .filter(col(\"src_hash\") != col(\"trgt_hash\"))\\\n",
    "    .select(src_df[\"id\"], src_df[\"name\"], lit(\"Y\").alias(\"is_active\"), col(\"src_dt\").alias(\"eff_start_dt\"), lit(\"9999-12-31\").alias(\"eff_end_dt\"), \"src_hash\")\n",
    "active_for_existing.show()\n",
    "\n",
    "new_rec = src_df.join(trgt_df, src_df[\"id\"]==trgt_df[\"id\"], \"left_anti\")\\\n",
    "    .select(src_df[\"id\"], src_df[\"name\"], lit(\"Y\").alias(\"is_active\"), col(\"src_dt\").alias(\"eff_start_dt\"), lit(\"9999-12-31\").alias(\"eff_end_dt\"), \"src_hash\")\n",
    "new_rec.show()\n",
    "\n",
    "final_data = active_to_inactive.union(active_for_existing).union(new_rec)\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbe39abb-3131-414c-b2c6-7ede7b443f57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+-----------+\n| id|  name|    src_dt|   src_hash|\n+---+------+----------+-----------+\n|  1|Joshua|2025-01-03|-1131722944|\n+---+------+----------+-----------+\n\n+---+----+---------+------------+----------+----------+\n| id|name|is_active|eff_start_dt|eff_end_dt| trgt_hash|\n+---+----+---------+------------+----------+----------+\n|  1|John|        N|  2025-01-01|2025-01-02| 199793157|\n|  1|Josh|        Y|  2025-01-02|9999-12-31|-850647380|\n+---+----+---------+------------+----------+----------+\n\n+---+----+---------+------------+----------+----------+\n| id|name|is_active|eff_start_dt|eff_end_dt| trgt_hash|\n+---+----+---------+------------+----------+----------+\n|  1|Josh|        N|  2025-01-02|2025-01-03|-850647380|\n+---+----+---------+------------+----------+----------+\n\n+---+------+---------+------------+----------+-----------+\n| id|  name|is_active|eff_start_dt|eff_end_dt|   src_hash|\n+---+------+---------+------------+----------+-----------+\n|  1|Joshua|        Y|  2025-01-03|9999-12-31|-1131722944|\n+---+------+---------+------------+----------+-----------+\n\n+---+----+---------+------------+----------+--------+\n| id|name|is_active|eff_start_dt|eff_end_dt|src_hash|\n+---+----+---------+------------+----------+--------+\n+---+----+---------+------------+----------+--------+\n\n+---+------+---------+------------+----------+-----------+\n| id|  name|is_active|eff_start_dt|eff_end_dt|  trgt_hash|\n+---+------+---------+------------+----------+-----------+\n|  1|  Josh|        N|  2025-01-02|2025-01-03| -850647380|\n|  1|Joshua|        Y|  2025-01-03|9999-12-31|-1131722944|\n|  1|  John|        N|  2025-01-01|2025-01-02|  199793157|\n+---+------+---------+------------+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "src_df = spark.createDataFrame([\n",
    "    (\"1\", \"Joshua\",\"2025-01-03\")\n",
    "], [\"id\", \"name\", \"src_dt\"])\n",
    "src_df = src_df.withColumn(\"id\", col(\"id\").cast(\"int\")).withColumn(\"src_hash\", hash(\"id\", \"name\"))\n",
    "trgt_df = spark.createDataFrame([\n",
    "    (1, \"John\", \"N\", \"2025-01-01\", \"2025-01-02\", 199793157),\n",
    "    (1, \"Josh\", \"Y\", \"2025-01-02\", \"9999-12-31\", -850647380)\n",
    "], [\"id\", \"name\", \"is_active\", \"eff_start_dt\", \"eff_end_dt\", \"trgt_hash\"])\n",
    "\n",
    "src_df.show()\n",
    "trgt_df.show()\n",
    "\n",
    "### mark only active records to inactive first\n",
    "active_to_inactive = trgt_df.join(src_df, trgt_df[\"id\"] == src_df[\"id\"], \"left\")\\\n",
    "    .filter((col(\"src_hash\") != col(\"trgt_hash\")) & (col(\"is_active\") == \"Y\"))\\\n",
    "    .withColumn(\"is_active\", lit(\"N\")).withColumn(\"eff_end_dt\", col(\"src_dt\"))\\\n",
    "    .select(trgt_df[\"id\"], trgt_df[\"name\"], \"is_active\", \"eff_start_dt\", \"eff_end_dt\", \"trgt_hash\")\n",
    "active_to_inactive.show()\n",
    "\n",
    "active_for_existing = trgt_df.join(src_df, trgt_df[\"id\"] == src_df[\"id\"], \"left\")\\\n",
    "    .filter(col(\"src_hash\") != col(\"trgt_hash\"))\\\n",
    "    .select(src_df[\"id\"], src_df[\"name\"], lit(\"Y\").alias(\"is_active\"), col(\"src_dt\").alias(\"eff_start_dt\"), lit(\"9999-12-31\").alias(\"eff_end_dt\"), \"src_hash\")\\\n",
    "    .dropDuplicates()\n",
    "active_for_existing.show()\n",
    "\n",
    "new_rec = src_df.join(trgt_df, src_df[\"id\"]==trgt_df[\"id\"], \"left_anti\")\\\n",
    "    .select(src_df[\"id\"], src_df[\"name\"], lit(\"Y\").alias(\"is_active\"), col(\"src_dt\").alias(\"eff_start_dt\"), lit(\"9999-12-31\").alias(\"eff_end_dt\"), \"src_hash\")\n",
    "new_rec.show()\n",
    "\n",
    "final_data = active_to_inactive.union(active_for_existing).union(new_rec).union(trgt_df.join(src_df, trgt_df[\"id\"] == src_df[\"id\"], \"left\").filter(col(\"is_active\") == \"N\").select(trgt_df[\"id\"], trgt_df[\"name\"], \"is_active\", \"eff_start_dt\", \"eff_end_dt\", \"trgt_hash\"))\n",
    "final_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a7c929e-794f-4894-9ef6-887f978d2d92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+-----------+\n| id| name|    src_dt|   src_hash|\n+---+-----+----------+-----------+\n|  2|Scott|2025-01-04|-2119877053|\n|  1| Jake|2025-01-04| -811822073|\n+---+-----+----------+-----------+\n\n+---+------+---------+------------+----------+-----------+\n| id|  name|is_active|eff_start_dt|eff_end_dt|  trgt_hash|\n+---+------+---------+------------+----------+-----------+\n|  1|  John|        N|  2025-01-01|2025-01-02|  199793157|\n|  1|  Josh|        N|  2025-01-02|2025-01-03| -850647380|\n|  1|Joshua|        Y|  2025-01-03|9999-12-31|-1131722944|\n+---+------+---------+------------+----------+-----------+\n\n+---+------+---------+------------+----------+-----------+\n| id|  name|is_active|eff_start_dt|eff_end_dt|  trgt_hash|\n+---+------+---------+------------+----------+-----------+\n|  1|Joshua|        N|  2025-01-03|2025-01-04|-1131722944|\n+---+------+---------+------------+----------+-----------+\n\n+---+----+---------+------------+----------+----------+\n| id|name|is_active|eff_start_dt|eff_end_dt|  src_hash|\n+---+----+---------+------------+----------+----------+\n|  1|Jake|        Y|  2025-01-04|9999-12-31|-811822073|\n+---+----+---------+------------+----------+----------+\n\n+---+-----+---------+------------+----------+-----------+\n| id| name|is_active|eff_start_dt|eff_end_dt|   src_hash|\n+---+-----+---------+------------+----------+-----------+\n|  2|Scott|        Y|  2025-01-04|9999-12-31|-2119877053|\n+---+-----+---------+------------+----------+-----------+\n\n+---+------+---------+------------+----------+-----------+\n| id|  name|is_active|eff_start_dt|eff_end_dt|  trgt_hash|\n+---+------+---------+------------+----------+-----------+\n|  1|Joshua|        N|  2025-01-03|2025-01-04|-1131722944|\n|  1|  Jake|        Y|  2025-01-04|9999-12-31| -811822073|\n|  2| Scott|        Y|  2025-01-04|9999-12-31|-2119877053|\n|  1|  John|        N|  2025-01-01|2025-01-02|  199793157|\n|  1|  Josh|        N|  2025-01-02|2025-01-03| -850647380|\n+---+------+---------+------------+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "src_df = spark.createDataFrame([\n",
    "    (2, \"Scott\",\"2025-01-04\"),\n",
    "    (1, \"Jake\",\"2025-01-04\")\n",
    "], [\"id\", \"name\", \"src_dt\"])\n",
    "src_df = src_df.withColumn(\"id\", col(\"id\").cast(\"int\")).withColumn(\"src_hash\", hash(\"id\", \"name\"))\n",
    "trgt_df = spark.createDataFrame([\n",
    "    (1, \"John\", \"N\", \"2025-01-01\", \"2025-01-02\", 199793157),\n",
    "    (1, \"Josh\", \"N\", \"2025-01-02\", \"2025-01-03\", -850647380),\n",
    "    (1, \"Joshua\", \"Y\", \"2025-01-03\", \"9999-12-31\", -1131722944)\n",
    "], [\"id\", \"name\", \"is_active\", \"eff_start_dt\", \"eff_end_dt\", \"trgt_hash\"])\n",
    "\n",
    "src_df.show()\n",
    "trgt_df.show()\n",
    "\n",
    "### mark only active records to inactive first\n",
    "active_to_inactive = trgt_df.join(src_df, trgt_df[\"id\"] == src_df[\"id\"], \"left\")\\\n",
    "    .filter((col(\"src_hash\") != col(\"trgt_hash\")) & (col(\"is_active\") == \"Y\"))\\\n",
    "    .withColumn(\"is_active\", lit(\"N\")).withColumn(\"eff_end_dt\", col(\"src_dt\"))\\\n",
    "    .select(trgt_df[\"id\"], trgt_df[\"name\"], \"is_active\", \"eff_start_dt\", \"eff_end_dt\", \"trgt_hash\")\n",
    "active_to_inactive.show()\n",
    "\n",
    "active_for_existing = trgt_df.join(src_df, trgt_df[\"id\"] == src_df[\"id\"], \"left\")\\\n",
    "    .filter(col(\"src_hash\") != col(\"trgt_hash\"))\\\n",
    "    .select(src_df[\"id\"], src_df[\"name\"], lit(\"Y\").alias(\"is_active\"), col(\"src_dt\").alias(\"eff_start_dt\"), lit(\"9999-12-31\").alias(\"eff_end_dt\"), \"src_hash\")\\\n",
    "    .dropDuplicates()\n",
    "active_for_existing.show()\n",
    "\n",
    "new_rec = src_df.join(trgt_df, src_df[\"id\"]==trgt_df[\"id\"], \"left_anti\")\\\n",
    "    .select(src_df[\"id\"], src_df[\"name\"], lit(\"Y\").alias(\"is_active\"), col(\"src_dt\").alias(\"eff_start_dt\"), lit(\"9999-12-31\").alias(\"eff_end_dt\"), \"src_hash\")\n",
    "new_rec.show()\n",
    "\n",
    "final_data = active_to_inactive.union(active_for_existing).union(new_rec).union(trgt_df.join(src_df, trgt_df[\"id\"] == src_df[\"id\"], \"left\").filter(col(\"is_active\") == \"N\").select(trgt_df[\"id\"], trgt_df[\"name\"], \"is_active\", \"eff_start_dt\", \"eff_end_dt\", \"trgt_hash\"))\n",
    "final_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82e16560-1419-49eb-9708-a377f9d923b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+----------+\n| id|name|    src_dt|  src_hash|\n+---+----+----------+----------+\n|  1|Josh|2025-01-02|-850647380|\n+---+----+----------+----------+\n\n+---+----+----------+---------+\n| id|name|   trgt_dt|trgt_hash|\n+---+----+----------+---------+\n|  1|John|2025-01-01|199793157|\n+---+----+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Consider same data as above to implement SCD Type 1\n",
    "    1 - Find matching rec from src and trg, and fully overwrite existing rec with incoming src rec\n",
    "    2 - Use left_anti join to determine only new rec\n",
    "    3 - Union 1, 2 \n",
    "\"\"\" \n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "src_df = spark.createDataFrame([\n",
    "    (\"1\", \"Josh\",\"2025-01-02\")\n",
    "], [\"id\", \"name\", \"src_dt\"])\n",
    "\n",
    "trgt_df = spark.createDataFrame([\n",
    "    (\"1\", \"John\", \"2025-01-01\")\n",
    "], [\"id\", \"name\", \"trgt_dt\"])\n",
    "\n",
    "src_df = src_df.withColumn(\"id\", col(\"id\").cast(\"int\")).withColumn(\"src_hash\", hash(\"id\", \"name\"))\n",
    "trgt_df = trgt_df.withColumn(\"id\", col(\"id\").cast(\"int\")).withColumn(\"trgt_hash\", hash(\"id\", \"name\"))\n",
    "\n",
    "src_df.show()\n",
    "trgt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4de1c6d-46de-4849-ab01-ef4c8d7aca65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+----------+\n| id|name|    src_dt|  src_hash|\n+---+----+----------+----------+\n|  1|Josh|2025-01-02|-850647380|\n+---+----+----------+----------+\n\n+---+----+-------+--------+\n| id|name|trgt_dt|src_hash|\n+---+----+-------+--------+\n+---+----+-------+--------+\n\n+---+----+----------+----------+\n| id|name|    src_dt|  src_hash|\n+---+----+----------+----------+\n|  1|Josh|2025-01-02|-850647380|\n+---+----+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "overwrite_for_existing = trgt_df.join(src_df, trgt_df[\"id\"] == src_df[\"id\"], \"left\")\\\n",
    "    .filter(col(\"src_hash\") != col(\"trgt_hash\"))\\\n",
    "    .select(src_df[\"id\"], src_df[\"name\"], \"src_dt\" , \"src_hash\")\\\n",
    "    .dropDuplicates()\n",
    "overwrite_for_existing.show()\n",
    "\n",
    "new_rec = src_df.join(trgt_df, src_df[\"id\"]==trgt_df[\"id\"], \"left_anti\")\\\n",
    "    .select(src_df[\"id\"], src_df[\"name\"], col(\"src_dt\").alias(\"trgt_dt\"), \"src_hash\")\n",
    "new_rec.show()\n",
    "\n",
    "final_data = overwrite_for_existing.union(new_rec)\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9656138c-45aa-40eb-8c84-b7fe7b41e9c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+-----------+\n| id|  name|    src_dt|   src_hash|\n+---+------+----------+-----------+\n|  1|Joshua|2025-01-03|-1131722944|\n+---+------+----------+-----------+\n\n+---+----+----------+----------+\n| id|name|   trgt_dt| trgt_hash|\n+---+----+----------+----------+\n|  1|Josh|2025-01-02|-850647380|\n+---+----+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "src_df = spark.createDataFrame([\n",
    "    (\"1\", \"Joshua\",\"2025-01-03\")\n",
    "], [\"id\", \"name\", \"src_dt\"])\n",
    "src_df = src_df.withColumn(\"id\", col(\"id\").cast(\"int\")).withColumn(\"src_hash\", hash(\"id\", \"name\"))\n",
    "trgt_df = spark.createDataFrame([\n",
    "    (1, \"Josh\",  \"2025-01-02\", -850647380)\n",
    "], [\"id\", \"name\", \"trgt_dt\", \"trgt_hash\"])\n",
    "\n",
    "src_df.show()\n",
    "trgt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a5f4f37-e40a-4831-82a4-6791eef42da7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+-----------+\n| id|  name|    src_dt|   src_hash|\n+---+------+----------+-----------+\n|  1|Joshua|2025-01-03|-1131722944|\n+---+------+----------+-----------+\n\n+---+----+-------+--------+\n| id|name|trgt_dt|src_hash|\n+---+----+-------+--------+\n+---+----+-------+--------+\n\n+---+------+----------+-----------+\n| id|  name|    src_dt|   src_hash|\n+---+------+----------+-----------+\n|  1|Joshua|2025-01-03|-1131722944|\n+---+------+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "overwrite_for_existing = trgt_df.join(src_df, trgt_df[\"id\"] == src_df[\"id\"], \"left\")\\\n",
    "    .filter(col(\"src_hash\") != col(\"trgt_hash\"))\\\n",
    "    .select(src_df[\"id\"], src_df[\"name\"], \"src_dt\" , \"src_hash\")\\\n",
    "    .dropDuplicates()\n",
    "overwrite_for_existing.show()\n",
    "\n",
    "new_rec = src_df.join(trgt_df, src_df[\"id\"]==trgt_df[\"id\"], \"left_anti\")\\\n",
    "    .select(src_df[\"id\"], src_df[\"name\"], col(\"src_dt\").alias(\"trgt_dt\"), \"src_hash\")\n",
    "new_rec.show()\n",
    "\n",
    "final_data = overwrite_for_existing.union(new_rec)\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c4278bc-0486-4afb-a2aa-bae8f8acf228",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+-----------+\n| id| name|    src_dt|   src_hash|\n+---+-----+----------+-----------+\n|  2|Scott|2025-01-04|-2119877053|\n|  1| Jake|2025-01-04| -811822073|\n+---+-----+----------+-----------+\n\n+---+------+----------+-----------+\n| id|  name|   trgt_dt|  trgt_hash|\n+---+------+----------+-----------+\n|  1|Joshua|2025-01-03|-1131722944|\n+---+------+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "src_df = spark.createDataFrame([\n",
    "    (2, \"Scott\",\"2025-01-04\"),\n",
    "    (1, \"Jake\",\"2025-01-04\")\n",
    "], [\"id\", \"name\", \"src_dt\"])\n",
    "src_df = src_df.withColumn(\"id\", col(\"id\").cast(\"int\")).withColumn(\"src_hash\", hash(\"id\", \"name\"))\n",
    "trgt_df = spark.createDataFrame([\n",
    "    (1, \"Joshua\",  \"2025-01-03\", -1131722944)\n",
    "], [\"id\", \"name\", \"trgt_dt\", \"trgt_hash\"])\n",
    "\n",
    "src_df.show()\n",
    "trgt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c259b401-79c3-4181-a82c-457fa36a2cad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+----------+\n| id|name|    src_dt|  src_hash|\n+---+----+----------+----------+\n|  1|Jake|2025-01-04|-811822073|\n+---+----+----------+----------+\n\n+---+-----+----------+-----------+\n| id| name|   trgt_dt|   src_hash|\n+---+-----+----------+-----------+\n|  2|Scott|2025-01-04|-2119877053|\n+---+-----+----------+-----------+\n\n+---+-----+----------+-----------+\n| id| name|    src_dt|   src_hash|\n+---+-----+----------+-----------+\n|  1| Jake|2025-01-04| -811822073|\n|  2|Scott|2025-01-04|-2119877053|\n+---+-----+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "overwrite_for_existing = trgt_df.join(src_df, trgt_df[\"id\"] == src_df[\"id\"], \"left\")\\\n",
    "    .filter(col(\"src_hash\") != col(\"trgt_hash\"))\\\n",
    "    .select(src_df[\"id\"], src_df[\"name\"], \"src_dt\" , \"src_hash\")\\\n",
    "    .dropDuplicates()\n",
    "overwrite_for_existing.show()\n",
    "\n",
    "new_rec = src_df.join(trgt_df, src_df[\"id\"]==trgt_df[\"id\"], \"left_anti\")\\\n",
    "    .select(src_df[\"id\"], src_df[\"name\"], col(\"src_dt\").alias(\"trgt_dt\"), \"src_hash\")\n",
    "new_rec.show()\n",
    "\n",
    "final_data = overwrite_for_existing.union(new_rec)\n",
    "final_data.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "25 SCD 1 and 2 Fully Working Example PySpark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
