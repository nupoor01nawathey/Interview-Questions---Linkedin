/*
3057. Employees Project Allocation

Table: Project
+-------------+---------+
| Column Name | Type    |
+-------------+---------+
| project_id  | int   |
| employee_id | int   |
| workload    | int   |
+-------------+---------+
employee_id is the primary key (column with unique values) of this table.
employee_id is a foreign key (reference column) to Employee table.
Each row of this table indicates that the employee with employee_id is working on the project with project_id and the workload of the project.

Table: Employees
+------------------+---------+
| Column Name      | Type    |
+------------------+---------+
| employee_id    | int     |
| name           | varchar |
| team           | varchar |
+------------------+---------+
employee_id is the primary key (column with unique values) of this table.
Each row of this table contains information about one employee.
Write a solution to find the employees who are allocated to projects with a workload that exceeds the average workload of all employees for their respective teams

Return the result table ordered by employee_id, project_id in ascending order.
The result format is in the following example.

Example 1:

Input: 
Project table:
+-------------+-------------+----------+
| project_id  | employee_id | workload |
+-------------+-------------+----------+
| 1           | 1          |  45        |
| 1           | 2          |  90        | 
| 2           | 3          |  12        |
| 2           | 4          |  68        |
+-------------+-------------+----------+

Employees table:
+-------------+--------+------+
| employee_id | name   | team |
+-------------+--------+------+
| 1           | Khaled | A    |
| 2           | Ali    | B    |
| 3           | John   | B    |
| 4           | Doe    | A    |
+-------------+--------+------+
Output: 
+-------------+------------+---------------+--------------------+
| employee_id | project_id | employee_name | project_workload|
+-------------+-----------+--------------+--------------------+  
| 2           | 1          | Ali           | 90             | 
| 4           | 2          | Doe           | 68             | 
+-------------+------------+---------------+--------------------+
Explanation: 
- Employee with ID 1 has a project workload of 45 and belongs to Team A, where the average workload is 56.50. Since his project workload does not exceed the team's average workload, he will be excluded.
- Employee with ID 2 has a project workload of 90 and belongs to Team B, where the average workload is 51.00. Since his project workload does exceed the team's average workload, he will be included.
- Employee with ID 3 has a project workload of 12 and belongs to Team B, where the average workload is 51.00. Since his project workload does not exceed the team's average workload, he will be excluded.
- Employee with ID 4 has a project workload of 68 and belongs to Team A, where the average workload is 56.50. Since his project workload does exceed the team's average workload, he will be included.
Result table orderd by employee_id, project_id in ascending order.
*/


Create table Project (project_id int, employee_id int, workload int);
Create table Employees (employee_id int, name varchar(20), team varchar(20));

insert into Project values 
(1, 1, 45),
(1, 2, 90),
(2, 3, 12),
(2, 4, 68);

insert into Employees values 
(1, "Khaled", "A"),
(2, "Ali", "B"),
(3, "John", "B"),
(4, "Doe", "A");

with cte as (
SELECT
  p.*,
  e.name as employee_name,
  avg(p.workload) over(partition by e.team) avg_workload
FROM Project p LEFT JOIN Employees e
on p.employee_id = e.employee_id
)
select
  employee_id, project_id, employee_name, workload as project_workload
from cte 
where workload > avg_workload
order by employee_id, project_id
;



import org.apache.spark.sql.Row
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window

val dataProject = Seq(
  Row(1, 1, 45),
  Row(1, 2, 90),
  Row(2, 3, 12),
  Row(2, 4, 68)
)

val dataEmployees = Seq(
  Row(1, "Khaled", "A"),
  Row(2, "Ali", "B"),
  Row(3, "John", "B"),
  Row(4, "Doe", "A")
)

val schemaProject = StructType(List(
  StructField("project_id", IntegerType),
  StructField("employee_id", IntegerType),
  StructField("workload", IntegerType)
))

val schemaEmployees = StructType(List(
  StructField("employee_id", IntegerType),
  StructField("name", StringType),
  StructField("team", StringType)
))

val rddProject = spark.sparkContext.parallelize(dataProject)
val dfProject = spark.createDataFrame(rddProject, schemaProject)

val rddEmployees = spark.sparkContext.parallelize(dataEmployees)
val dfEmployees = spark.createDataFrame(rddEmployees, schemaEmployees)

dfProject.alias("p").join(dfEmployees.alias("e"), $"p.employee_id" === $"e.employee_id", "left"
).withColumn("avg_workload", avg($"workload").over(Window.partitionBy($"e.team"))
).filter($"workload" > $"avg_workload"
).withColumnRenamed("name", "employee_name"
).withColumnRenamed("workload", "project_workload"
).select("e.employee_id", "p.project_id", "employee_name", "project_workload"
).orderBy($"employee_id", $"project_id").show(false)

