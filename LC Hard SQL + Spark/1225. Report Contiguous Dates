/*
1225. Report Contiguous Dates

Table: Failed
+--------------+---------+
| Column Name  | Type    |
+--------------+---------+
| fail_date    | date    |
+--------------+---------+
Primary key for this table is fail_date.
Failed table contains the days of failed tasks.

Table: Succeeded
+--------------+---------+
| Column Name  | Type    |
+--------------+---------+
| success_date | date    |
+--------------+---------+
Primary key for this table is success_date.
Succeeded table contains the days of succeeded tasks.

A system is running one task every day. Every task is independent of the previous tasks. The tasks can fail or succeed.
Write an SQL query to generate a report of period_state for each continuous interval of days in the period from 2019-01-01 to 2019-12-31.
period_state is 'failed' if tasks in this interval failed or 'succeeded' if tasks in this interval succeeded. Interval of days are retrieved as start_date and end_date.
Order result by start_date.
The query result format is in the following example:

Failed table:
+-------------------+
| fail_date         |
+-------------------+
| 2018-12-28        |
| 2018-12-29        |
| 2019-01-04        |
| 2019-01-05        |
+-------------------+

Succeeded table:
+-------------------+
| success_date      |
+-------------------+
| 2018-12-30        |
| 2018-12-31        |
| 2019-01-01        |
| 2019-01-02        |
| 2019-01-03        |
| 2019-01-06        |
+-------------------+

Result table:
+--------------+--------------+--------------+
| period_state | start_date   | end_date     |
+--------------+--------------+--------------+
| succeeded    | 2019-01-01   | 2019-01-03   |
| failed       | 2019-01-04   | 2019-01-05   |
| succeeded    | 2019-01-06   | 2019-01-06   |
+--------------+--------------+--------------+

The report ignored the system state in 2018 as we care about the system in the period 2019-01-01 to 2019-12-31.
From 2019-01-01 to 2019-01-03 all tasks succeeded and the system state was "succeeded".
From 2019-01-04 to 2019-01-05 all tasks failed and system state was "failed".
From 2019-01-06 to 2019-01-06 all tasks succeeded and system state was "succeeded".

*/


create table Failed (fail_date date);
create table Succeeded (success_date date);

insert into Failed values 
("2018-12-28"),
("2018-12-29"),
("2019-01-04"),
("2019-01-05")
;

insert into Succeeded values 
("2018-12-30"),
("2018-12-31"),
("2019-01-01"),
("2019-01-02"),
("2019-01-03"),
("2019-01-06")
;

select * from Failed ;
select * from Succeeded ;

with combined_status as (
  select 'failed' as status, fail_date as dt from Failed
  union
  select 'succeeded' as status, success_date as dt from Succeeded
), ranked_dates as (
select 
  *,
  row_number() over(partition by status order by dt asc) as rn
from combined_status 
), grouped_dates as (
select
  *,
  date_sub(dt, interval rn day) as grp
from ranked_dates 
)
select
  status as period_state,
  min(dt) start_date,
  max(dt) end_date
from grouped_dates 
where dt between "2019-01-01" and "2019-12-31"
group by grp, status 
order by start_date 


import org.apache.spark.sql.Row
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window

val dataFailed = Seq(
  Row("2018-12-28"),
  Row("2018-12-29"),
  Row("2019-01-04"),
  Row("2019-01-05")
)

val dataSucceeded = Seq(
  Row("2018-12-30"),
  Row("2018-12-31"),
  Row("2019-01-01"),
  Row("2019-01-02"),
  Row("2019-01-03"),
  Row("2019-01-06")
)

val schemaFailed = StructType(List(
  StructField("fail_date", StringType)
))

val schemaSucceeded = StructType(List(
  StructField("success_date", StringType)
))

val rddFailed = spark.sparkContext.parallelize(dataFailed)
val dfFailed = spark.createDataFrame(rddFailed, schemaFailed)

val rddSucceeded = spark.sparkContext.parallelize(dataSucceeded)
val dfSucceeded = spark.createDataFrame(rddSucceeded, schemaSucceeded)

dfFailed.withColumn("status", lit("failed")
).withColumnRenamed("fail_date", "dt"
).union(dfSucceeded.withColumn("status", lit("succeeded"))
).withColumn("rn", row_number().over(Window.partitionBy($"status").orderBy($"dt"))
).withColumn("grp", date_sub(date_format($"dt", "yyyy-MM-dd"), $"rn")
).filter($"dt" >= "2019-01-01" && $"dt" <= "2019-12-31"
).groupBy($"grp", $"status").agg(min($"dt").alias("start_date"), max($"dt").alias("end_date")
).orderBy($"start_date"
).drop($"grp").show(false)


