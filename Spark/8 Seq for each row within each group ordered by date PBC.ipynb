{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29ea51f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- GroupID: string (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      "\n",
      "+-------+----------+\n",
      "|GroupID|Date      |\n",
      "+-------+----------+\n",
      "|A      |2023-01-01|\n",
      "|A      |2023-01-02|\n",
      "|B      |2023-01-01|\n",
      "|B      |2023-01-03|\n",
      "+-------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.to_date\n",
       "group_data1: org.apache.spark.sql.DataFrame = [GroupID: string, Date: string]\n",
       "group_data: org.apache.spark.sql.DataFrame = [GroupID: string, Date: date]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.to_date\n",
    "\n",
    "val group_data1 = Seq(\n",
    "    (\"A\",\"2023-01-01\"),\n",
    "    (\"A\",\"2023-01-02\"),\n",
    "    (\"B\",\"2023-01-01\"),\n",
    "    (\"B\",\"2023-01-03\")\n",
    ").toDF(\"GroupID\", \"Date\")\n",
    "\n",
    "val group_data = group_data1.withColumn(\"Date\", to_date($\"Date\", \"yyyy-MM-dd\"))\n",
    "\n",
    "group_data.printSchema()\n",
    "group_data.show(false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d16becaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---+\n",
      "|GroupID|Date      |rn |\n",
      "+-------+----------+---+\n",
      "|A      |2023-01-01|1  |\n",
      "|A      |2023-01-02|2  |\n",
      "|B      |2023-01-01|1  |\n",
      "|B      |2023-01-03|2  |\n",
      "+-------+----------+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions._\n",
       "window: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@3f47c3ce\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import org.apache.spark.sql.expressions._\n",
    "\n",
    "val window = Window.partitionBy($\"GroupID\").orderBy($\"Date\")\n",
    "\n",
    "group_data.withColumn(\"rn\", row_number().over(window)).show(false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd54d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
