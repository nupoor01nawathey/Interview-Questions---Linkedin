{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c84f112-19fc-4171-a124-c2defeca840b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+---------+\n| id|   name|salary|     dept|\n+---+-------+------+---------+\n|  1| manish| 50000|       IT|\n|  2| vikash| 60000|    sales|\n|  3|raushan| 70000|marketing|\n|  4| mukesh| 80000|       IT|\n|  5| pritam| 90000|    sales|\n|  6| nikita| 45000|marketing|\n|  7| ragini| 55000|marketing|\n|  8| rakesh|100000|       IT|\n|  9| aditya| 65000|       IT|\n| 10|  rahul| 50000|marketing|\n+---+-------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "emp_df = spark.createDataFrame(\n",
    "    [\n",
    "        (1,'manish',50000,'IT'),\n",
    "        (2,'vikash',60000,'sales'),\n",
    "        (3,'raushan',70000,'marketing'),\n",
    "        (4,'mukesh',80000,'IT'),\n",
    "        (5,'pritam',90000,'sales'),\n",
    "        (6,'nikita',45000,'marketing'),\n",
    "        (7,'ragini',55000,'marketing'),\n",
    "        (8,'rakesh',100000,'IT'),\n",
    "        (9,'aditya',65000,'IT'),\n",
    "        (10,'rahul',50000,'marketing')\n",
    "    ], [\"id\", \"name\", \"salary\", \"dept\"]\n",
    ")\n",
    "\n",
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c744207-c415-495d-b6a3-34b315b1263d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+\n|     dept|dept_wise_salary|\n+---------+----------------+\n|       IT|          295000|\n|    sales|          150000|\n|marketing|          220000|\n+---------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "emp_df.groupBy(\"dept\").agg(sum(\"salary\").alias(\"dept_wise_salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd6f517c-b967-4ea4-a084-853ee8a98c4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+---------+-------+\n| id|   name|salary|     dept|country|\n+---+-------+------+---------+-------+\n|  1| manish| 50000|       IT|  india|\n|  2| vikash| 60000|    sales|     us|\n|  3|raushan| 70000|marketing|  india|\n|  4| mukesh| 80000|       IT|     us|\n|  5| pritam| 90000|    sales|  india|\n|  6| nikita| 45000|marketing|     us|\n|  7| ragini| 55000|marketing|  india|\n|  8| rakesh|100000|       IT|     us|\n|  9| aditya| 65000|       IT|  india|\n| 10|  rahul| 50000|marketing|     us|\n+---+-------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "emp1_df = spark.createDataFrame(\n",
    "    [\n",
    "        (1,'manish',50000,'IT','india'),\n",
    "        (2,'vikash',60000,'sales','us'),\n",
    "        (3,'raushan',70000,'marketing','india'),\n",
    "        (4,'mukesh',80000,'IT','us'),\n",
    "        (5,'pritam',90000,'sales','india'),\n",
    "        (6,'nikita',45000,'marketing','us'),\n",
    "        (7,'ragini',55000,'marketing','india'),\n",
    "        (8,'rakesh',100000,'IT','us'),\n",
    "        (9,'aditya',65000,'IT','india'),\n",
    "        (10,'rahul',50000,'marketing','us')\n",
    "    ], [\"id\", \"name\", \"salary\", \"dept\", \"country\"]\n",
    ")\n",
    "\n",
    "emp1_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c4ab381-3872-438e-af93-cee28b25f6bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------------------------+\n|     dept|country|dept_country_total_salary|\n+---------+-------+-------------------------+\n|       IT|  india|                   115000|\n|    sales|     us|                    60000|\n|marketing|  india|                   125000|\n|    sales|  india|                    90000|\n|       IT|     us|                   180000|\n|marketing|     us|                    95000|\n+---------+-------+-------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "emp1_df.groupBy(\"dept\", \"country\").agg(sum(\"salary\").alias(\"dept_country_total_salary\")).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "17 GroupBy PySpark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}