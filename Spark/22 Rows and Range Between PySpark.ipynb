{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b05b70d-dddd-4824-a599-c7b0feb31302",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Find out the difference in sales of each product from their first month sales to latest sales.\n",
    "2. Send a mail to employee who all have not completed 8 hours in office when they come to office.\n",
    "3. Find out the performance of the sales based on last 3 months average.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "+----------+------------+----------+-------+\n",
    "|product_id|product_name|sales_date|  sales|\n",
    "+----------+------------+----------+-------+\n",
    "|         1|      iphone|01-01-2023|1500000| 1 -2 | unbounded preceding\n",
    "|         2|     samsung|01-01-2023|1100000| 2 -1 |\n",
    "|         3|     oneplus|01-01-2023|1100000| 3 0  || current row\n",
    "|         1|      iphone|01-02-2023|1300000| 4 1  |\n",
    "|         2|     samsung|01-02-2023|1120000| 5 2  | unbounded following\n",
    "|         3|     oneplus|01-02-2023|1120000| 6 3  |\n",
    "|         1|      iphone|01-03-2023|1600000|\n",
    "|         2|     samsung|01-03-2023|1080000| default is rowsBetween unbounded preceding and current row\n",
    "|         3|     oneplus|01-03-2023|1160000| this is why you don't get correct result for last function in a window\n",
    "|         1|      iphone|01-04-2023|1700000| use unbounded preceding and unbounded following so that entire\n",
    "|         2|     samsung|01-04-2023|1800000| window is considered to calc last \n",
    "|         3|     oneplus|01-04-2023|1170000|\n",
    "|         1|      iphone|01-05-2023|1200000|\n",
    "|         2|     samsung|01-05-2023| 980000|\n",
    "|         3|     oneplus|01-05-2023|1175000|\n",
    "|         1|      iphone|01-06-2023|1100000|\n",
    "|         2|     samsung|01-06-2023|1100000|\n",
    "|         3|     oneplus|01-06-2023|1200000|\n",
    "+----------+------------+----------+-------+\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28ef2fb7-3fd2-4216-8513-d8fa1795a46c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+\n|product_id|product_name|sales_date|  sales|\n+----------+------------+----------+-------+\n|         2|     samsung|01-01-1995|  11000|\n|         1|      iphone|01-02-2023|1300000|\n|         2|     samsung|01-02-2023|1120000|\n|         3|     oneplus|01-02-2023|1120000|\n|         1|      iphone|01-03-2023|1600000|\n|         2|     samsung|01-03-2023|1080000|\n|         3|     oneplus|01-03-2023|1160000|\n|         1|      iphone|01-01-2006|  15000|\n|         1|      iphone|01-04-2023|1700000|\n|         2|     samsung|01-04-2023|1800000|\n|         3|     oneplus|01-04-2023|1170000|\n|         1|      iphone|01-05-2023|1200000|\n|         2|     samsung|01-05-2023| 980000|\n|         3|     oneplus|01-05-2023|1175000|\n|         1|      iphone|01-06-2023|1100000|\n|         3|     oneplus|01-01-2010|  23000|\n|         2|     samsung|01-06-2023|1100000|\n|         3|     oneplus|01-06-2023|1200000|\n+----------+------------+----------+-------+\n\n+----------+------------+------------+\n|product_id|product_name|sales_differ|\n+----------+------------+------------+\n|         1|      iphone|     1085000|\n|         2|     samsung|     1089000|\n|         3|     oneplus|     1177000|\n+----------+------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Find out the difference in sales of each product from their first month sales to latest sales.\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "product_data = [\n",
    "(2,\"samsung\",\"01-01-1995\",11000),\n",
    "(1,\"iphone\",\"01-02-2023\",1300000),\n",
    "(2,\"samsung\",\"01-02-2023\",1120000),\n",
    "(3,\"oneplus\",\"01-02-2023\",1120000),\n",
    "(1,\"iphone\",\"01-03-2023\",1600000),\n",
    "(2,\"samsung\",\"01-03-2023\",1080000),\n",
    "(3,\"oneplus\",\"01-03-2023\",1160000),\n",
    "(1,\"iphone\",\"01-01-2006\",15000),\n",
    "(1,\"iphone\",\"01-04-2023\",1700000),\n",
    "(2,\"samsung\",\"01-04-2023\",1800000),\n",
    "(3,\"oneplus\",\"01-04-2023\",1170000),\n",
    "(1,\"iphone\",\"01-05-2023\",1200000),\n",
    "(2,\"samsung\",\"01-05-2023\",980000),\n",
    "(3,\"oneplus\",\"01-05-2023\",1175000),\n",
    "(1,\"iphone\",\"01-06-2023\",1100000),\n",
    "(3,\"oneplus\",\"01-01-2010\",23000),\n",
    "(2,\"samsung\",\"01-06-2023\",1100000),\n",
    "(3,\"oneplus\",\"01-06-2023\",1200000)\n",
    "]\n",
    "\n",
    "product_schema=[\"product_id\",\"product_name\",\"sales_date\",\"sales\"]\n",
    "\n",
    "product_df = spark.createDataFrame(data=product_data,schema=product_schema)\n",
    "product_df.show()\n",
    "\n",
    "product_df.withColumn(\"first_mnth_sales\", first(\"sales\").over(Window.partitionBy(\"product_id\").orderBy(\"sales_date\")))\\\n",
    "    .withColumn(\"last_mnth_sales\", last(\"sales\").over(Window.partitionBy(\"product_id\").orderBy(\"sales_date\").rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)))\\\n",
    "    .withColumn(\"first_last_mnth_diff\", col(\"last_mnth_sales\") - col(\"first_mnth_sales\"))\\\n",
    "    .groupBy(\"product_id\", \"product_name\").agg(first(\"first_last_mnth_diff\").alias(\"sales_differ\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "775ff63c-0868-4ba2-ac51-b23fb0306263",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+-----+\n| id|  name|      date| time|\n+---+------+----------+-----+\n|  1|manish|11-07-2023|10:20|\n|  1|manish|11-07-2023|11:20|\n|  2|rajesh|11-07-2023|11:20|\n|  1|manish|11-07-2023|11:50|\n|  2|rajesh|11-07-2023|13:20|\n|  1|manish|11-07-2023|19:20|\n|  2|rajesh|11-07-2023|17:20|\n|  1|manish|12-07-2023|10:32|\n|  1|manish|12-07-2023|12:20|\n|  3|vikash|12-07-2023|09:12|\n|  1|manish|12-07-2023|16:23|\n|  3|vikash|12-07-2023|18:08|\n+---+------+----------+-----+\n\n+---+------+----------+-----+-------------------+-------------------+-------------------+-----------------+\n|id |name  |date      |time |ts                 |login              |logout             |less_than_8hr_ofc|\n+---+------+----------+-----+-------------------+-------------------+-------------------+-----------------+\n|1  |manish|12-07-2023|10:32|2023-07-12 10:32:00|2023-07-12 10:32:00|2023-07-12 16:23:00|5.85             |\n|1  |manish|12-07-2023|12:20|2023-07-12 12:20:00|2023-07-12 10:32:00|2023-07-12 16:23:00|5.85             |\n|1  |manish|12-07-2023|16:23|2023-07-12 16:23:00|2023-07-12 10:32:00|2023-07-12 16:23:00|5.85             |\n|2  |rajesh|11-07-2023|11:20|2023-07-11 11:20:00|2023-07-11 11:20:00|2023-07-11 17:20:00|6.0              |\n|2  |rajesh|11-07-2023|13:20|2023-07-11 13:20:00|2023-07-11 11:20:00|2023-07-11 17:20:00|6.0              |\n|2  |rajesh|11-07-2023|17:20|2023-07-11 17:20:00|2023-07-11 11:20:00|2023-07-11 17:20:00|6.0              |\n+---+------+----------+-----+-------------------+-------------------+-------------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 2. Send a mail to employee who all have not completed 8 hours in office when they come to office.\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "emp_data = [(1,\"manish\",\"11-07-2023\",\"10:20\"),\n",
    "        (1,\"manish\",\"11-07-2023\",\"11:20\"),\n",
    "        (2,\"rajesh\",\"11-07-2023\",\"11:20\"),\n",
    "        (1,\"manish\",\"11-07-2023\",\"11:50\"),\n",
    "        (2,\"rajesh\",\"11-07-2023\",\"13:20\"),\n",
    "        (1,\"manish\",\"11-07-2023\",\"19:20\"),\n",
    "        (2,\"rajesh\",\"11-07-2023\",\"17:20\"),\n",
    "        (1,\"manish\",\"12-07-2023\",\"10:32\"),\n",
    "        (1,\"manish\",\"12-07-2023\",\"12:20\"),\n",
    "        (3,\"vikash\",\"12-07-2023\",\"09:12\"),\n",
    "        (1,\"manish\",\"12-07-2023\",\"16:23\"),\n",
    "        (3,\"vikash\",\"12-07-2023\",\"18:08\")]\n",
    "\n",
    "emp_schema = [\"id\", \"name\", \"date\", \"time\"]\n",
    "\n",
    "emp_df = spark.createDataFrame(data=emp_data, schema=emp_schema)\n",
    "emp_df.show()\n",
    "\n",
    "# Here while converting to timestamp need to provide input format only\n",
    "emp_df.withColumn(\"ts\", from_unixtime(unix_timestamp(expr(\"CONCAT(date, ' ',time)\"), \"dd-MM-yyyy HH:mm\")))\\\n",
    "    .withColumn(\"login\", first(\"ts\").over(Window.partitionBy(\"id\", \"date\").orderBy(\"ts\")))\\\n",
    "    .withColumn(\"logout\", last(\"ts\").over(Window.partitionBy(\"id\", \"date\").orderBy(\"ts\").rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)))\\\n",
    "    .withColumn(\"login\", to_timestamp(\"login\", \"yyyy-MM-dd HH:mm:ss\"))\\\n",
    "    .withColumn(\"logout\", to_timestamp(\"logout\", \"yyyy-MM-dd HH:mm:ss\"))\\\n",
    "    .withColumn(\"less_than_8hr_ofc\", (col(\"logout\").cast(\"long\")-col(\"login\").cast(\"long\"))/3600)\\\n",
    "    .filter(col(\"less_than_8hr_ofc\") < 8.0)\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7963f24-1620-4b2c-8cb0-d28e5212b1a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------+\n|product_id|product_name|sales_date|  sales|\n+----------+------------+----------+-------+\n|         1|      iphone|01-01-2023|1500000|\n|         2|     samsung|01-01-2023|1100000|\n|         3|     oneplus|01-01-2023|1100000|\n|         1|      iphone|01-02-2023|1300000|\n|         2|     samsung|01-02-2023|1120000|\n|         3|     oneplus|01-02-2023|1120000|\n|         1|      iphone|01-03-2023|1600000|\n|         2|     samsung|01-03-2023|1080000|\n|         3|     oneplus|01-03-2023|1160000|\n|         1|      iphone|01-04-2023|1700000|\n|         2|     samsung|01-04-2023|1800000|\n|         3|     oneplus|01-04-2023|1170000|\n|         1|      iphone|01-05-2023|1200000|\n|         2|     samsung|01-05-2023| 980000|\n|         3|     oneplus|01-05-2023|1175000|\n|         1|      iphone|01-06-2023|1100000|\n|         2|     samsung|01-06-2023|1100000|\n|         3|     oneplus|01-06-2023|1200000|\n+----------+------------+----------+-------+\n\n+----------+------------+----------+-------+-----------+---------+\n|product_id|product_name|sales_date|  sales|running_sum|avg_sales|\n+----------+------------+----------+-------+-----------+---------+\n|         1|      iphone|01-03-2023|1600000|    4400000|533333.33|\n|         1|      iphone|01-04-2023|1700000|    4600000|566666.67|\n|         1|      iphone|01-05-2023|1200000|    4500000| 400000.0|\n|         1|      iphone|01-06-2023|1100000|    4000000|366666.67|\n|         2|     samsung|01-03-2023|1080000|    3300000| 360000.0|\n|         2|     samsung|01-04-2023|1800000|    4000000| 600000.0|\n|         2|     samsung|01-05-2023| 980000|    3860000|326666.67|\n|         2|     samsung|01-06-2023|1100000|    3880000|366666.67|\n|         3|     oneplus|01-03-2023|1160000|    3380000|386666.67|\n|         3|     oneplus|01-04-2023|1170000|    3450000| 390000.0|\n|         3|     oneplus|01-05-2023|1175000|    3505000|391666.67|\n|         3|     oneplus|01-06-2023|1200000|    3545000| 400000.0|\n+----------+------------+----------+-------+-----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# 3. Find out the performance of the sales based on last 3 months average.\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "product_data = [\n",
    "(1,\"iphone\",\"01-01-2023\",1500000),\n",
    "(2,\"samsung\",\"01-01-2023\",1100000),\n",
    "(3,\"oneplus\",\"01-01-2023\",1100000),\n",
    "(1,\"iphone\",\"01-02-2023\",1300000),\n",
    "(2,\"samsung\",\"01-02-2023\",1120000),\n",
    "(3,\"oneplus\",\"01-02-2023\",1120000),\n",
    "(1,\"iphone\",\"01-03-2023\",1600000),\n",
    "(2,\"samsung\",\"01-03-2023\",1080000),\n",
    "(3,\"oneplus\",\"01-03-2023\",1160000),\n",
    "(1,\"iphone\",\"01-04-2023\",1700000),\n",
    "(2,\"samsung\",\"01-04-2023\",1800000),\n",
    "(3,\"oneplus\",\"01-04-2023\",1170000),\n",
    "(1,\"iphone\",\"01-05-2023\",1200000),\n",
    "(2,\"samsung\",\"01-05-2023\",980000),\n",
    "(3,\"oneplus\",\"01-05-2023\",1175000),\n",
    "(1,\"iphone\",\"01-06-2023\",1100000),\n",
    "(2,\"samsung\",\"01-06-2023\",1100000),\n",
    "(3,\"oneplus\",\"01-06-2023\",1200000)\n",
    "]\n",
    "\n",
    "product_schema=[\"product_id\",\"product_name\",\"sales_date\",\"sales\"]\n",
    "\n",
    "product_df = spark.createDataFrame(data=product_data,schema=product_schema)\n",
    "product_df.show()\n",
    "\n",
    "product_df.withColumn(\"running_sum\", sum(\"sales\").over(Window.partitionBy(\"product_id\").orderBy(\"sales_date\").rowsBetween(-2,0)))\\\n",
    "    .withColumn(\"rn\", row_number().over(Window.partitionBy(\"product_id\").orderBy(\"sales_date\")))\\\n",
    "    .filter(col(\"rn\")>2)\\\n",
    "    .withColumn(\"avg_sales\", round(col(\"sales\")/3.0, 2))\\\n",
    "    .drop(\"rn\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "22 Rows and Range Between PySpark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}