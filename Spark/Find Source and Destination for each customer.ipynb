{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb6a9496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+------------+\n",
      "|customer|start_location|end_location|\n",
      "+--------+--------------+------------+\n",
      "|      c1|      New York|        Lima|\n",
      "|      c1|        London|    New York|\n",
      "|      c1|          Lima|   Sao Paulo|\n",
      "|      c1|     Sao Paulo|   New Delhi|\n",
      "|      c2|        Mumbai|   Hyderabad|\n",
      "|      c2|         Surat|        Pune|\n",
      "|      c2|     Hyderabad|       Surat|\n",
      "|      c3|         Kochi|     Kurnool|\n",
      "|      c3|       Lucknow|        Agra|\n",
      "|      c3|          Agra|      Jaipur|\n",
      "|      c3|        Jaipur|       Kochi|\n",
      "+--------+--------------+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.Row\n",
       "import org.apache.spark.sql._\n",
       "import spark.implicits._\n",
       "data: Seq[org.apache.spark.sql.Row] = List([c1,New York,Lima], [c1,London,New York], [c1,Lima,Sao Paulo], [c1,Sao Paulo,New Delhi], [c2,Mumbai,Hyderabad], [c2,Surat,Pune], [c2,Hyderabad,Surat], [c3,Kochi,Kurnool], [c3,Lucknow,Agra], [c3,Agra,Jaipur], [c3,Jaipur,Kochi])\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(customer,StringType,true),StructField(start_location,StringType,true),StructField(end_location,StringType,true))\n",
       "rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = ParallelCollectionRDD[86] at parallelize at <console>:66\n",
       "df: org.apache.spark.sql.DataFrame = [customer: string, start_location: string ... 1 more field]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql._\n",
    "import spark.implicits._\n",
    "\n",
    "\n",
    "val data = Seq(\n",
    "    Row(\"c1\", \"New York\", \"Lima\"),\n",
    "    Row(\"c1\", \"London\", \"New York\"),\n",
    "    Row(\"c1\", \"Lima\", \"Sao Paulo\"),\n",
    "    Row(\"c1\", \"Sao Paulo\", \"New Delhi\"),\n",
    "    Row(\"c2\", \"Mumbai\", \"Hyderabad\"),\n",
    "    Row(\"c2\", \"Surat\", \"Pune\"),\n",
    "    Row(\"c2\", \"Hyderabad\", \"Surat\"),\n",
    "    Row(\"c3\", \"Kochi\", \"Kurnool\"),\n",
    "    Row(\"c3\", \"Lucknow\", \"Agra\"),\n",
    "    Row(\"c3\", \"Agra\", \"Jaipur\"),\n",
    "    Row(\"c3\", \"Jaipur\", \"Kochi\")) \n",
    "\n",
    "val schema = StructType(Array(\n",
    "    StructField(\"customer\", StringType),\n",
    "    StructField(\"start_location\", StringType),\n",
    "    StructField(\"end_location\", StringType)\n",
    "))\n",
    "\n",
    "val rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "val df = spark.createDataFrame(rdd, schema)\n",
    "\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eac184eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|customer|location |\n",
      "+--------+---------+\n",
      "|c1      |New York |\n",
      "|c1      |London   |\n",
      "|c1      |Lima     |\n",
      "|c1      |Sao Paulo|\n",
      "|c2      |Mumbai   |\n",
      "|c2      |Surat    |\n",
      "|c2      |Hyderabad|\n",
      "|c3      |Kochi    |\n",
      "|c3      |Lucknow  |\n",
      "|c3      |Agra     |\n",
      "|c3      |Jaipur   |\n",
      "+--------+---------+\n",
      "\n",
      "+--------+---------+\n",
      "|customer|location |\n",
      "+--------+---------+\n",
      "|c1      |Lima     |\n",
      "|c1      |New York |\n",
      "|c1      |Sao Paulo|\n",
      "|c1      |New Delhi|\n",
      "|c2      |Hyderabad|\n",
      "|c2      |Pune     |\n",
      "|c2      |Surat    |\n",
      "|c3      |Kurnool  |\n",
      "|c3      |Agra     |\n",
      "|c3      |Jaipur   |\n",
      "|c3      |Kochi    |\n",
      "+--------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_start: org.apache.spark.sql.DataFrame = [customer: string, location: string]\n",
       "df_end: org.apache.spark.sql.DataFrame = [customer: string, location: string]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_start = df.select($\"customer\", $\"start_location\").withColumnRenamed(\"start_location\",\"location\")\n",
    "val df_end = df.select($\"customer\", $\"end_location\").withColumnRenamed(\"end_location\",\"location\")\n",
    "\n",
    "df_start.show(false)\n",
    "df_end.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e415305e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|customer|location |\n",
      "+--------+---------+\n",
      "|c1      |New York |\n",
      "|c1      |London   |\n",
      "|c1      |Lima     |\n",
      "|c1      |Sao Paulo|\n",
      "|c2      |Mumbai   |\n",
      "|c2      |Surat    |\n",
      "|c2      |Hyderabad|\n",
      "|c3      |Kochi    |\n",
      "|c3      |Lucknow  |\n",
      "|c3      |Agra     |\n",
      "|c3      |Jaipur   |\n",
      "|c1      |Lima     |\n",
      "|c1      |New York |\n",
      "|c1      |Sao Paulo|\n",
      "|c1      |New Delhi|\n",
      "|c2      |Hyderabad|\n",
      "|c2      |Pune     |\n",
      "|c2      |Surat    |\n",
      "|c3      |Kurnool  |\n",
      "|c3      |Agra     |\n",
      "+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_combined: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [customer: string, location: string]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_combined = df_start.union(df_end)\n",
    "\n",
    "df_combined.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cdc87d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|customer|location |\n",
      "+--------+---------+\n",
      "|c1      |London   |\n",
      "|c1      |New Delhi|\n",
      "|c2      |Mumbai   |\n",
      "|c2      |Pune     |\n",
      "|c3      |Lucknow  |\n",
      "|c3      |Kurnool  |\n",
      "+--------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "location_df: org.apache.spark.sql.DataFrame = [customer: string, location: string]\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val location_df = df_combined.groupBy($\"customer\", $\"location\"\n",
    "                                     ).agg(count(\"*\").as(\"cn\")\n",
    "                                     ).where($\"cn\"===1).orderBy($\"customer\").drop(\"cn\")\n",
    "\n",
    "location_df.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15ea5128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+------------+---------+\n",
      "|customer|start_location|end_location|location |\n",
      "+--------+--------------+------------+---------+\n",
      "|c1      |London        |New York    |London   |\n",
      "|c1      |Sao Paulo     |New Delhi   |New Delhi|\n",
      "|c2      |Mumbai        |Hyderabad   |Mumbai   |\n",
      "|c2      |Surat         |Pune        |Pune     |\n",
      "|c3      |Kochi         |Kurnool     |Kurnool  |\n",
      "|c3      |Lucknow       |Agra        |Lucknow  |\n",
      "+--------+--------------+------------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "final_df: org.apache.spark.sql.DataFrame = [customer: string, start_location: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val final_df = df.as(\"a\").join(location_df.as(\"b\"), \n",
    "                $\"a.customer\" === $\"b.customer\" && \n",
    "                $\"a.start_location\" === $\"b.location\" || \n",
    "                $\"a.end_location\" === $\"b.location\",\n",
    "                \"inner\"\n",
    "                ).drop($\"b.customer\")\n",
    "\n",
    "final_df.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eb32ba1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+------------+\n",
      "|customer|start_location|end_location|\n",
      "+--------+--------------+------------+\n",
      "|c1      |London        |New Delhi   |\n",
      "|c2      |Mumbai        |Pune        |\n",
      "|c3      |Lucknow       |Kurnool     |\n",
      "+--------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.withColumn(\"start_loc\", when($\"start_location\" === $\"location\", $\"location\")\n",
    "        ).withColumn(\"end_loc\", when($\"end_location\" === $\"location\", $\"location\")\n",
    "        ).groupBy($\"customer\").agg(min(\"start_loc\").as(\"start_location\"), min(\"end_loc\").as(\"end_location\")\n",
    "        ).show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30878ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
