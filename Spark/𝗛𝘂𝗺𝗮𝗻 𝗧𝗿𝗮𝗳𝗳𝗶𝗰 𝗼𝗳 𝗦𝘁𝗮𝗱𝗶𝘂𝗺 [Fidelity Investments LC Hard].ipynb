{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "628a7c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+\n",
      "|id |visit_date|people|\n",
      "+---+----------+------+\n",
      "|1  |2017-01-01|10    |\n",
      "|2  |2017-01-02|109   |\n",
      "|3  |2017-01-03|150   |\n",
      "|4  |2017-01-04|99    |\n",
      "|5  |2017-01-05|145   |\n",
      "|6  |2017-01-06|1455  |\n",
      "|7  |2017-01-07|199   |\n",
      "|8  |2017-01-09|188   |\n",
      "+---+----------+------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- visit_date: date (nullable = true)\n",
      " |-- people: integer (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "visitors: org.apache.spark.sql.DataFrame = [id: int, visit_date: date ... 1 more field]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// This question was asked in Fidelity Investments interview for Data Engineer (Senior role).\n",
    "\n",
    "// It was a LeetCode SQL problem (Hard level) -- 𝗛𝘂𝗺𝗮𝗻 𝗧𝗿𝗮𝗳𝗳𝗶𝗰 𝗼𝗳 𝗦𝘁𝗮𝗱𝗶𝘂𝗺.\n",
    "\n",
    "// ❓ Display the records with three or more rows with consecutive id's, and the number of people is greater than or equal to 100 for each.\n",
    "\n",
    "// 👉 Table Column - id (int), visit_date (date), people (int)\n",
    "\n",
    "// 𝗜𝗻𝗽𝘂𝘁:\n",
    "// +------+------------+-----------+\n",
    "// | id   | visit_date | people    |\n",
    "// +------+------------+-----------+\n",
    "// | 1    | 2017-01-01 | 10        |\n",
    "// | 2    | 2017-01-02 | 109       |\n",
    "// | 3    | 2017-01-03 | 150       |\n",
    "// | 4    | 2017-01-04 | 99        |\n",
    "// | 5    | 2017-01-05 | 145       |\n",
    "// | 6    | 2017-01-06 | 1455      |\n",
    "// | 7    | 2017-01-07 | 199       |\n",
    "// | 8    | 2017-01-09 | 188       |\n",
    "// +------+------------+-----------+\n",
    "\n",
    "\n",
    "// 𝗢𝘂𝘁𝗽𝘂𝘁:\n",
    "// +------+------------+-----------+\n",
    "// | id   |visit_date  |   people  |\n",
    "// +------+------------+-----------+\n",
    "// | 5    | 2017-01-05 |   145     |\n",
    "// | 6    | 2017-01-06 |   1455    |\n",
    "// | 7    | 2017-01-07 |   199     |\n",
    "// | 8    | 2017-01-09 |   188     |\n",
    "// +------+------------+-----------+\n",
    "\n",
    "\n",
    "val visitors=Seq(\n",
    "    (1,\"2017-01-01\",10), \n",
    "    (2,\"2017-01-02\",109), \n",
    "    (3,\"2017-01-03\",150), \n",
    "    (4,\"2017-01-04\",99), \n",
    "    (5,\"2017-01-05\",145), \n",
    "    (6,\"2017-01-06\",1455), \n",
    "    (7,\"2017-01-07\",199), \n",
    "    (8,\"2017-01-09\",188)\n",
    ").toDF(\"id\",\"visit_date\",\"people\").withColumn(\"visit_date\", to_date($\"visit_date\", \"yyyy-MM-dd\"))\n",
    "\n",
    "visitors.show(false)\n",
    "visitors.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23588776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+----+------+\n",
      "|id |visit_date|people|diff|counts|\n",
      "+---+----------+------+----+------+\n",
      "|5  |2017-01-05|145   |2   |4     |\n",
      "|6  |2017-01-06|1455  |2   |4     |\n",
      "|7  |2017-01-07|199   |2   |4     |\n",
      "|8  |2017-01-09|188   |2   |4     |\n",
      "+---+----------+------+----+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [id: int, visit_date: date ... 3 more fields]\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visitors.createOrReplaceTempView(\"visitors_tbl\")\n",
    "\n",
    "\n",
    "// +---+----------+------+---+----+-------+\n",
    "// |id |visit_date|people|rn |diff|counts |\n",
    "// +---+----------+------+---+----+-------+\n",
    "// |2  |2017-01-02|109   |1  |1   |2      |\n",
    "// |3  |2017-01-03|150   |2  |1   |2      |\n",
    "// |5  |2017-01-05|145   |3  |2   |4      |\n",
    "// |6  |2017-01-06|1455  |4  |2   |4      |\n",
    "// |7  |2017-01-07|199   |5  |2   |4      |\n",
    "// |8  |2017-01-09|188   |6  |2   |4      |\n",
    "// +---+----------+------+---+----+-------+\n",
    "\n",
    "val data = spark.sql(\"\"\"\n",
    "WITH CTE AS (\n",
    "    SELECT\n",
    "        *,\n",
    "        id-row_number() over(order by visit_date) diff\n",
    "    FROM visitors_tbl\n",
    "    WHERE people>=100\n",
    "),\n",
    "CTE2 AS (\n",
    "SELECT \n",
    "    *,\n",
    "    COUNT(id) over(partition by diff order by diff) counts \n",
    "FROM CTE\n",
    ")\n",
    "SELECT * FROM CTE2 WHERE counts >= 3\n",
    "\"\"\")\n",
    "\n",
    "data.show(false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e209fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+\n",
      "|id |visit_date|people|\n",
      "+---+----------+------+\n",
      "|5  |2017-01-05|145   |\n",
      "|6  |2017-01-06|1455  |\n",
      "|7  |2017-01-07|199   |\n",
      "|8  |2017-01-09|188   |\n",
      "+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.drop(\"diff\",\"rnk\").show(false)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
