{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ec73de2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+--------+---------+-----------+\n",
      "|left_id|left_key|left_value|right_id|right_key|right_value|\n",
      "+-------+--------+----------+--------+---------+-----------+\n",
      "|1      |test    |value     |1       |test     |value      |\n",
      "|2      |test    |value3    |2       |test2    |value4     |\n",
      "+-------+--------+----------+--------+---------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "import org.apache.spark.sql.DataFrame\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.catalyst.encoders.RowEncoder\n",
       "import org.apache.spark.sql.Row\n",
       "renameColumns: (df: org.apache.spark.sql.DataFrame, prefix: String)org.apache.spark.sql.DataFrame\n",
       "seq1: org.apache.spark.sql.DataFrame = [id: string, key: string ... 1 more field]\n",
       "df1: org.apache.spark.sql.DataFrame = [left_id: string, left_key: string ... 1 more field]\n",
       "seq2: org.apache.spark.sql.DataFrame = [id: string, key: string ... 1 more field]\n",
       "df2: org.apache.spark.sql.DataFrame = [right_id: string, right_key: string ... 1 more field]\n",
       "joined: org.apache.spark.sql.DataFrame = [left_id: string, left_key: string ... 4 more fields]\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.catalyst.encoders.RowEncoder\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "\n",
    "def renameColumns(df: DataFrame, prefix:String) = {\n",
    "  val cols= df.columns.map(t=> col(t).as(prefix + \"_\" + t))\n",
    "  df.select(cols: _*)\n",
    "}\n",
    "\n",
    "val seq1 = Seq((\"1\", \"test\", \"value\"), (\"2\", \"test\", \"value3\")).toDF(\"id\", \"key\", \"value\")\n",
    "val df1 = renameColumns(seq1, \"left\")\n",
    "\n",
    "val seq2 = Seq((\"1\", \"test\", \"value\"), (\"2\", \"test2\", \"value4\")).toDF(\"id\", \"key\", \"value\")\n",
    "val df2 = renameColumns(seq2, \"right\")\n",
    "\n",
    "val joined = df1.join(df2, df1.col(\"left_id\") === df2.col(\"right_id\"), \"outer\")\n",
    "\n",
    "joined.show(false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8ecf312d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+--------+---------+-----------+\n",
      "|left_id|left_key|left_value|right_id|right_key|right_value|\n",
      "+-------+--------+----------+--------+---------+-----------+\n",
      "|       |        |          |        |         |           |\n",
      "|       |test    |value3    |        |test2    |value4     |\n",
      "+-------+--------+----------+--------+---------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "enc: org.apache.spark.sql.catalyst.encoders.ExpressionEncoder[org.apache.spark.sql.Row] = class[left_id[0]: string, left_key[0]: string, left_value[0]: string, right_id[0]: string, right_key[0]: string, right_value[0]: string]\n",
       "diff: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [left_id: string, left_key: string ... 4 more fields]\n"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit val enc = RowEncoder(joined.schema)\n",
    "\n",
    "val diff = joined.map(eachRow => {\n",
    "  val values = eachRow.getValuesMap(eachRow.schema.fieldNames)\n",
    "  val left: Map[String, Any] = values.filterKeys(t => t.contains(\"left\"))\n",
    "  val right: Map[String, Any] = values.filterKeys(t => t.contains(\"right\"))\n",
    "\n",
    "  val row = eachRow.schema.fieldNames.map(fieldName => {\n",
    "    if (fieldName.contains(\"left_\")) {\n",
    "      val leftVal = left.getOrElse(fieldName, \"\")\n",
    "      val rightVal = right.getOrElse(\"right_\" + fieldName.stripPrefix(\"left_\"), \"\")\n",
    "      if (leftVal == rightVal) \"\" else leftVal\n",
    "    }\n",
    "    else {\n",
    "      val leftVal = left.getOrElse(\"left_\" + fieldName.stripPrefix(\"right_\"), \"\")\n",
    "      val rightVal = right.getOrElse(fieldName, \"\")\n",
    "      if (leftVal == rightVal) \"\" else rightVal\n",
    "    }\n",
    "  })\n",
    "  // (eachRow.getAs[String](\"left_id\"), eachRow.getAs[String](\"right_id\"), eachRow.getAs[String](\"right_key\"))\n",
    "  Row.fromSeq(row)\n",
    "})\n",
    "\n",
    "diff.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f6115ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                            |\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "|{right_value -> null, left_key -> null, left_value -> null, right_key -> null, right_id -> null, left_id -> null}|\n",
      "|{right_value -> null, left_key -> null, left_value -> null, right_key -> null, right_id -> null, left_id -> null}|\n",
      "+-----------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined.map(t => {\n",
    "    val values = t.getValuesMap(t.schema.fieldNames)\n",
    "    values\n",
    "}).show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1bef20e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [left_id: string, left_key: string ... 4 more fields]\n",
       "res112: org.apache.spark.sql.Dataset[Array[String]] = [value: array<string>]\n"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = df1.join(df2, df1.col(\"left_id\") === df2.col(\"right_id\"), \"outer\")\n",
    "\n",
    "df.map(t => {\n",
    "    t.schema.fieldNames.map(t1 => {\n",
    "      t.getAs[String](0)\n",
    "  })\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7e056385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------+\n",
      "|value                                                            |\n",
      "+-----------------------------------------------------------------+\n",
      "|[left_id, left_key, left_value, right_id, right_key, right_value]|\n",
      "|[left_id, left_key, left_value, right_id, right_key, right_value]|\n",
      "+-----------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [left_id: string, left_key: string ... 4 more fields]\n"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = df1.join(df2, df1.col(\"left_id\") === df2.col(\"right_id\"), \"outer\")\n",
    "\n",
    "df.map(t => {\n",
    "    t.schema.fieldNames.map(t1 => {\n",
    "     t1\n",
    "  })\n",
    "}).show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3b7b02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
