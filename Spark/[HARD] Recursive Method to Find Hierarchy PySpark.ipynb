{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f63e7908-5190-45f4-b96a-f864c432830f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+\n|level_0|level_1|level_2|level_3|\n+-------+-------+-------+-------+\n|    801|   null|   null|   null|\n|   1016|    801|   null|   null|\n|   1003|    801|   null|   null|\n|   1019|    801|   null|   null|\n|   1010|   1003|    801|   null|\n|   1004|   1003|    801|   null|\n|   1001|   1003|    801|   null|\n|   1012|   1004|   1003|    801|\n|   1002|   1004|   1003|    801|\n|   1015|   1004|   1003|    801|\n|   1008|   1019|    801|   null|\n|   1006|   1019|    801|   null|\n|   1014|   1019|    801|   null|\n|   1011|   1019|    801|   null|\n+-------+-------+-------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Employee DF\n",
    "schema = 'EMPLOYEE_NUMBER int, MANAGER_EMPLOYEE_NUMBER int'\n",
    "employees = spark.createDataFrame(\n",
    "[[801,None], \n",
    "[1016,801], \n",
    "[1003,801], \n",
    "[1019,801], \n",
    "[1010,1003], \n",
    "[1004,1003], \n",
    "[1001,1003],\n",
    "[1012,1004], \n",
    "[1002,1004], \n",
    "[1015,1004], \n",
    "[1008,1019], \n",
    "[1006,1019], \n",
    "[1014,1019],\n",
    "[1011,1019]], schema=schema)\n",
    "\n",
    "# initial DataFrame\n",
    "empDF = employees \\\n",
    "    .withColumnRenamed('EMPLOYEE_NUMBER', 'level_0') \\\n",
    "    .withColumnRenamed('MANAGER_EMPLOYEE_NUMBER', 'level_1')\n",
    "\n",
    "i = 1\n",
    "\n",
    "# Loop Through if you dont know recusrsive depth\n",
    "while True:\n",
    "    this_level = 'level_{}'.format(i)\n",
    "    next_level = 'level_{}'.format(i+1)\n",
    "\n",
    "    \"\"\"\n",
    "        print(this_level, next_level)\n",
    "        level_1 level_2\n",
    "        level_2 level_3\n",
    "        level_3 level_4\n",
    "    \"\"\"\n",
    "\n",
    "    emp_level = employees \\\n",
    "        .withColumnRenamed('EMPLOYEE_NUMBER', this_level) \\\n",
    "        .withColumnRenamed('MANAGER_EMPLOYEE_NUMBER', next_level)\n",
    "\n",
    "    empDF = empDF.join(emp_level, on=this_level, how='left')\n",
    "  \n",
    "    # Check if DF is empty. Break loop if empty, Otherwise continue with next level\n",
    "    if empDF.where(col(next_level).isNotNull()).count() == 0:\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "# Sort columns and show\n",
    "empDF.select('level_0', 'level_1', 'level_2', 'level_3').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f960fd37-d1c1-4516-ac50-ccfcdcfcca8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n|parent|child|\n+------+-----+\n|    C1|   C2|\n|    C2|   C3|\n|    C2|   C5|\n|    C4|   C6|\n+------+-----+\n\n+-------+-------+\n|level_1|level_0|\n+-------+-------+\n|     C1|     C2|\n|     C2|     C3|\n|     C2|     C5|\n|     C4|     C6|\n+-------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "subsidiary = spark.createDataFrame(\n",
    "    [\n",
    "        [\"C1\", \"C2\"],\n",
    "        [\"C2\", \"C3\"],\n",
    "        [\"C2\", \"C5\"],\n",
    "        [\"C4\", \"C6\"]\n",
    "    ], [\"parent\", \"child\"]\n",
    ")\n",
    "\n",
    "subsidiary.show()\n",
    "\n",
    "subsidiary_df = subsidiary.withColumnRenamed(\"child\", \"level_0\").withColumnRenamed(\"parent\", \"level_1\")\n",
    "subsidiary_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be4e54b7-5341-47a9-b7ff-27f7d1117499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+\n|level_0|level_1|level_2|\n+-------+-------+-------+\n|     C1|     C2|     C5|\n|     C1|     C2|     C3|\n|     C2|     C3|   null|\n|     C2|     C5|   null|\n|     C4|     C6|   null|\n+-------+-------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "https://www.youtube.com/watch?v=zxcLgJkR_LI\n",
    "\n",
    "-- Investments\n",
    "INSERT INTO Invested (pname, cname) VALUES\n",
    "('Don', 'C1'),\n",
    "('Don', 'C4'),\n",
    "('Ron', 'C1'),\n",
    "('Hil', 'C3');\n",
    "\n",
    "-- Subsidiary relationships\n",
    "INSERT INTO Subsidiary (parent, child) VALUES\n",
    "('C1', 'C2'),\n",
    "('C2', 'C3'),\n",
    "('C2', 'C5'),\n",
    "('C4', 'C6');\n",
    "\n",
    "select * from Invested ;\n",
    "select * from Subsidiary ;\n",
    "\n",
    "SQL SOlution but Recursive cte is not supported in Spark\n",
    "with recursive cte as (\n",
    "  select parent, child from Subsidiary\n",
    "  union all\n",
    "  select cte.parent, s.child\n",
    "  from cte inner join Subsidiary s \n",
    "  on cte.child = s.parent\n",
    ")\n",
    "select p.pname, c.pname from cte \n",
    "inner join Invested p \n",
    "on cte.parent = p.cname\n",
    "inner join Invested c\n",
    "on cte.child = c.cname ;\n",
    " \n",
    "\n",
    "\"\"\"\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Employee DF\n",
    "schema = 'EMPLOYEE_NUMBER string, MANAGER_EMPLOYEE_NUMBER string'\n",
    "employees = spark.createDataFrame(\n",
    "    [\n",
    "        [\"C1\", \"C2\"],\n",
    "        [\"C2\", \"C3\"],\n",
    "        [\"C2\", \"C5\"],\n",
    "        [\"C4\", \"C6\"]\n",
    "    ], schema=schema)\n",
    "\n",
    "# initial DataFrame\n",
    "empDF = employees \\\n",
    "    .withColumnRenamed('EMPLOYEE_NUMBER', 'level_0') \\\n",
    "    .withColumnRenamed('MANAGER_EMPLOYEE_NUMBER', 'level_1')\n",
    "\n",
    "i = 1\n",
    "\n",
    "# Loop Through if you dont know recusrsive depth\n",
    "while True:\n",
    "    this_level = 'level_{}'.format(i)\n",
    "    next_level = 'level_{}'.format(i+1)\n",
    "\n",
    "    emp_level = employees \\\n",
    "        .withColumnRenamed('EMPLOYEE_NUMBER', this_level) \\\n",
    "        .withColumnRenamed('MANAGER_EMPLOYEE_NUMBER', next_level)\n",
    "\n",
    "    empDF = empDF.join(emp_level, on=this_level, how='left')\n",
    "  \n",
    "    # Check if DF is empty. Break loop if empty, Otherwise continue with next level\n",
    "    if empDF.where(col(next_level).isNotNull()).count() == 0:\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "# Sort columns and show\n",
    "empDF.select('level_0', 'level_1', 'level_2').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76425a45-32c9-4625-9e5e-30746a48d24d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n|level_0|level_1|\n+-------+-------+\n|     C1|     C2|\n|     C2|     C3|\n|     C2|     C5|\n|     C4|     C6|\n|     C1|     C5|\n|     C1|     C3|\n+-------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.createOrReplaceTempView(\"emp\")\n",
    "\n",
    "subsidiary_df = spark.sql(\"\"\"\n",
    "          with cte as (\n",
    "            select level_0, level_1 from emp\n",
    "            union\n",
    "            select level_0, level_2 from emp\n",
    "            )\n",
    "            select * from cte where level_1 is not null\n",
    "          \"\"\")\n",
    "          \n",
    "subsidiary_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8095b413-73a5-4f84-9b99-c0c3c583f442",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n|pname|cname|\n+-----+-----+\n|  Don|   C1|\n|  Don|   C4|\n|  Ron|   C1|\n|  Hil|   C3|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "investment_df = spark.createDataFrame(\n",
    "    [\n",
    "        ['Don', 'C1'],\n",
    "        ['Don', 'C4'],\n",
    "        ['Ron', 'C1'],\n",
    "        ['Hil', 'C3']\n",
    "    ],\n",
    "    [\"pname\", \"cname\"]\n",
    ")\n",
    "\n",
    "investment_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13c5b72c-46f5-4a00-8451-42574bbf60d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+\n|level_0|level_1|pname_i|cname_i|\n+-------+-------+-------+-------+\n|     C1|     C3|    Don|     C1|\n|     C1|     C5|    Don|     C1|\n|     C1|     C2|    Don|     C1|\n|     C4|     C6|    Don|     C4|\n|     C1|     C3|    Ron|     C1|\n|     C1|     C5|    Ron|     C1|\n|     C1|     C2|    Ron|     C1|\n+-------+-------+-------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df1 = subsidiary_df.join(investment_df, subsidiary_df[\"level_0\"] == investment_df[\"cname\"], \"inner\")\\\n",
    "    .withColumnRenamed(\"level_1\", \"level_1_i\").withColumnRenamed(\"pname\", \"pname_i\").withColumnRenamed(\"cname\", \"cname_i\")\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc5c9370-6cb2-45d3-8273-83d796c5f198",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n|cname|pname|\n+-----+-----+\n|  Don|  Hil|\n|  Ron|  Hil|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "subsidiary_df.createOrReplaceTempView(\"cte\")\n",
    "investment_df.createOrReplaceTempView(\"Invested\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "            select c.pname as cname, p.pname as pname \n",
    "            from cte \n",
    "            inner join Invested p \n",
    "            on cte.level_1 = p.cname\n",
    "            inner join Invested c\n",
    "            on cte.level_0 = c.cname\n",
    "          \"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "[HARD] Recursive Method to Find Hierarchy PySpark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}