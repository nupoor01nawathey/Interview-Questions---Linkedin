{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c05e2988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df1: org.apache.spark.sql.DataFrame = [id: int, df1_name: string ... 1 more field]\n",
       "df2: org.apache.spark.sql.DataFrame = [id: int, df2_name: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df1 = Seq((1,\"John_1\",\"IT_1\")).toDF(\"id\", \"df1_name\", \"df1_dept\")\n",
    "val df2 = Seq((1,\"John_2\",\"IT_2\", \"dummy_val\")).toDF(\"id\", \"df2_name\", \"df2_dept\", \"dummy_col\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0699447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df1_cols: Seq[String] = WrappedArray(id, df1_name, df1_dept)\n",
       "df2_cols: Seq[String] = WrappedArray(id, df2_name, df2_dept, dummy_col)\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df1_cols = df1.columns.toSeq\n",
    "val df2_cols = df2.columns.toSeq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "27be951f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "src_hashmap: scala.collection.immutable.Map[String,String] = Map(id -> id, df1_name -> df1_name, df1_dept -> df1_dept)\n",
       "trgt_hashmap: scala.collection.immutable.Map[String,String] = Map(id -> id, df2_name -> df2_name, df2_dept -> df2_dept, dummy_col -> dummy_col)\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val src_hashmap = df1.schema.fields.map( f => f.name -> f.name).toMap\n",
    "val trgt_hashmap = df2.schema.fields.map( f => f.name -> f.name).toMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9ed91612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "common_fields: List[String] = List(id)\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val common_fields = original_fields.filter( \n",
    "    f => original_fields_name_hashmap.get(f.name) == target_fields_name_hashmap.get(f.name)\n",
    ").map(f => f.name).toList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3a28a82d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res30: org.apache.spark.sql.DataFrame = [id: int]\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.select(common_fields.map(col): _*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d40a205c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res31: org.apache.spark.sql.DataFrame = [id: int]\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.select(common_fields.map(col): _*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6220ad7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
