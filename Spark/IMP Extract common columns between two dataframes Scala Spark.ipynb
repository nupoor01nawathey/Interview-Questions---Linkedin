{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c2b6baa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df1: org.apache.spark.sql.DataFrame = [id: int, fname: string ... 2 more fields]\n",
       "original_fields: Array[org.apache.spark.sql.types.StructField] = Array(StructField(id,IntegerType,false), StructField(fname,StringType,true), StructField(lname,StringType,true), StructField(version,DoubleType,false))\n",
       "original_fields_name_hashmap: scala.collection.immutable.Map[String,org.apache.spark.sql.types.DataType] = Map(id -> IntegerType, fname -> StringType, lname -> StringType, version -> DoubleType)\n",
       "df2: org.apache.spark.sql.DataFrame = [id: int, fname: string ... 3 more fields]\n",
       "target_fields: Array[org.apache.spark.sql.types.StructField] = Array(StructField(id,IntegerType,false), StructField(fname,StringType,true), StructField(version,DoubleType,false), StructField(lname,StringType,true), StructFi...\n"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df1 = Seq((1, \"John\", \"Mayer\",1.0)).toDF(\"id\", \"fname\", \"lname\", \"version\")\n",
    "val original_fields = df1.schema.fields\n",
    "val original_fields_name_hashmap = original_fields.map( f => f.name -> f.dataType).toMap\n",
    "\n",
    "val df2 = Seq((1, \"John\", 1.0, \"Mayer\", \"dummy_val\")).toDF(\"id\", \"fname\", \"version\", \"lname\", \"dummy_col2\")\n",
    "val target_fields = df2.schema.fields\n",
    "val target_fields_name_hashmap = target_fields.map( f => f.name -> f.dataType).toMap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "533676ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "common_fields: List[String] = List(id, fname, lname, version)\n"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val common_fields = original_fields.filter( \n",
    "    f => original_fields_name_hashmap.get(f.name) == target_fields_name_hashmap.get(f.name) \n",
    ").map(c => c.name).toList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ef418f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-------+\n",
      "|id |fname|lname|version|\n",
      "+---+-----+-----+-------+\n",
      "|1  |John |Mayer|1.0    |\n",
      "+---+-----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(common_fields.map(col): _*).show(false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "98096bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-------+\n",
      "|id |fname|lname|version|\n",
      "+---+-----+-----+-------+\n",
      "|1  |John |Mayer|1.0    |\n",
      "+---+-----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(common_fields.map(col): _*).show(false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a666c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
