{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9f8fada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|user_id|login_date|\n",
      "+-------+----------+\n",
      "|      1|2024-03-01|\n",
      "|      1|2024-03-02|\n",
      "|      1|2024-03-03|\n",
      "|      1|2024-03-04|\n",
      "|      1|2024-03-06|\n",
      "|      1|2024-03-10|\n",
      "|      1|2024-03-11|\n",
      "|      1|2024-03-12|\n",
      "|      1|2024-03-13|\n",
      "|      1|2024-03-14|\n",
      "|      1|2024-03-20|\n",
      "|      1|2024-03-25|\n",
      "|      1|2024-03-26|\n",
      "|      1|2024-03-27|\n",
      "|      1|2024-03-28|\n",
      "|      1|2024-03-29|\n",
      "|      1|2024-03-30|\n",
      "|      1|2024-03-31|\n",
      "|      1|2024-04-01|\n",
      "|      1|2024-04-02|\n",
      "+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.expressions._\n",
       "df: org.apache.spark.sql.DataFrame = [user_id: int, login_date: string]\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*\n",
    "    Identify dates where a user has logged in for 5 or more consecutive days, sorting based on user_id\n",
    "\n",
    "    USE DENSE_RANK() and CHECK LAST CELL AS datediff() excludes 1 day\n",
    "    \n",
    "*/\n",
    "\n",
    "\n",
    "\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.expressions._\n",
    "\n",
    "\n",
    "val df = Seq(\n",
    "    (1, \"2024-03-01\"),\n",
    "    (1, \"2024-03-02\"),\n",
    "    (1, \"2024-03-03\"),\n",
    "    (1, \"2024-03-04\"),\n",
    "    (1, \"2024-03-06\"),\n",
    "    (1, \"2024-03-10\"),\n",
    "    (1, \"2024-03-11\"),\n",
    "    (1, \"2024-03-12\"),\n",
    "    (1, \"2024-03-13\"),\n",
    "    (1, \"2024-03-14\"),\n",
    "    (1, \"2024-03-20\"),\n",
    "    (1, \"2024-03-25\"),\n",
    "    (1, \"2024-03-26\"),\n",
    "    (1, \"2024-03-27\"),\n",
    "    (1, \"2024-03-28\"),\n",
    "    (1, \"2024-03-29\"),\n",
    "    (1, \"2024-03-30\"),\n",
    "    (1, \"2024-03-31\"),\n",
    "    (1, \"2024-04-01\"),\n",
    "    (1, \"2024-04-02\"),\n",
    "    (2, \"2024-03-01\"),\n",
    "    (2, \"2024-03-02\"),\n",
    "    (2, \"2024-03-03\"),\n",
    "    (2, \"2024-03-04\"),\n",
    "    (3, \"2024-03-01\"),\n",
    "    (3, \"2024-03-02\"),\n",
    "    (3, \"2024-03-03\"),\n",
    "    (3, \"2024-03-04\"),\n",
    "    (3, \"2024-03-04\"),\n",
    "    (3, \"2024-03-04\"),\n",
    "    (3, \"2024-03-05\"),\n",
    "    (4, \"2024-03-01\"),\n",
    "    (4, \"2024-03-02\"),\n",
    "    (4, \"2024-03-03\"),\n",
    "    (4, \"2024-03-04\"),\n",
    "    (4, \"2024-03-04\")\n",
    ").toDF(\"user_id\", \"login_date\")\n",
    "\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49e79ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|user_id|login_date|\n",
      "+-------+----------+\n",
      "|1      |2024-03-01|\n",
      "|1      |2024-03-02|\n",
      "|1      |2024-03-03|\n",
      "|1      |2024-03-04|\n",
      "|1      |2024-03-06|\n",
      "|1      |2024-03-10|\n",
      "|1      |2024-03-11|\n",
      "|1      |2024-03-12|\n",
      "|1      |2024-03-13|\n",
      "|1      |2024-03-14|\n",
      "|1      |2024-03-20|\n",
      "|1      |2024-03-25|\n",
      "|1      |2024-03-26|\n",
      "|1      |2024-03-27|\n",
      "|1      |2024-03-28|\n",
      "|1      |2024-03-29|\n",
      "|1      |2024-03-30|\n",
      "|1      |2024-03-31|\n",
      "|1      |2024-04-01|\n",
      "|1      |2024-04-02|\n",
      "+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "formatted_date: org.apache.spark.sql.DataFrame = [user_id: int, login_date: date]\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val formatted_date = df.withColumn(\"login_date\", to_date($\"login_date\", \"yyyy-MM-dd\"))\n",
    "formatted_date.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "865a765d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---+---+\n",
      "|user_id|login_date|rn |dr |\n",
      "+-------+----------+---+---+\n",
      "|1      |2024-03-01|1  |1  |\n",
      "|1      |2024-03-02|2  |2  |\n",
      "|1      |2024-03-03|3  |3  |\n",
      "|1      |2024-03-04|4  |4  |\n",
      "|1      |2024-03-06|5  |5  |\n",
      "|1      |2024-03-10|6  |6  |\n",
      "|1      |2024-03-11|7  |7  |\n",
      "|1      |2024-03-12|8  |8  |\n",
      "|1      |2024-03-13|9  |9  |\n",
      "|1      |2024-03-14|10 |10 |\n",
      "|1      |2024-03-20|11 |11 |\n",
      "|1      |2024-03-25|12 |12 |\n",
      "|1      |2024-03-26|13 |13 |\n",
      "|1      |2024-03-27|14 |14 |\n",
      "|1      |2024-03-28|15 |15 |\n",
      "|1      |2024-03-29|16 |16 |\n",
      "|1      |2024-03-30|17 |17 |\n",
      "|1      |2024-03-31|18 |18 |\n",
      "|1      |2024-04-01|19 |19 |\n",
      "|1      |2024-04-02|20 |20 |\n",
      "|2      |2024-03-01|1  |1  |\n",
      "|2      |2024-03-02|2  |2  |\n",
      "|2      |2024-03-03|3  |3  |\n",
      "|2      |2024-03-04|4  |4  |\n",
      "|3      |2024-03-01|1  |1  |\n",
      "|3      |2024-03-02|2  |2  |\n",
      "|3      |2024-03-03|3  |3  |\n",
      "|3      |2024-03-04|4  |4  |\n",
      "|3      |2024-03-04|5  |4  |\n",
      "|3      |2024-03-04|6  |4  |\n",
      "|3      |2024-03-05|7  |5  |\n",
      "|4      |2024-03-01|1  |1  |\n",
      "|4      |2024-03-02|2  |2  |\n",
      "|4      |2024-03-03|3  |3  |\n",
      "|4      |2024-03-04|4  |4  |\n",
      "|4      |2024-03-04|5  |4  |\n",
      "+-------+----------+---+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dr_df: org.apache.spark.sql.DataFrame = [user_id: int, login_date: date ... 2 more fields]\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dr_df = formatted_date.withColumn(\"rn\", row_number().over(Window.partitionBy($\"user_id\").orderBy($\"login_date\"))\n",
    "               ).withColumn(\"dr\", dense_rank().over(Window.partitionBy($\"user_id\").orderBy($\"login_date\"))\n",
    "               )\n",
    "\n",
    "dr_df.show(50,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a70d5d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---+----------+\n",
      "|user_id|login_date|dr |group_    |\n",
      "+-------+----------+---+----------+\n",
      "|1      |2024-03-01|1  |2024-02-29|\n",
      "|1      |2024-03-02|2  |2024-02-29|\n",
      "|1      |2024-03-03|3  |2024-02-29|\n",
      "|1      |2024-03-04|4  |2024-02-29|\n",
      "|1      |2024-03-06|5  |2024-03-01|\n",
      "|1      |2024-03-10|6  |2024-03-04|\n",
      "|1      |2024-03-11|7  |2024-03-04|\n",
      "|1      |2024-03-12|8  |2024-03-04|\n",
      "|1      |2024-03-13|9  |2024-03-04|\n",
      "|1      |2024-03-14|10 |2024-03-04|\n",
      "|1      |2024-03-20|11 |2024-03-09|\n",
      "|1      |2024-03-25|12 |2024-03-13|\n",
      "|1      |2024-03-26|13 |2024-03-13|\n",
      "|1      |2024-03-27|14 |2024-03-13|\n",
      "|1      |2024-03-28|15 |2024-03-13|\n",
      "|1      |2024-03-29|16 |2024-03-13|\n",
      "|1      |2024-03-30|17 |2024-03-13|\n",
      "|1      |2024-03-31|18 |2024-03-13|\n",
      "|1      |2024-04-01|19 |2024-03-13|\n",
      "|1      |2024-04-02|20 |2024-03-13|\n",
      "|2      |2024-03-01|1  |2024-02-29|\n",
      "|2      |2024-03-02|2  |2024-02-29|\n",
      "|2      |2024-03-03|3  |2024-02-29|\n",
      "|2      |2024-03-04|4  |2024-02-29|\n",
      "|3      |2024-03-01|1  |2024-02-29|\n",
      "|3      |2024-03-02|2  |2024-02-29|\n",
      "|3      |2024-03-03|3  |2024-02-29|\n",
      "|3      |2024-03-04|4  |2024-02-29|\n",
      "|3      |2024-03-04|4  |2024-02-29|\n",
      "|3      |2024-03-04|4  |2024-02-29|\n",
      "|3      |2024-03-05|5  |2024-02-29|\n",
      "|4      |2024-03-01|1  |2024-02-29|\n",
      "|4      |2024-03-02|2  |2024-02-29|\n",
      "|4      |2024-03-03|3  |2024-02-29|\n",
      "|4      |2024-03-04|4  |2024-02-29|\n",
      "|4      |2024-03-04|4  |2024-02-29|\n",
      "+-------+----------+---+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "differ_dt: org.apache.spark.sql.DataFrame = [user_id: int, login_date: date ... 2 more fields]\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val differ_dt = dr_df.withColumn(\"group_\", $\"login_date\" - $\"dr\").drop(\"rn\")\n",
    "\n",
    "differ_dt.show(50,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a95231d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+----------+\n",
      "|user_id|group_    |start_date|end_date  |\n",
      "+-------+----------+----------+----------+\n",
      "|1      |2024-02-29|2024-03-01|2024-03-04|\n",
      "|1      |2024-03-01|2024-03-06|2024-03-06|\n",
      "|1      |2024-03-04|2024-03-10|2024-03-14|\n",
      "|1      |2024-03-09|2024-03-20|2024-03-20|\n",
      "|1      |2024-03-13|2024-03-25|2024-04-02|\n",
      "|2      |2024-02-29|2024-03-01|2024-03-04|\n",
      "|3      |2024-02-29|2024-03-01|2024-03-05|\n",
      "|4      |2024-02-29|2024-03-01|2024-03-04|\n",
      "+-------+----------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "aggr_dates: org.apache.spark.sql.DataFrame = [user_id: int, group_: date ... 2 more fields]\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val aggr_dates = differ_dt.groupBy($\"user_id\", $\"group_\").agg(min($\"login_date\").as(\"start_date\"), \n",
    "                                             max($\"login_date\").as(\"end_date\"))\n",
    "\n",
    "aggr_dates.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40fc3b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+----------+-------------------+\n",
      "|user_id|group_    |start_date|end_date  |consecutive_date_cn|\n",
      "+-------+----------+----------+----------+-------------------+\n",
      "|1      |2024-03-04|2024-03-10|2024-03-14|5                  |\n",
      "|1      |2024-03-13|2024-03-25|2024-04-02|9                  |\n",
      "|3      |2024-02-29|2024-03-01|2024-03-05|5                  |\n",
      "+-------+----------+----------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aggr_dates.withColumn(\"consecutive_date_cn\", datediff($\"end_date\", $\"start_date\") + 1\n",
    "                     ).filter($\"consecutive_date_cn\" >= 5).show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343307cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
