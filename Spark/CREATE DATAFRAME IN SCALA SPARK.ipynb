{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8640e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.101:4042\n",
       "SparkContext available as 'sc' (version = 3.4.1, master = local[*], app id = local-1713025646436)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|language|id    |\n",
      "+--------+------+\n",
      "|Java    |20000 |\n",
      "|Python  |100000|\n",
      "|Scala   |3000  |\n",
      "+--------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data: Seq[(String, String)] = List((Java,20000), (Python,100000), (Scala,3000))\n",
       "rdd: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[0] at parallelize at <console>:27\n",
       "df: org.apache.spark.sql.DataFrame = [language: string, id: string]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Different methods to create dataframe in Spark Scala\n",
    "// Create DataFrame Method 1 \n",
    "\n",
    "val data = Seq((\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\"))\n",
    "val rdd = spark.sparkContext.parallelize(data)\n",
    "val df = rdd.toDF(\"language\",\"id\")\n",
    "df.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d47e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3da2ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|language|users |\n",
      "+--------+------+\n",
      "|Java    |20000 |\n",
      "|Python  |100000|\n",
      "|Scala   |3000  |\n",
      "+--------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.{StringType, StructField, StructType}\n",
       "import org.apache.spark.sql.Row\n",
       "import scala.collection.JavaConversions._\n",
       "rowData: Seq[org.apache.spark.sql.Row] = List([Java,20000], [Python,100000], [Scala,3000])\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(language,StringType,true),StructField(users,StringType,true))\n",
       "dfFromRowData: org.apache.spark.sql.DataFrame = [language: string, users: string]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create DataFrame Method 2\n",
    "\n",
    "import org.apache.spark.sql.types.{StringType, StructField, StructType}\n",
    "import org.apache.spark.sql.Row\n",
    "import scala.collection.JavaConversions._  // this import is mandat for createDataFrame to work\n",
    "\n",
    "val rowData = Seq(Row(\"Java\", \"20000\"), Row(\"Python\", \"100000\"), Row(\"Scala\", \"3000\"))\n",
    "\n",
    "val schema = StructType(Array(\n",
    "    StructField(\"language\", StringType),\n",
    "    StructField(\"users\", StringType)\n",
    "))\n",
    "val dfFromRowData = spark.createDataFrame(rowData,schema)\n",
    "\n",
    "dfFromRowData.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6616175c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3df7427f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|language|users |\n",
      "+--------+------+\n",
      "|Java    |20000 |\n",
      "|Python  |100000|\n",
      "|Scala   |3000  |\n",
      "+--------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.{StringType, StructField, StructType}\n",
       "import org.apache.spark.sql.Row\n",
       "data: Seq[(String, String)] = List((Java,20000), (Python,100000), (Scala,3000))\n",
       "rdd: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[4] at parallelize at <console>:37\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(language,StringType,true),StructField(users,StringType,true))\n",
       "rowRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[5] at map at <console>:43\n",
       "dfFromRDD2: org.apache.spark.sql.DataFrame = [language: string, users: string]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create DataFrame Method 3\n",
    "\n",
    "import org.apache.spark.sql.types.{StringType, StructField, StructType}\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val data = Seq((\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\"))\n",
    "val rdd = spark.sparkContext.parallelize(data)\n",
    "val schema = StructType(Array(\n",
    "    StructField(\"language\", StringType, true),\n",
    "    StructField(\"users\", StringType)\n",
    "))\n",
    "\n",
    "val rowRDD=rdd.map(attributes => Row(attributes._1, attributes._2))\n",
    "val dfFromRDD2=spark.createDataFrame(rowRDD,schema)\n",
    "dfFromRDD2.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68471b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
