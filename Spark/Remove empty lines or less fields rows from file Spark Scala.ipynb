{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce478b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+----------+-----------+-----+----------+---+----+--------------+----+\n",
      "|name|address  |client_add|result_code|bytes|req_method|url|user|hierarchy_code|type|\n",
      "+----+---------+----------+-----------+-----+----------+---+----+--------------+----+\n",
      "|debo|bangalore|3         |4          |5    |6         |7  |8   |9             |Yes |\n",
      "|debo|banaglore|4         |5          |6    |7         |8  |9   |10            |Yes |\n",
      "|abhi|Delhi    |6         |7          |9    |10        |99 |99  |00            |No  |\n",
      "+----+---------+----------+-----------+-----+----------+---+----+--------------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.{StructType, StringType, StructField}\n",
       "fileHeader: String = name address client_add result_code bytes req_method url user hierarchy_code type\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(name,StringType,true),StructField(address,StringType,true),StructField(client_add,StringType,true),StructField(result_code,StringType,true),StructField(bytes,StringType,true),StructField(req_method,StringType,true),StructField(url,StringType,true),StructField(user,StringType,true),StructField(hierarchy_code,StringType,true),StructField(type,StringType,true))\n",
       "csvFilePath: String = corrupted_textfile.txt\n",
       "df: org.apache.spark.sql.DataFrame = [name: string, address: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// File has empty line after row number 3 and fields are also incomplete, drop such records from df\n",
    "\n",
    "// debo bangalore 3 4 5 6 7 8 9 Yes\n",
    "\n",
    "// debo banaglore 4 5 6 7 8 9 10 Yes\n",
    "\n",
    "// abhi Delhi 6 7 9 10 99 99 00 No\n",
    "\n",
    "\n",
    "// alex Newyork\n",
    "\n",
    "\n",
    "import org.apache.spark.sql.types.{StructType,StringType,StructField}\n",
    "\n",
    "val fileHeader = \"name address client_add result_code bytes req_method url user hierarchy_code type\"\n",
    "\n",
    "val schema= StructType(fileHeader.split(\" \").map(field=>StructField(field,StringType,true)))\n",
    "\n",
    "val csvFilePath = \"corrupted_textfile.txt\"\n",
    "\n",
    "val df = spark.read.option(\"header\", \"false\"\n",
    "                          ).option(\"delimiter\", \" \"\n",
    "                          ).schema(schema\n",
    "                          ).option(\"mode\", \"DROPMALFORMED\"\n",
    "                          ).csv(csvFilePath)\n",
    "\n",
    "df.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9d75c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
