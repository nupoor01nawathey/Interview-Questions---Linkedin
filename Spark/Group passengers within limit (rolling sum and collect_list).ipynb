{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84fe7071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.5:4040\n",
       "SparkContext available as 'sc' (version = 3.4.1, master = local[*], app id = local-1716989732517)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "|id |capacity_kg|\n",
      "+---+-----------+\n",
      "|1  |300        |\n",
      "|2  |350        |\n",
      "+---+-----------+\n",
      "\n",
      "+--------------+---------+-------+\n",
      "|passenger_name|weight_kg|lift_id|\n",
      "+--------------+---------+-------+\n",
      "|Rahul         |85       |1      |\n",
      "|Adarsh        |73       |1      |\n",
      "|Riti          |95       |1      |\n",
      "|Viraj         |80       |1      |\n",
      "|Vimal         |83       |2      |\n",
      "|Neha          |77       |2      |\n",
      "|Priti         |73       |2      |\n",
      "|Himanshi      |85       |2      |\n",
      "+--------------+---------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\n",
       "import org.apache.spark.sql.types._\n",
       "lift_df: org.apache.spark.sql.DataFrame = [id: int, capacity_kg: int]\n",
       "lift_passengers_data: Seq[org.apache.spark.sql.Row] = List([Rahul,85,1], [Adarsh,73,1], [Riti,95,1], [Viraj,80,1], [Vimal,83,2], [Neha,77,2], [Priti,73,2], [Himanshi,85,2])\n",
       "lift_passengers_schema: org.apache.spark.sql.types.StructType = StructType(StructField(passenger_name,StringType,true),StructField(weight_kg,IntegerType,true),StructField(lift_id,IntegerType,true))\n",
       "lift_passengers_rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = ParallelCollectionRDD[0] at parallelize at <console>:50\n",
       "lift_passengers_df: org.apache.spark.sql.DataFrame = [passenger_name: string, weight_kg: int ... 1 more field]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "\n",
    "val lift_df = Seq(\n",
    "    (1,300),\n",
    "    (2,350)\n",
    ").toDF(\"id\", \"capacity_kg\")\n",
    "\n",
    "\n",
    "val lift_passengers_data = Seq(\n",
    "    Row(\"Rahul\",85,1),\n",
    "    Row(\"Adarsh\",73,1),\n",
    "    Row(\"Riti\",95,1),\n",
    "    Row(\"Viraj\",80,1),\n",
    "    Row(\"Vimal\",83,2),\n",
    "    Row(\"Neha\",77,2),\n",
    "    Row(\"Priti\",73,2),\n",
    "    Row(\"Himanshi\",85,2)\n",
    ")\n",
    "\n",
    "val lift_passengers_schema = StructType(Array(\n",
    "  StructField(\"passenger_name\", StringType),\n",
    "  StructField(\"weight_kg\", IntegerType),\n",
    "  StructField(\"lift_id\", IntegerType)\n",
    "))\n",
    "\n",
    "val lift_passengers_rdd = spark.sparkContext.parallelize(lift_passengers_data)\n",
    "val lift_passengers_df = spark.createDataFrame(lift_passengers_rdd, lift_passengers_schema)\n",
    "\n",
    "\n",
    "lift_df.show(false)\n",
    "lift_passengers_df.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "621ccda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------------+\n",
      "|id |concat_ws(,, collect_list(passenger_name))|\n",
      "+---+------------------------------------------+\n",
      "|1  |Adarsh,Viraj,Rahul                        |\n",
      "|2  |Priti,Neha,Vimal,Himanshi                 |\n",
      "+---+------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions._\n",
       "import org.apache.spark.sql.functions.{collect_list, concat_ws}\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions._\n",
    "import org.apache.spark.sql.functions.{collect_list,concat_ws}\n",
    "\n",
    "lift_df.join(lift_passengers_df, $\"id\" === $\"lift_id\", \"inner\"\n",
    "            ).withColumn(\"running_sum\", sum($\"weight_kg\").over(Window.partitionBy($\"lift_id\").orderBy($\"weight_kg\".asc))\n",
    "                        ).where($\"running_sum\" <= $\"capacity_kg\"\n",
    "                               ).groupBy($\"id\").agg(concat_ws(\",\",collect_list($\"passenger_name\"))\n",
    "                                                   ).as(\"passenger_names\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583f6734",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
