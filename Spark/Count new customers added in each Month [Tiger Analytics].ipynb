{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3166e2af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.102:4040\n",
       "SparkContext available as 'sc' (version = 3.4.1, master = local[*], app id = local-1704624295653)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---+\n",
      "|order_date|customer|qty|\n",
      "+----------+--------+---+\n",
      "|2021-01-01|C1      |20 |\n",
      "|2021-01-01|C2      |30 |\n",
      "|2021-02-01|C1      |10 |\n",
      "|2021-02-01|C3      |15 |\n",
      "|2021-03-01|C5      |19 |\n",
      "|2021-03-01|C4      |10 |\n",
      "|2021-04-01|C3      |13 |\n",
      "|2021-04-01|C5      |15 |\n",
      "|2021-04-01|C6      |10 |\n",
      "+----------+--------+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql._\n",
       "data: Seq[org.apache.spark.sql.Row] = List([2021-01-01,C1,20], [2021-01-01,C2,30], [2021-02-01,C1,10], [2021-02-01,C3,15], [2021-03-01,C5,19], [2021-03-01,C4,10], [2021-04-01,C3,13], [2021-04-01,C5,15], [2021-04-01,C6,10])\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(order_date,StringType,true),StructField(customer,StringType,true),StructField(qty,IntegerType,true))\n",
       "rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = ParallelCollectionRDD[0] at parallelize at <console>:53\n",
       "df: org.apache.spark.sql.DataFrame = [order_date: string, customer: string ... 1 more field]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// 📌𝐓𝐢𝐠𝐞𝐫 𝐀𝐧𝐚𝐥𝐲𝐭𝐢𝐜𝐬 𝐒𝐐𝐋 𝐈𝐧𝐭𝐞𝐫𝐯𝐢𝐞𝐰 𝐐𝐮𝐞𝐬𝐭𝐢𝐨𝐧\n",
    "\n",
    "// Here is today's SQL question \n",
    "\n",
    "// ❓Find the count of new customers added in each Month\n",
    "\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "\n",
    "val data = Seq(\n",
    "  Row(\"2021-01-01\", \"C1\", 20),\n",
    "  Row(\"2021-01-01\", \"C2\", 30),\n",
    "  Row(\"2021-02-01\", \"C1\", 10),\n",
    "  Row(\"2021-02-01\", \"C3\", 15),\n",
    "  Row(\"2021-03-01\", \"C5\", 19),\n",
    "  Row(\"2021-03-01\", \"C4\", 10),\n",
    "  Row(\"2021-04-01\", \"C3\", 13),\n",
    "  Row(\"2021-04-01\", \"C5\", 15),\n",
    "  Row(\"2021-04-01\", \"C6\", 10)\n",
    ")\n",
    "\n",
    "val schema = StructType(Array(\n",
    "  StructField(\"order_date\", StringType),\n",
    "  StructField(\"customer\", StringType),\n",
    "  StructField(\"qty\", IntegerType)\n",
    "))\n",
    "      \n",
    "val rdd = spark.sparkContext.parallelize(data)\n",
    "val df = spark.createDataFrame(rdd, schema)\n",
    "\n",
    "                        \n",
    "df.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5184838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---+---+\n",
      "|order_date|customer|qty|rn |\n",
      "+----------+--------+---+---+\n",
      "|2021-01-01|C1      |20 |1  |\n",
      "|2021-02-01|C1      |10 |2  |\n",
      "|2021-01-01|C2      |30 |1  |\n",
      "|2021-02-01|C3      |15 |1  |\n",
      "|2021-04-01|C3      |13 |2  |\n",
      "|2021-03-01|C4      |10 |1  |\n",
      "|2021-03-01|C5      |19 |1  |\n",
      "|2021-04-01|C5      |15 |2  |\n",
      "|2021-04-01|C6      |10 |1  |\n",
      "+----------+--------+---+---+\n",
      "\n",
      "+----------+----------+-----+\n",
      "|month_name|order_date|count|\n",
      "+----------+----------+-----+\n",
      "|April     |2021-04-01|1    |\n",
      "|February  |2021-02-01|1    |\n",
      "|January   |2021-01-01|2    |\n",
      "|March     |2021-03-01|2    |\n",
      "+----------+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.Window\n",
       "df1: org.apache.spark.sql.DataFrame = [order_date: date, customer: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "val df1 = df.withColumn(\"order_date\", to_date($\"order_date\", \"yyyy-MM-dd\")\n",
    "             ).withColumn(\"rn\", row_number().over(Window.partitionBy($\"customer\").orderBy($\"order_date\"))\n",
    "                         )\n",
    "\n",
    "df1.show(false)\n",
    "\n",
    "\n",
    "df1.filter($\"rn\" === 1\n",
    "          ).withColumn(\"month_name\", date_format($\"order_date\", \"MMMM\")\n",
    "                      ).groupBy($\"month_name\", $\"order_date\").count().show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54b94d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
