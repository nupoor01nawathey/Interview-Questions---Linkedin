{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "521dbd3d-55a4-4354-9201-c50560df0697",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----------+\n| id|  name|salary|manager_id|\n+---+------+------+----------+\n| 10|  Anil| 50000|        18|\n| 11| Vikas| 75000|        16|\n| 12| Nisha| 40000|        18|\n| 13| Nidhi| 60000|        17|\n| 14| Priya| 80000|        18|\n| 15| Mohit| 45000|        18|\n| 16|Rajesh| 90000|        10|\n| 17| Raman| 55000|        16|\n| 18|   Sam| 65000|        17|\n+---+------+------+----------+\n\n+---+-----+------+----------+\n| id| name|salary|manager_id|\n+---+-----+------+----------+\n| 19|Sohan| 50000|        18|\n| 20| Sima| 75000|        17|\n+---+-----+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "manager_df=spark.createDataFrame(\n",
    "    [\n",
    "        (10 ,'Anil',50000, 18),\n",
    "        (11 ,'Vikas',75000,  16),\n",
    "        (12 ,'Nisha',40000,  18),\n",
    "        (13 ,'Nidhi',60000,  17),\n",
    "        (14 ,'Priya',80000,  18),\n",
    "        (15 ,'Mohit',45000,  18),\n",
    "        (16 ,'Rajesh',90000, 10),\n",
    "        (17 ,'Raman',55000, 16),\n",
    "        (18 ,'Sam',65000,   17)\n",
    "    ],\n",
    "    [\"id\", \"name\", \"salary\", \"manager_id\"]\n",
    ")\n",
    "\n",
    "manager_df1=spark.createDataFrame(\n",
    "    [\n",
    "        (19 ,'Sohan',50000, 18),\n",
    "        (20 ,'Sima',75000,  17)\n",
    "    ], [\"id\", \"name\", \"salary\", \"manager_id\"]\n",
    "    )\n",
    "\n",
    "manager_df.show()\n",
    "manager_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac46ddc3-7e3b-43d6-a0db-8d1d368df9b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----------+\n| id|  name|salary|manager_id|\n+---+------+------+----------+\n| 10|  Anil| 50000|        18|\n| 11| Vikas| 75000|        16|\n| 12| Nisha| 40000|        18|\n| 13| Nidhi| 60000|        17|\n| 14| Priya| 80000|        18|\n| 15| Mohit| 45000|        18|\n| 16|Rajesh| 90000|        10|\n| 17| Raman| 55000|        16|\n| 18|   Sam| 65000|        17|\n| 19| Sohan| 50000|        18|\n| 20|  Sima| 75000|        17|\n+---+------+------+----------+\n\nOut[20]: 11"
     ]
    }
   ],
   "source": [
    "manager_df.union(manager_df1).show()\n",
    "manager_df.union(manager_df1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0cd903a-6d81-4fbb-a3c5-72c665c49bfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[21]: 11"
     ]
    }
   ],
   "source": [
    "manager_df.unionAll(manager_df1).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8015b6d-13d1-4e8f-98cf-8f24f6694312",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----------+\n| id|  name|salary|manager_id|\n+---+------+------+----------+\n| 10|  Anil| 50000|        18|\n| 11| Vikas| 75000|        16|\n| 12| Nisha| 40000|        18|\n| 13| Nidhi| 60000|        17|\n| 14| Priya| 80000|        18|\n| 15| Mohit| 45000|        18|\n| 16|Rajesh| 90000|        10|\n| 17| Raman| 55000|        16|\n| 18|   Sam| 65000|        17|\n| 18|   Sam| 55000|        17|\n| 18|   Sam| 65000|        17|\n| 19| Sohan| 50000|        18|\n| 20|  Sima| 75000|        17|\n+---+------+------+----------+\n\nOut[22]: 13"
     ]
    }
   ],
   "source": [
    "# Add dup data\n",
    "dup_manager_df=spark.createDataFrame(\n",
    "    [\n",
    "        (10 ,'Anil',50000, 18),\n",
    "        (11 ,'Vikas',75000,  16),\n",
    "        (12 ,'Nisha',40000,  18),\n",
    "        (13 ,'Nidhi',60000,  17),\n",
    "        (14 ,'Priya',80000,  18),\n",
    "        (15 ,'Mohit',45000,  18),\n",
    "        (16 ,'Rajesh',90000, 10),\n",
    "        (17 ,'Raman',55000, 16),\n",
    "        (18 ,'Sam',65000,   17),\n",
    "        (18 ,'Sam',55000,   17), \n",
    "        (18 ,'Sam',65000,   17) \n",
    "    ],\n",
    "    [\"id\", \"name\", \"salary\", \"manager_id\"]\n",
    ")\n",
    "\n",
    "dup_manager_df.union(manager_df1).show()\n",
    "dup_manager_df.union(manager_df1).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "badf4a17-4bcf-41ec-9b2c-2c84d6c24bd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----------+\n| id|  name|salary|manager_id|\n+---+------+------+----------+\n| 10|  Anil| 50000|        18|\n| 11| Vikas| 75000|        16|\n| 12| Nisha| 40000|        18|\n| 13| Nidhi| 60000|        17|\n| 14| Priya| 80000|        18|\n| 15| Mohit| 45000|        18|\n| 16|Rajesh| 90000|        10|\n| 17| Raman| 55000|        16|\n| 18|   Sam| 65000|        17|\n| 18|   Sam| 55000|        17|\n| 18|   Sam| 65000|        17|\n| 19| Sohan| 50000|        18|\n| 20|  Sima| 75000|        17|\n+---+------+------+----------+\n\nOut[23]: 13"
     ]
    }
   ],
   "source": [
    "dup_manager_df.unionAll(manager_df1).show()\n",
    "dup_manager_df.unionAll(manager_df1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70511ce9-c828-4190-b535-e69fe6a3d56b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----------+\n| id|  name|salary|manager_id|\n+---+------+------+----------+\n| 10|  Anil| 50000|        18|\n| 11| Vikas| 75000|        16|\n| 12| Nisha| 40000|        18|\n| 13| Nidhi| 60000|        17|\n| 14| Priya| 80000|        18|\n| 15| Mohit| 45000|        18|\n| 16|Rajesh| 90000|        10|\n| 17| Raman| 55000|        16|\n| 18|   Sam| 65000|        17|\n| 18|   Sam| 55000|        17|\n| 18|   Sam| 65000|        17|\n| 19| Sohan| 50000|        18|\n| 20|  Sima| 75000|        17|\n+---+------+------+----------+\n\nOut[27]: 13"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    PySpark DF API treats union and unionAll same way but if you use these in spark.sql server then union would remove dup\n",
    "    union all / unionAll preserves dup rows\n",
    "\"\"\"\n",
    "dup_manager_df.createOrReplaceTempView(\"dup_manager_tbl\")\n",
    "manager_df1.createOrReplaceTempView(\"manager_tbl\")\n",
    "\n",
    "spark.sql(\"\"\"select * from dup_manager_tbl union all select * from manager_tbl\"\"\").show()\n",
    "spark.sql(\"\"\"select * from dup_manager_tbl union all select * from manager_tbl\"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14c239b1-3036-4089-b59e-4d45329b7115",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----------+\n| id|  name|salary|manager_id|\n+---+------+------+----------+\n| 10|  Anil| 50000|        18|\n| 11| Vikas| 75000|        16|\n| 12| Nisha| 40000|        18|\n| 13| Nidhi| 60000|        17|\n| 14| Priya| 80000|        18|\n| 15| Mohit| 45000|        18|\n| 16|Rajesh| 90000|        10|\n| 17| Raman| 55000|        16|\n| 18|   Sam| 65000|        17|\n| 18|   Sam| 55000|        17|\n| 19| Sohan| 50000|        18|\n| 20|  Sima| 75000|        17|\n+---+------+------+----------+\n\nOut[28]: 12"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from dup_manager_tbl union select * from manager_tbl\"\"\").show()\n",
    "spark.sql(\"\"\"select * from dup_manager_tbl union select * from manager_tbl\"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69a2df33-9b8e-4484-9899-07cd75d14741",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+----------+\n| id| name|salary|manager_id|\n+---+-----+------+----------+\n| 19|Sohan| 50000|        18|\n| 20| Sima| 75000|        17|\n+---+-----+------+----------+\n\n+---+------+----------+-----+\n| id|salary|manager_id| name|\n+---+------+----------+-----+\n| 19| 50000|        18|Sohan|\n| 20| 75000|        17| Sima|\n+---+------+----------+-----+\n\n+---+-----+------+----------+\n| id| name|salary|manager_id|\n+---+-----+------+----------+\n| 19|Sohan| 50000|        18|\n| 20| Sima| 75000|        17|\n| 19|50000|    18|     Sohan|\n| 20|75000|    17|      Sima|\n+---+-----+------+----------+\n\n+---+-----+------+----------+\n| id| name|salary|manager_id|\n+---+-----+------+----------+\n| 19|Sohan| 50000|        18|\n| 20| Sima| 75000|        17|\n| 19|Sohan| 50000|        18|\n| 20| Sima| 75000|        17|\n+---+-----+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wrong_manager_df=spark.createDataFrame([(19 ,50000, 18,'Sohan'),\n",
    "(20 ,75000,  17,'Sima')], [\"id\", \"salary\", \"manager_id\", \"name\"])\n",
    "\n",
    "manager_df1.show()\n",
    "wrong_manager_df.show()\n",
    "\n",
    "manager_df1.union(wrong_manager_df).show()\n",
    "\"\"\"\n",
    "union does not validate col vs data mapping\n",
    "it does not follow order of col names, it will simply union without matching the col name\n",
    "\"\"\"\n",
    "\n",
    "manager_df1.unionByName(wrong_manager_df).show()\n",
    "\"\"\"\n",
    "unionByName if col names exactly matches from all df then does union as expected\n",
    "here also col names should match\n",
    "if col names are different it wont union and throw exception\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d87e2e1-2bb4-447e-8c3e-681359a4e71f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+-----+-----+\n| id|salary|manager_id| name|bonus|\n+---+------+----------+-----+-----+\n| 19| 50000|        18|Sohan|   10|\n| 20| 75000|        17| Sima|   20|\n+---+------+----------+-----+-----+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1672088760047802>:7\u001B[0m\n",
       "\u001B[1;32m      4\u001B[0m wrong_column_df\u001B[38;5;241m.\u001B[39mshow()\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# wrong_manager_df.union(wrong_column_df).show()\u001B[39;00m\n",
       "\u001B[0;32m----> 7\u001B[0m wrong_manager_df\u001B[38;5;241m.\u001B[39munionByName(wrong_column_df)\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3738\u001B[0m, in \u001B[0;36mDataFrame.unionByName\u001B[0;34m(self, other, allowMissingColumns)\u001B[0m\n",
       "\u001B[1;32m   3682\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munionByName\u001B[39m(\u001B[38;5;28mself\u001B[39m, other: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m, allowMissingColumns: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m   3683\u001B[0m     \u001B[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` containing union of rows in this and another\u001B[39;00m\n",
       "\u001B[1;32m   3684\u001B[0m \u001B[38;5;124;03m    :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m   3685\u001B[0m \n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3736\u001B[0m \u001B[38;5;124;03m    +----+----+----+----+\u001B[39;00m\n",
       "\u001B[1;32m   3737\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3738\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munionByName\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallowMissingColumns\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 4 columns and the second input has 5 columns.;\n",
       "'Union false, false\n",
       ":- LogicalRDD [id#2996L, salary#2997L, manager_id#2998L, name#2999], false\n",
       "+- Project [id#3183L, salary#3184L, manager_id#3185L, name#3186, bonus#3187L]\n",
       "   +- LogicalRDD [id#3183L, salary#3184L, manager_id#3185L, name#3186, bonus#3187L], false\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-1672088760047802>:7\u001B[0m\n\u001B[1;32m      4\u001B[0m wrong_column_df\u001B[38;5;241m.\u001B[39mshow()\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# wrong_manager_df.union(wrong_column_df).show()\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m wrong_manager_df\u001B[38;5;241m.\u001B[39munionByName(wrong_column_df)\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3738\u001B[0m, in \u001B[0;36mDataFrame.unionByName\u001B[0;34m(self, other, allowMissingColumns)\u001B[0m\n\u001B[1;32m   3682\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munionByName\u001B[39m(\u001B[38;5;28mself\u001B[39m, other: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m, allowMissingColumns: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   3683\u001B[0m     \u001B[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` containing union of rows in this and another\u001B[39;00m\n\u001B[1;32m   3684\u001B[0m \u001B[38;5;124;03m    :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   3685\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3736\u001B[0m \u001B[38;5;124;03m    +----+----+----+----+\u001B[39;00m\n\u001B[1;32m   3737\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3738\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munionByName\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallowMissingColumns\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 4 columns and the second input has 5 columns.;\n'Union false, false\n:- LogicalRDD [id#2996L, salary#2997L, manager_id#2998L, name#2999], false\n+- Project [id#3183L, salary#3184L, manager_id#3185L, name#3186, bonus#3187L]\n   +- LogicalRDD [id#3183L, salary#3184L, manager_id#3185L, name#3186, bonus#3187L], false\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 4 columns and the second input has 5 columns.;\n'Union false, false\n:- LogicalRDD [id#2996L, salary#2997L, manager_id#2998L, name#2999], false\n+- Project [id#3183L, salary#3184L, manager_id#3185L, name#3186, bonus#3187L]\n   +- LogicalRDD [id#3183L, salary#3184L, manager_id#3185L, name#3186, bonus#3187L], false\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "wrong_column_df=spark.createDataFrame([(19 ,50000, 18,'Sohan',10),\n",
    "(20 ,75000,  17,'Sima',20)], [\"id\", \"salary\", \"manager_id\", \"name\", \"bonus\"])\n",
    "\n",
    "wrong_column_df.show()\n",
    "\n",
    "\"\"\"\n",
    "AnalysisException: [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 4 columns and the second input has 5 columns.;\n",
    "Both below throw Exception\n",
    "To fix this select common cols from both df and then perform union\n",
    "\"\"\"\n",
    "wrong_manager_df.union(wrong_column_df).show()\n",
    "wrong_manager_df.unionByName(wrong_column_df).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "13 Union Vs UnionAll PySpark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}