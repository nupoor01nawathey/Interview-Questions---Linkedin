{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcedda12-7faf-48a2-9690-9b4711a20193",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+-------+------+--------------------+------------------+\n| id|  name|   city|country|active|effective_start_date|effective_end_date|\n+---+------+-------+-------+------+--------------------+------------------+\n|  1|manish|  arwal|  india|     N|          2022-09-15|        2022-09-25|\n|  2|vikash|  patna|  india|     Y|          2023-08-12|              null|\n|  3|nikita|  delhi|  india|     Y|          2023-09-10|              null|\n|  4|rakesh| jaipur|  india|     Y|          2023-06-10|              null|\n|  5| ayush|     NY|    USA|     Y|          2023-06-10|              null|\n|  1|manish|gurgaon|  india|     Y|          2022-09-25|              null|\n+---+------+-------+-------+------+--------------------+------------------+\n\n+--------+-----------+-------------+----------+---------------------+---------------------+---------+\n|sales_id|customer_id|customer_name|sales_date|food_delivery_address|food_delivery_country|food_cost|\n+--------+-----------+-------------+----------+---------------------+---------------------+---------+\n|       1|          1|       manish|2023-01-16|              gurgaon|                india|      380|\n|      77|          1|       manish|2023-03-11|            bangalore|                india|      300|\n|      12|          3|       nikita|2023-09-20|                delhi|                india|      127|\n|      54|          4|       rakesh|2023-08-10|               jaipur|                india|      321|\n|      65|          5|        ayush|2023-09-07|                mosco|               russia|      765|\n|      89|          6|        rajat|2023-08-10|               jaipur|                india|      321|\n+--------+-----------+-------------+----------+---------------------+---------------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We are going to identify Type 2 rows on the basis of change in address\n",
    "If we have to identify changes in any field then better to create md5 hash and store that in fact sales_df and use that to filter new or update records\n",
    "\"\"\"\n",
    "\n",
    "customer_dim_data = [\n",
    "    (1,'manish','arwal','india','N','2022-09-15','2022-09-25'),\n",
    "    (2,'vikash','patna','india','Y','2023-08-12',None),\n",
    "    (3,'nikita','delhi','india','Y','2023-09-10',None),\n",
    "    (4,'rakesh','jaipur','india','Y','2023-06-10',None),\n",
    "    (5,'ayush','NY','USA','Y','2023-06-10',None),\n",
    "    (1,'manish','gurgaon','india','Y','2022-09-25',None),\n",
    "]\n",
    "customer_schema= ['id','name','city','country','active','effective_start_date','effective_end_date']\n",
    "customer_dim_df = spark.createDataFrame(data= customer_dim_data,schema=customer_schema)\n",
    "\n",
    "sales_data = [\n",
    "    (1,1,'manish','2023-01-16','gurgaon','india',380),\n",
    "    (77,1,'manish','2023-03-11','bangalore','india',300),\n",
    "    (12,3,'nikita','2023-09-20','delhi','india',127),\n",
    "    (54,4,'rakesh','2023-08-10','jaipur','india',321),\n",
    "    (65,5,'ayush','2023-09-07','mosco','russia',765),\n",
    "    (89,6,'rajat','2023-08-10','jaipur','india',321)\n",
    "]\n",
    "sales_schema = ['sales_id', 'customer_id','customer_name', 'sales_date', 'food_delivery_address','food_delivery_country', 'food_cost']\n",
    "sales_df = spark.createDataFrame(data=sales_data,schema=sales_schema)\n",
    "\n",
    "customer_dim_df.show() # dim target/trgt data\n",
    "sales_df.show()        # incoming source/src data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5181f794-21e0-4254-9cb1-0ca6e088d322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>city</th><th>country</th><th>active</th><th>effective_start_date</th><th>effective_end_date</th><th>sales_id</th><th>customer_id</th><th>customer_name</th><th>sales_date</th><th>food_delivery_address</th><th>food_delivery_country</th><th>food_cost</th></tr></thead><tbody><tr><td>1</td><td>manish</td><td>arwal</td><td>india</td><td>N</td><td>2022-09-15</td><td>2022-09-25</td><td>77</td><td>1</td><td>manish</td><td>2023-03-11</td><td>bangalore</td><td>india</td><td>300</td></tr><tr><td>1</td><td>manish</td><td>arwal</td><td>india</td><td>N</td><td>2022-09-15</td><td>2022-09-25</td><td>1</td><td>1</td><td>manish</td><td>2023-01-16</td><td>gurgaon</td><td>india</td><td>380</td></tr><tr><td>2</td><td>vikash</td><td>patna</td><td>india</td><td>Y</td><td>2023-08-12</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>3</td><td>nikita</td><td>delhi</td><td>india</td><td>Y</td><td>2023-09-10</td><td>null</td><td>12</td><td>3</td><td>nikita</td><td>2023-09-20</td><td>delhi</td><td>india</td><td>127</td></tr><tr><td>4</td><td>rakesh</td><td>jaipur</td><td>india</td><td>Y</td><td>2023-06-10</td><td>null</td><td>54</td><td>4</td><td>rakesh</td><td>2023-08-10</td><td>jaipur</td><td>india</td><td>321</td></tr><tr><td>5</td><td>ayush</td><td>NY</td><td>USA</td><td>Y</td><td>2023-06-10</td><td>null</td><td>65</td><td>5</td><td>ayush</td><td>2023-09-07</td><td>mosco</td><td>russia</td><td>765</td></tr><tr><td>1</td><td>manish</td><td>gurgaon</td><td>india</td><td>Y</td><td>2022-09-25</td><td>null</td><td>77</td><td>1</td><td>manish</td><td>2023-03-11</td><td>bangalore</td><td>india</td><td>300</td></tr><tr><td>1</td><td>manish</td><td>gurgaon</td><td>india</td><td>Y</td><td>2022-09-25</td><td>null</td><td>1</td><td>1</td><td>manish</td><td>2023-01-16</td><td>gurgaon</td><td>india</td><td>380</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "manish",
         "arwal",
         "india",
         "N",
         "2022-09-15",
         "2022-09-25",
         77,
         1,
         "manish",
         "2023-03-11",
         "bangalore",
         "india",
         300
        ],
        [
         1,
         "manish",
         "arwal",
         "india",
         "N",
         "2022-09-15",
         "2022-09-25",
         1,
         1,
         "manish",
         "2023-01-16",
         "gurgaon",
         "india",
         380
        ],
        [
         2,
         "vikash",
         "patna",
         "india",
         "Y",
         "2023-08-12",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null
        ],
        [
         3,
         "nikita",
         "delhi",
         "india",
         "Y",
         "2023-09-10",
         null,
         12,
         3,
         "nikita",
         "2023-09-20",
         "delhi",
         "india",
         127
        ],
        [
         4,
         "rakesh",
         "jaipur",
         "india",
         "Y",
         "2023-06-10",
         null,
         54,
         4,
         "rakesh",
         "2023-08-10",
         "jaipur",
         "india",
         321
        ],
        [
         5,
         "ayush",
         "NY",
         "USA",
         "Y",
         "2023-06-10",
         null,
         65,
         5,
         "ayush",
         "2023-09-07",
         "mosco",
         "russia",
         765
        ],
        [
         1,
         "manish",
         "gurgaon",
         "india",
         "Y",
         "2022-09-25",
         null,
         77,
         1,
         "manish",
         "2023-03-11",
         "bangalore",
         "india",
         300
        ],
        [
         1,
         "manish",
         "gurgaon",
         "india",
         "Y",
         "2022-09-25",
         null,
         1,
         1,
         "manish",
         "2023-01-16",
         "gurgaon",
         "india",
         380
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "active",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "effective_start_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "effective_end_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sales_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "customer_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sales_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "food_delivery_address",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "food_delivery_country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "food_cost",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "joined_data = customer_dim_df.join(sales_df, customer_dim_df[\"id\"]==sales_df[\"customer_id\"], \"left\")\n",
    "display(joined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b7ee7b1-8f31-4c41-956f-8224422140de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------+---------------------+------+--------------------+------------------+\n|customer_id|customer_name|     city|food_delivery_country|active|effective_start_date|effective_end_date|\n+-----------+-------------+---------+---------------------+------+--------------------+------------------+\n|          1|       manish|bangalore|                india|     Y|          2023-03-11|              null|\n|          5|        ayush|    mosco|               russia|     Y|          2023-09-07|              null|\n+-----------+-------------+---------+---------------------+------+--------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Handle update records Step 1 is to mark active\n",
    "new_record_df = joined_data.filter((col(\"food_delivery_address\") != col(\"city\")) & (col(\"active\") == \"Y\"))\\\n",
    "    .withColumn(\"active\", lit(\"Y\"))\\\n",
    "    .withColumn(\"effective_start_date\", col(\"sales_date\"))\\\n",
    "    .withColumn(\"effective_end_date\", lit(None))\\\n",
    "    .select(\"customer_id\", \"customer_name\", col(\"food_delivery_address\").alias(\"city\"), \"food_delivery_country\", \"active\", \"effective_start_date\", \"effective_end_date\")\n",
    "\n",
    "new_record_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04ed72d9-d073-491f-ab99-23b8a5700e38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------------+------+--------------------+------------------+\n|customer_id|customer_name|   city|food_delivery_country|active|effective_start_date|effective_end_date|\n+-----------+-------------+-------+---------------------+------+--------------------+------------------+\n|          1|       manish|gurgaon|                india|     N|          2022-09-25|        2023-03-11|\n|          5|        ayush|     NY|               russia|     N|          2023-06-10|        2023-09-07|\n+-----------+-------------+-------+---------------------+------+--------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Handle update records Step 2 is to mark existing records as inactive (active=N, and update effective_end_date)\n",
    "\n",
    "old_record_df = joined_data.filter((col(\"food_delivery_address\") != col(\"city\")) & (col(\"active\") == \"Y\"))\\\n",
    "    .withColumn(\"active\", lit(\"N\"))\\\n",
    "    .withColumn(\"effective_end_date\", col(\"sales_date\"))\\\n",
    "    .select(\"customer_id\", \"customer_name\", \"city\", \"food_delivery_country\", \"active\", \"effective_start_date\", \"effective_end_date\")\n",
    "\n",
    "old_record_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a8e29b0-6686-44ec-b6c0-b83b0d02e4f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------------------+---------------------+------+--------------------+------------------+\n|customer_id|customer_name|food_delivery_address|food_delivery_country|active|effective_start_date|effective_end_date|\n+-----------+-------------+---------------------+---------------------+------+--------------------+------------------+\n|          6|        rajat|               jaipur|                india|     Y|          2023-08-10|              null|\n+-----------+-------------+---------------------+---------------------+------+--------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Handle new records \n",
    "\n",
    "new_customer_df = sales_df.join(customer_dim_df, sales_df[\"customer_id\"]==customer_dim_df[\"id\"], \"left_anti\")\\\n",
    "    .withColumn(\"active\", lit(\"Y\"))\\\n",
    "    .withColumn(\"effective_start_date\", col(\"sales_date\"))\\\n",
    "    .withColumn(\"effective_end_date\", lit(None))\\\n",
    "    .select(\"customer_id\", \"customer_name\", \"food_delivery_address\", \"food_delivery_country\", \"active\", \"effective_start_date\", \"effective_end_date\")\n",
    "\n",
    "new_customer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0285b4c-a574-4ee6-acd2-be063bb7f2f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+-------+------+--------------------+------------------+\n| id|  name|     city|country|active|effective_start_date|effective_end_date|\n+---+------+---------+-------+------+--------------------+------------------+\n|  1|manish|    arwal|  india|     N|          2022-09-15|        2022-09-25|\n|  2|vikash|    patna|  india|     Y|          2023-08-12|              null|\n|  3|nikita|    delhi|  india|     Y|          2023-09-10|              null|\n|  4|rakesh|   jaipur|  india|     Y|          2023-06-10|              null|\n|  5| ayush|       NY|    USA|     Y|          2023-06-10|              null|\n|  1|manish|  gurgaon|  india|     Y|          2022-09-25|              null|\n|  1|manish|bangalore|  india|     Y|          2023-03-11|              null|\n|  5| ayush|    mosco| russia|     Y|          2023-09-07|              null|\n|  1|manish|  gurgaon|  india|     N|          2022-09-25|        2023-03-11|\n|  5| ayush|       NY| russia|     N|          2023-06-10|        2023-09-07|\n|  6| rajat|   jaipur|  india|     Y|          2023-08-10|              null|\n+---+------+---------+-------+------+--------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "final_record_df = customer_dim_df.union(new_record_df).union(old_record_df).union(new_customer_df)\n",
    "\n",
    "final_record_df.show()\n",
    "\"\"\"\n",
    "|  1|manish|  gurgaon|  india|     Y|          2022-09-25|              null|\n",
    "|  1|manish|bangalore|  india|     Y|          2023-03-11|              null|\n",
    "How can Manish exists in 2 locations ?!\n",
    "Here drop unwanted records based on effective_start_date desc, keep latest row drop old rows\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1481979d-c4e0-4a3e-b073-0d628a18d081",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- city: string (nullable = true)\n |-- country: string (nullable = true)\n |-- active: string (nullable = true)\n |-- effective_start_date: date (nullable = true)\n |-- effective_end_date: string (nullable = true)\n |-- rn: integer (nullable = false)\n\n+---+------+---------+-------+------+--------------------+------------------+---+\n| id|  name|     city|country|active|effective_start_date|effective_end_date| rn|\n+---+------+---------+-------+------+--------------------+------------------+---+\n|  1|manish|  gurgaon|  india|     N|          2022-09-25|        2023-03-11|  1|\n|  1|manish|    arwal|  india|     N|          2022-09-15|        2022-09-25|  2|\n|  1|manish|bangalore|  india|     Y|          2023-03-11|              null|  1|\n|  2|vikash|    patna|  india|     Y|          2023-08-12|              null|  1|\n|  3|nikita|    delhi|  india|     Y|          2023-09-10|              null|  1|\n|  4|rakesh|   jaipur|  india|     Y|          2023-06-10|              null|  1|\n|  5| ayush|       NY| russia|     N|          2023-06-10|        2023-09-07|  1|\n|  5| ayush|    mosco| russia|     Y|          2023-09-07|              null|  1|\n|  6| rajat|   jaipur|  india|     Y|          2023-08-10|              null|  1|\n+---+------+---------+-------+------+--------------------+------------------+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import *\n",
    "\n",
    "# sorting wont happen correctly if type is string, cast to date first and then sort\n",
    "final_record_df.withColumn(\"effective_start_date\", col(\"effective_start_date\").cast(\"date\")).withColumn(\"rn\", row_number().over(Window.partitionBy(\"id\", \"active\").orderBy(desc(\"effective_start_date\")))).printSchema()\n",
    "\n",
    "\n",
    "final_record_df.withColumn(\"effective_start_date\", col(\"effective_start_date\").cast(\"date\")).withColumn(\"rn\", row_number().over(Window.partitionBy(\"id\", \"active\").orderBy(col(\"effective_start_date\").desc())))\\\n",
    "    .filter(~((col(\"rn\")>=2) & (col(\"active\")==\"Y\")))\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92a16ebf-6081-48bd-9000-6828609c402f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n|   City| Country|\n+-------+--------+\n|  Delhi|   India|\n|Kolkata|   India|\n| Mumbai|   India|\n|Nairobi|   Kenya|\n|Colombo|Srilanka|\n+-------+--------+\n\n+----+-------+\n|Code|   City|\n+----+-------+\n| 011|  Delhi|\n| 022| Mumbai|\n| 033|Kolkata|\n| 044|Chennai|\n+----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "cityDF = spark.createDataFrame([ \n",
    "        (\"Delhi\",\"India\"),\n",
    "        (\"Kolkata\",\"India\"),\n",
    "        (\"Mumbai\",\"India\"),\n",
    "        (\"Nairobi\",\"Kenya\"),\n",
    "        (\"Colombo\",\"Srilanka\")],\n",
    "        [\"City\",\"Country\"]\n",
    "    )\n",
    "codeDF = spark.createDataFrame([\n",
    "        (\"011\",\"Delhi\"),\n",
    "        (\"022\",\"Mumbai\"),\n",
    "        (\"033\",\"Kolkata\"),\n",
    "        (\"044\",\"Chennai\")], \n",
    "        [\"Code\",\"City\"]\n",
    "    )\n",
    "\n",
    "cityDF.show()\n",
    "codeDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db129935-49dc-4722-ba03-33e192bba6ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n|   City|Country|\n+-------+-------+\n|  Delhi|  India|\n|Kolkata|  India|\n| Mumbai|  India|\n+-------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "cityDF.join(codeDF, cityDF[\"City\"] == codeDF[\"City\"], \"left_semi\").show()\n",
    "\n",
    "\"\"\"\n",
    "    left_semi join acts as exists in sql\n",
    "    so when left table is joined with right using left_semi output will return only matching rows+ left table columns\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "664f654f-804b-4a17-adea-3696ff46f275",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n|   City| Country|\n+-------+--------+\n|Nairobi|   Kenya|\n|Colombo|Srilanka|\n+-------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "cityDF.join(codeDF, cityDF[\"City\"] == codeDF[\"City\"], \"left_anti\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ad23d2e-5b59-4e4f-a673-0759db46bcbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#cols will vary for each dim table\n",
    "#generic scd2 should be applied for all dim tables\n",
    "\n",
    "1. target_hash based on col values and have it in dim table(cols should match with source)\n",
    "2. source_hash for incoming source data, have it in source table (cols should match with dim)\n",
    "\n",
    "TODO above\n",
    "==================================================================================================\n",
    "==================================================================================================\n",
    "\n",
    "Update scenario\n",
    "    :mark existing rec active=N, end_date=todays date\n",
    "    :mark new rec active=Y, end_date=null\n",
    "\n",
    "Consider id as pk\n",
    "==================================================================================================\n",
    "DAY 1 \n",
    "\n",
    "Full Load will be treated as all new rows \n",
    "active=Y\n",
    "eff_start_dt=row's date\n",
    "eff_end_dt=9999-12-31\n",
    "\n",
    "===================================================================================================================\n",
    "1 John Y 01-Jan 9999-12-31   <---- trgt/dim table\n",
    "1 Josh   02-Jan              <---- src/incoming table\n",
    "\n",
    "Left join when left table=trgt and right table=src + active=Y (mark inactive)\n",
    "1 John Y 01-Jan null  1 Josh   02-Jan      \n",
    "1 John 01-Jan.withColumn(\"active\", lit(\"N\")).withColumn(\"eff_end_dt\", src.date which is 02-Jan) => 1 John N 01-Jan 02-Jan\n",
    "\n",
    "dim_t\n",
    "1 John N 01-Jan 02-Jan \n",
    "\n",
    "**** LEFT_SEMI JOIN WILL RETURN MATCHING ROWS BUT COLUMNS ONLY FROM LEFT TABLE ****\n",
    "Left semi join when left table=src and right table=trgt + active!=Y\n",
    "1 Josh   02-Jan 1 John N 01-Jan 02-Jan  \n",
    "1 Josh   02-Jan.withColumn(\"active\", lit(\"Y\")).withColumn(\"eff_end_dt\", null or 9999-12-31) =>  1 Josh Y 02-Jan 9999-12-31\n",
    "\n",
    "dim_t\n",
    "1 Josh 02-Jan 9999-12-31 Y\n",
    "1 John 01-Jan 02-Jan     N\n",
    "===================================================================================================================\n",
    "src\n",
    "1 Joshua   03-Jan\n",
    "\n",
    "Left join when left table=trgt and right table=src + active=Y\n",
    "1 Josh 02-Jan Y 9999-12-31      1 Joshua   03-Jan\n",
    "1 Josh 02-Jan.withColumn(\"active\", lit(\"N\")).withColumn(\"eff_end_dt\", src.date which is 03-Jan) => 1 Josh 02-Jan N 03-Jan \n",
    "\n",
    "dim_t\n",
    "1 Josh 02-Jan 03-Jan  N \n",
    "1 John 01-Jan 02-Jan  N \n",
    "\n",
    "Left semi join when left table=src and right table=trgt + active!=Y\n",
    "1 Joshua   03-Jan   1 Josh 02-Jan 03-Jan  N |\n",
    "1 Joshua   03-Jan   1 John 01-Jan 02-Jan  N | src_df\n",
    "src_df.select(\"left table cols\").distinct().withColumn(\"active\", lit(\"Y\")).withColumn(\"eff_end_dt\", null or 9999-12-31) =>  1 Joshua   03-Jan Y 9999-12-31\n",
    "\n",
    "dim_t\n",
    "1 Joshua  Y  03-Jan  9999-12-31\n",
    "1 Josh    N  02-Jan  03-Jan \n",
    "1 John    N  01-Jan  02-Jan \n",
    "===================================================================================================================\n",
    "src\n",
    "2 Scott     04-Jan\n",
    "1 Jake      04-Jan\n",
    "\n",
    "Left join when left table=trgt and right table=src + active=Y\n",
    "1 Joshua  Y  03-Jan  9999-12-31 1 Jake      04-Jan\n",
    "1 Joshua  Y  03-Jan.withColumn(\"active\", lit(\"N\")).withColumn(\"eff_end_dt\", src.date which is 04-Jan) => 1 Joshua 03-Jan 04-Jan N\n",
    "\n",
    "dim_t\n",
    "1 Joshua  N  03-Jan  9999-12-31\n",
    "1 Josh    N  02-Jan  03-Jan \n",
    "1 John    N  01-Jan  02-Jan \n",
    "\n",
    "Left semi join when left table=src and right table=trgt + active!=Y\n",
    "2 Scott     04-Jan  null     null   null     null   |\n",
    "1 Jake      04-Jan  Joshua  N     03-Jan   04-Jan   |src_df\n",
    "1 Jake      04-Jan  Josh    N     02-Jan   03-Jan   |\n",
    "1 Jake      04-Jan  John    N     01-Jan   02-Jan   |\n",
    "\n",
    "src_df.select(\"left table cols\").distinct().withColumn(\"active\", lit(\"Y\")).withColumn(\"eff_end_dt\", null or 9999-12-31) => \n",
    "2 Scott     04-Jan Y  9999-12-31\n",
    "1 Jake      04-Jan Y  9999-12-31 \n",
    "\n",
    "dim_t\n",
    "2 Scott   Y  04-Jan  9999-12-31\n",
    "1 Jake    Y  04-Jan  9999-12-31\n",
    "1 Joshua  N  03-Jan  04-Jan\n",
    "1 Josh    N  02-Jan  03-Jan \n",
    "1 John    N  01-Jan  02-Jan\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "24 SCD 2 PySpark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}