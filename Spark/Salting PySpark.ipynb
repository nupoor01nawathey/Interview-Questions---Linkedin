{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4de6d665-532e-4e82-898f-b1d144920e43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[25]: 1000035"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "\n",
    "\n",
    "id_values1 = [1] * 100000 + [2] * 5 + [3] * 5\n",
    "table1 = spark.createDataFrame(id_values1, \"int\").toDF(\"id\")\n",
    "\n",
    "\n",
    "id_values2 = [1] * 10 + [2] * 5 + [3] * 2\n",
    "table2 = spark.createDataFrame(id_values2, \"int\").toDF(\"id\")\n",
    "\n",
    "# table1.show()\n",
    "# table2.show() # prints One 10 times, Two 5 times and Three twice\n",
    "\n",
    "data = table1.join(table2, [\"id\"], \"inner\")\n",
    "\n",
    "# print(data.count()) # 1000035\n",
    "\n",
    "df_with_random = table1.withColumn(\"random_num\", (1 + rand() * 10 ).cast(\"int\")) # Generate random num from 1 to 10\n",
    "\n",
    "table1 = df_with_random.withColumn(\"salted_key\", concat(expr(\"id\"), lit(\"_\"), expr(\"random_num\"))) # concat id_randomnum\n",
    "\n",
    "\"\"\"\n",
    "table1.show()\n",
    "+---+----------+----------+\n",
    "| id|random_num|salted_key|\n",
    "+---+----------+----------+\n",
    "|  1|         3|       1_3|\n",
    "|  1|         9|       1_9|\n",
    "|  1|         3|       1_3|\n",
    "\"\"\"\n",
    "\n",
    "table2_replicated = table2.withColumn(\"sequence\", array([lit(i) for i in range(1, 11)])) \n",
    "\n",
    "table2 = table2_replicated\\\n",
    "    .withColumn(\"exploded_col\", explode(col(\"sequence\")))\\\n",
    "    .withColumn(\"salted_key\", concat(expr(\"id\"), lit(\"_\"), expr(\"exploded_col\")))\\\n",
    "    .drop(\"exploded_col\", \"sequence\")\n",
    "\n",
    "\"\"\"\n",
    "table2.show()\n",
    "+---+--------------------+------------+----------+\n",
    "| id|            sequence|exploded_col|salted_key|\n",
    "+---+--------------------+------------+----------+\n",
    "|  1|[1, 2, 3, 4, 5, 6...|           1|       1_1|\n",
    "|  1|[1, 2, 3, 4, 5, 6...|           2|       1_2|\n",
    "|  1|[1, 2, 3, 4, 5, 6...|           3|       1_3|\n",
    "\"\"\"\n",
    "\n",
    "data = table1.join(table2, [\"salted_key\"], \"inner\")\n",
    "\n",
    "data.count() # 1000035\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Salting PySpark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}