{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bd5178c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\n",
       "import org.apache.spark.sql.types._\n",
       "flatfile: org.apache.spark.rdd.RDD[String] = skip_first_3_rows_from_text_file.txt MapPartitionsRDD[23] at textFile at <console>:48\n",
       "flatfileWithDroppedLines: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[24] at mapPartitionsWithIndex at <console>:50\n",
       "res4: Array[String] = Array(1,Jer, 2,Tom, 3,Dan)\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "\n",
    "val flatfile = sc.textFile(\"skip_first_3_rows_from_text_file.txt\")\n",
    "\n",
    "val flatfileWithDroppedLines = flatfile.mapPartitionsWithIndex{ (id_x, iter) => if(id_x==0) iter.drop(4) else iter }\n",
    "// iter.drop(4) as 4th row is a header and first 3 rows have incorrect data\n",
    "\n",
    "\n",
    "flatfileWithDroppedLines.collect()\n",
    "// Array[String] = Array(1,Jer, 2,Tom, 3,Dan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "982d61b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|id |name|\n",
      "+---+----+\n",
      "|1  |Jer |\n",
      "|2  |Tom |\n",
      "|3  |Dan |\n",
      "+---+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(id,IntegerType,true),StructField(name,StringType,true))\n",
       "df: org.apache.spark.sql.DataFrame = [id: int, name: string]\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema = StructType(Array(\n",
    "    StructField(\"id\", IntegerType),\n",
    "    StructField(\"name\", StringType)\n",
    "))\n",
    "\n",
    "val df = spark.read\n",
    "  .format(\"com.databricks.spark.csv\")\n",
    "  .option(\"header\", \"false\")\n",
    "  .option(\"inferSchema\", \"false\")\n",
    "  .option(\"delimiter\", \",\")\n",
    "  //.option(\"mode\", \"DROPMALFORMED\") this alone will drop incorrect record, no need to check for isNotNull later\n",
    "  .schema(schema)\n",
    "  .load(\"skip_first_3_rows_from_text_file.txt\")\n",
    "\n",
    "df.show(false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "54f5db52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|id |name|\n",
      "+---+----+\n",
      "|1  |Jer |\n",
      "|2  |Tom |\n",
      "|3  |Dan |\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter($\"id\".isNotNull).show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377bc7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "// PySpark equivalent\n",
    "\n",
    "// import csv\n",
    "\n",
    "// customSchema = StructType([ \\\n",
    "//     StructField(\"Col1\", StringType(), True), \\\n",
    "//     StructField(\"Col2\", StringType(), True)])\n",
    "\n",
    "// df = sc.textFile(\"file.csv\")\\\n",
    "//         .mapPartitions(lambda partition: csv.reader([line.replace('\\0','') for line in partition],delimiter=',', quotechar='\"')).filter(lambda line: len(line) > 2 and line[0] != 'Col1')\\\n",
    "//         .toDF(customSchema)\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
