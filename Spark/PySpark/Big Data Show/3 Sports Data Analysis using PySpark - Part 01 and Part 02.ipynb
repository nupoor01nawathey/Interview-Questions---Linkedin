{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbf95769-1477-4846-bcc0-c6c0973cc780",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+--------------+\n|user_id|kit_id|login_date|sessions_count|\n+-------+------+----------+--------------+\n|      1|     2|2016-03-01|             5|\n|      1|     2|2016-03-02|             6|\n|      2|     3|2017-06-25|             1|\n|      3|     1|2016-03-02|             0|\n|      3|     4|2018-07-03|             5|\n+-------+------+----------+--------------+\n\nroot\n |-- user_id: integer (nullable = true)\n |-- kit_id: integer (nullable = true)\n |-- login_date: date (nullable = true)\n |-- sessions_count: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "https://www.youtube.com/watch?v=f9zMAfuH0z4&list=PPSV\n",
    "https://www.youtube.com/watch?v=JHaF3PFzOmk&list=PLiDUHlGx0KN8fBg645K15BIIP8jroHYpi&index=4\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window \n",
    "\n",
    "schema = StructType(\n",
    "    [ \n",
    "        StructField(\"user_id\", IntegerType(), True), \n",
    "        StructField(\"kit_id\", IntegerType(), True), \n",
    "        StructField(\"login_date\", StringType(), True), \n",
    "        StructField(\"sessions_count\", IntegerType(), True) \n",
    "    ]\n",
    ") \n",
    "\n",
    "data = [ \n",
    "            (1, 2, \"2016-03-01\", 5), \n",
    "            (1, 2, \"2016-03-02\", 6), \n",
    "            (2, 3, \"2017-06-25\", 1), \n",
    "            (3, 1, \"2016-03-02\", 0), \n",
    "            (3, 4, \"2018-07-03\", 5) \n",
    "        ] \n",
    "\n",
    "inputDF = spark.createDataFrame(data, schema=schema).withColumn(\"login_date\", col(\"login_date\").cast(\"date\"))\n",
    "\n",
    "inputDF.show()\n",
    "inputDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e97438b-dce7-4f0a-89d9-d4de8fafa4ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n|user_id|login_date|\n+-------+----------+\n|      1|2016-03-01|\n|      2|2017-06-25|\n|      3|2016-03-02|\n+-------+----------+\n\n+-------+----------+\n|user_id|login_date|\n+-------+----------+\n|      1|2016-03-01|\n|      2|2017-06-25|\n|      3|2016-03-02|\n+-------+----------+\n\n+-------+----------+\n|user_id|login_date|\n+-------+----------+\n|      1|2016-03-01|\n|      2|2017-06-25|\n|      3|2016-03-02|\n+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    User activity analysis : Extract 1st login for each user\n",
    "\"\"\"\n",
    "# 1. Use this only when other cols are not required as groupBy will not select other cols in select projection\n",
    "inputDF.groupBy(\"user_id\").agg(min(\"login_date\").alias(\"login_date\")).show()\n",
    "\n",
    "# 2. Flexible, retains all cols, this approach and spark.sql will create same query plan, all depends on how your org wants to write code which is either using Spark DF API or spark sql \n",
    "inputDF.withColumn(\"rn\", row_number().over(Window.partitionBy(\"user_id\").orderBy(\"login_date\"))) \\\n",
    "    .filter(col(\"rn\") == 1) \\\n",
    "    .select(\"user_id\", \"login_date\") \\\n",
    "    .show()\n",
    "\n",
    "# 3. Spark sql equivalent to Spark DF API code, all depends on how your org wants to write code which is either using Spark DF API or spark sql \n",
    "inputDF.createOrReplaceTempView(\"input_tbl\")\n",
    "spark.sql(\"\"\"\n",
    "            WITH rn_table AS (\n",
    "                SELECT\n",
    "                    *,\n",
    "                    ROW_NUMBER() OVER(PARTITION BY user_id ORDER BY login_date) AS rn\n",
    "                FROM input_tbl\n",
    "            )\n",
    "            SELECT user_id, login_date FROM rn_table where rn=1\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "279a7486-bd48-497d-b912-f62ddb473913",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------------------+\n|user_id|login_date|running_count_of_sessions|\n+-------+----------+-------------------------+\n|      1|2016-03-01|                        5|\n|      1|2016-03-02|                       11|\n|      2|2017-06-25|                        1|\n|      3|2016-03-02|                        0|\n|      3|2018-07-03|                        5|\n+-------+----------+-------------------------+\n\n+-------+----------+-------------------------+\n|user_id|login_date|running_count_of_sessions|\n+-------+----------+-------------------------+\n|      1|2016-03-01|                        5|\n|      1|2016-03-02|                       11|\n|      2|2017-06-25|                        1|\n|      3|2016-03-02|                        0|\n|      3|2018-07-03|                        5|\n+-------+----------+-------------------------+\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Window [user_id#73, login_date#81, sum(sessions_count#76) windowspecdefinition(user_id#73, login_date#81 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS running_count_of_sessions#1119L], [user_id#73], [login_date#81 ASC NULLS FIRST]\n   +- Sort [user_id#73 ASC NULLS FIRST, login_date#81 ASC NULLS FIRST], false, 0\n      +- Exchange hashpartitioning(user_id#73, 200), ENSURE_REQUIREMENTS, [plan_id=3507]\n         +- Project [user_id#73, cast(login_date#75 as date) AS login_date#81, sessions_count#76]\n            +- Scan ExistingRDD[user_id#73,kit_id#74,login_date#75,sessions_count#76]\n\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Window [user_id#73, login_date#81, sum(sessions_count#76) windowspecdefinition(user_id#73, login_date#81 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS running_count_of_sessions#1133L], [user_id#73], [login_date#81 ASC NULLS FIRST]\n   +- Sort [user_id#73 ASC NULLS FIRST, login_date#81 ASC NULLS FIRST], false, 0\n      +- Exchange hashpartitioning(user_id#73, 200), ENSURE_REQUIREMENTS, [plan_id=3550]\n         +- Project [user_id#73, cast(login_date#75 as date) AS login_date#81, sessions_count#76]\n            +- Scan ExistingRDD[user_id#73,kit_id#74,login_date#75,sessions_count#76]\n\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    User activity analysis : Write a solution to report for each player and date how many games played so far by the player. That is, the running sum of games played by the player.\n",
    "\"\"\"\n",
    "\n",
    "inputDF.withColumn(\"running_count_of_sessions\", sum(\"sessions_count\").over(Window.partitionBy(\"user_id\").orderBy(\"login_date\"))) \\\n",
    "    .select(\"user_id\", \"login_date\", \"running_count_of_sessions\") \\\n",
    "    .show()\n",
    "\n",
    "\n",
    "inputDF.createOrReplaceTempView(\"input_tbl\")\n",
    "spark.sql(\"\"\"\n",
    "          WITH CTE AS (\n",
    "            SELECT \n",
    "                *, \n",
    "                SUM(sessions_count) OVER(PARTITION BY user_id ORDER BY login_date) AS running_count_of_sessions\n",
    "            FROM input_tbl\n",
    "          )\n",
    "          SELECT user_id, login_date, running_count_of_sessions FROM CTE \n",
    "          \"\"\").show()\n",
    "\n",
    "\n",
    "inputDF.withColumn(\"running_count_of_sessions\", sum(\"sessions_count\").over(Window.partitionBy(\"user_id\").orderBy(\"login_date\"))) \\\n",
    "    .select(\"user_id\", \"login_date\", \"running_count_of_sessions\").explain()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "          WITH CTE AS (\n",
    "            SELECT \n",
    "                *, \n",
    "                SUM(sessions_count) OVER(PARTITION BY user_id ORDER BY login_date) AS running_count_of_sessions\n",
    "            FROM input_tbl\n",
    "          )\n",
    "          SELECT user_id, login_date, running_count_of_sessions FROM CTE \n",
    "          \"\"\").explain() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47c82d25-8f4f-49b1-b6bf-1d30f7cee16e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n|user_id|\n+-------+\n|      1|\n+-------+\n\n+-------+\n|user_id|\n+-------+\n|      1|\n+-------+\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [user_id#73]\n   +- Filter ((isnotnull(next_date#1041) AND isnotnull(first_login_date#1035)) AND (datediff(next_date#1041, first_login_date#1035) = 1))\n      +- Window [user_id#73, min(login_date#81) windowspecdefinition(user_id#73, login_date#81 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS first_login_date#1035, lead(login_date#81, 1, null) windowspecdefinition(user_id#73, login_date#81 ASC NULLS FIRST, specifiedwindowframe(RowFrame, 1, 1)) AS next_date#1041], [user_id#73], [login_date#81 ASC NULLS FIRST]\n         +- Sort [user_id#73 ASC NULLS FIRST, login_date#81 ASC NULLS FIRST], false, 0\n            +- Exchange hashpartitioning(user_id#73, 200), ENSURE_REQUIREMENTS, [plan_id=3201]\n               +- Project [user_id#73, cast(login_date#75 as date) AS login_date#81]\n                  +- Scan ExistingRDD[user_id#73,kit_id#74,login_date#75,sessions_count#76]\n\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [user_id#73]\n   +- Filter ((isnotnull(next_date#1055) AND isnotnull(first_login_date#1054)) AND (datediff(next_date#1055, first_login_date#1054) = 1))\n      +- Window [user_id#73, min(login_date#81) windowspecdefinition(user_id#73, login_date#81 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, unboundedpreceding$(), currentrow$())) AS first_login_date#1054, lead(login_date#81, 1, null) windowspecdefinition(user_id#73, login_date#81 ASC NULLS FIRST, specifiedwindowframe(RowFrame, 1, 1)) AS next_date#1055], [user_id#73], [login_date#81 ASC NULLS FIRST]\n         +- Sort [user_id#73 ASC NULLS FIRST, login_date#81 ASC NULLS FIRST], false, 0\n            +- Exchange hashpartitioning(user_id#73, 200), ENSURE_REQUIREMENTS, [plan_id=3262]\n               +- Project [user_id#73, cast(login_date#75 as date) AS login_date#81]\n                  +- Scan ExistingRDD[user_id#73,kit_id#74,login_date#75,sessions_count#76]\n\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    User activity analysis : You need to find the user_ids that logged in for at least two consecutive days starting from their first login date\n",
    "\"\"\"\n",
    "inputDF.withColumn(\"first_login_date\", min(\"login_date\").over(Window.partitionBy(\"user_id\").orderBy(\"login_date\"))) \\\n",
    "    .withColumn(\"next_date\", lead(\"login_date\").over(Window.partitionBy(\"user_id\").orderBy(\"login_date\"))) \\\n",
    "    .filter(datediff(\"next_date\", \"first_login_date\") == 1) \\\n",
    "    .select(\"user_id\") \\\n",
    "    .show()\n",
    "\n",
    "inputDF.createOrReplaceTempView(\"input_tbl\")\n",
    "spark.sql(\"\"\"\n",
    "            WITH CTE AS (\n",
    "                SELECT\n",
    "                    *,\n",
    "                    MIN(login_date) OVER(PARTITION BY user_id ORDER BY login_date) AS first_login_date,\n",
    "                    LEAD(login_date) OVER(PARTITION BY user_id ORDER BY login_date) AS next_date\n",
    "                FROM input_tbl\n",
    "            )\n",
    "            SELECT \n",
    "                user_id \n",
    "            FROM CTE\n",
    "            WHERE DATEDIFF(next_date, first_login_date) = 1\n",
    "          \"\"\").show()\n",
    "\n",
    "\n",
    "inputDF.withColumn(\"first_login_date\", min(\"login_date\").over(Window.partitionBy(\"user_id\").orderBy(\"login_date\"))) \\\n",
    "    .withColumn(\"next_date\", lead(\"login_date\").over(Window.partitionBy(\"user_id\").orderBy(\"login_date\"))) \\\n",
    "    .filter(datediff(\"next_date\", \"first_login_date\") == 1) \\\n",
    "    .select(\"user_id\").explain()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    WITH CTE AS (\n",
    "        SELECT\n",
    "            *,\n",
    "            MIN(login_date) OVER(PARTITION BY user_id ORDER BY login_date) AS first_login_date,\n",
    "            LEAD(login_date) OVER(PARTITION BY user_id ORDER BY login_date) AS next_date\n",
    "        FROM input_tbl\n",
    ")\n",
    "SELECT \n",
    "    user_id \n",
    "FROM CTE\n",
    "WHERE DATEDIFF(next_date, first_login_date) = 1\n",
    "\"\"\").explain()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3 Sports Data Analysis using PySpark - Part 01 and Part 02",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}