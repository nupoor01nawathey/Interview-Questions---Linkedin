{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1162e313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "|1   |2   |null|\n",
      "|null|3   |null|\n",
      "|5   |null|null|\n",
      "+----+----+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [col1: int, col2: int ... 1 more field]\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Input\n",
    "// col1  | col2 | col3\n",
    "// ------|------|-------\n",
    "// 1     | 2    | null\n",
    "// null  | 3    | null\n",
    "// 5     | null | null\n",
    "\n",
    "// Expected output\n",
    "// col1  | col2 \n",
    "// ------|------\n",
    "// 1     | 2    \n",
    "// null  | 3    \n",
    "// 5     | null \n",
    "\n",
    "\n",
    "val df = spark.createDataFrame(Seq(\n",
    "    (new Integer(1),new Integer(2),null.asInstanceOf[Integer]),\n",
    "    (null.asInstanceOf[Integer],new Integer(3),null.asInstanceOf[Integer]),\n",
    "    (new Integer(5),null.asInstanceOf[Integer],null.asInstanceOf[Integer])\n",
    ")).toDF(\"col1\", \"col2\", \"col3\")\n",
    "\n",
    "\n",
    "df.show(false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a8ea797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "exprn: scala.collection.immutable.Map[String,String] = Map(col1 -> count, col2 -> count, col3 -> count)\n",
       "res36: scala.collection.immutable.Map[String,String] = Map(col1 -> count, col2 -> count, col3 -> count)\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val exprn = df.columns.map((_ -> \"count\")).toMap\n",
    "\n",
    "exprn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f131580a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cnt: org.apache.spark.sql.Row = [2,2,0]\n",
       "res40: org.apache.spark.sql.Row = [2,2,0]\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cnt = df.agg(exprn).first\n",
    "\n",
    "cnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7e0c358f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|1   |2   |\n",
      "|null|3   |\n",
      "|5   |null|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.columns.filter(col => cnt.getAs[Long](\"count(\" + col + \")\") == 0 // This will return Array[String]=Array(col3)\n",
    "                 ).foldLeft(df)((tmp_df,col) => tmp_df.drop(col)).show(false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f9fae4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
